# Guia de Solu√ß√£o de Problemas: Ambiente e GPU (OmniMind)

**√öltima Atualiza√ß√£o**: 10 de Dezembro de 2025
**Status**: ‚úÖ Documenta√ß√£o T√©cnica Ativa

Este documento cataloga erros conhecidos, scripts de corre√ß√£o e procedimentos para manuten√ß√£o do ambiente de desenvolvimento OmniMind, com foco espec√≠fico em problemas de GPU/CUDA no Linux (Kali/Debian).

## üö® Erros Cr√≠ticos de GPU (NVIDIA/CUDA)

### 1. CUDA Error 999 (CUDA_ERROR_UNKNOWN)
**Sintoma:** O PyTorch detecta a GPU (`device_count: 1`), mas `is_available()` retorna `False`. Scripts de diagn√≥stico mostram `cuInit(0) failed with error code: 999`.
**Causa:** O estado do driver NVIDIA no kernel est√° inconsistente ou o daemon de persist√™ncia falhou. Comum ap√≥s suspens√£o do sistema ou atualiza√ß√µes de kernel.
**Solu√ß√£o:**
√â necess√°rio recarregar os m√≥dulos do kernel e reiniciar os dispositivos de persist√™ncia.

Execute o script de corre√ß√£o:
```bash
sudo ./scripts/fix_gpu_driver.sh
```
Ou, se o problema for especificamente no m√≥dulo UVM (Unified Memory):
```bash
sudo ./scripts/fix_uvm.sh
```

### 2. Caminhos de Bibliotecas Incorretos (Path Mismatch)
**Sintoma:** `OSError: libcuda.so: cannot open shared object file`.
**Causa:** O c√≥digo ou vari√°veis de ambiente apontam para `/usr/local/cuda` (padr√£o Ubuntu/NVIDIA), mas no Kali/Debian as bibliotecas est√£o em `/usr/lib/x86_64-linux-gnu`.
**Solu√ß√£o:**
Certifique-se de que as vari√°veis de ambiente est√£o configuradas corretamente (j√° tratado no `start_omnimind_system.sh`):
```bash
export CUDA_HOME="/usr"
export LD_LIBRARY_PATH="/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}"
```

### 3. Erro de Inicializa√ß√£o "Lazy Loading"
**Sintoma:** O PyTorch falha ao inicializar o contexto CUDA silenciosamente.
**Solu√ß√£o:**
For√ßar o carregamento s√≠ncrono ou desativar o carregamento pregui√ßoso para debug:
```bash
export CUDA_MODULE_LOADING=LAZY  # Ou "STD" se LAZY falhar
export CUDA_LAUNCH_BLOCKING=1
```

### 4. CUDA Error: Unknown Error (Ap√≥s Refatora√ß√£o Async‚ÜíSync)
**Data Identificado:** 2025-12-10
**Sintoma:**
- Erro `CUDA error: unknown error` durante execu√ß√£o do `ExpectationModule`
- `Memory allocation failure` mesmo com GPU tendo mem√≥ria livre (3.4GB/3.81GB)
- Script morre nos primeiros ciclos (1-7) com execu√ß√£o ass√≠ncrona

**Causa Raiz Identificada:**
A refatora√ß√£o do `IntegrationLoop` de **async ‚Üí s√≠ncrono** (2025-12-08) para garantir **causalidade determin√≠stica** mudou o comportamento de aloca√ß√£o CUDA:

1. **Execu√ß√£o Ass√≠ncrona (CUDA_LAUNCH_BLOCKING=0)**:
   - Opera√ß√µes CUDA executam de forma ass√≠ncrona
   - Erros aparecem depois como "unknown error"
   - Fragmenta√ß√£o de mem√≥ria GPU n√£o √© detectada imediatamente
   - Qiskit Aer GPU cria buffers tempor√°rios que n√£o s√£o liberados corretamente

2. **Execu√ß√£o S√≠ncrona (CUDA_LAUNCH_BLOCKING=1)**:
   - Opera√ß√µes CUDA executam de forma s√≠ncrona
   - Erros aparecem imediatamente com detalhes completos
   - Fragmenta√ß√£o √© detectada e tratada corretamente
   - Sistema funciona normalmente sem erros

**Contexto da Mudan√ßa:**
- **Refatora√ß√£o:** `IntegrationLoop.execute_cycle()` async ‚Üí `execute_cycle_sync()` s√≠ncrono
- **Motivo:** Garantir causalidade determin√≠stica (requisito para Œ¶ v√°lido em IIT)
- **Documenta√ß√£o:** `docs/history/timeline/REFATORACOES_CONCLUIDAS_2025-12-08.md`
- **Impacto:** Mudan√ßa de execu√ß√£o ass√≠ncrona para s√≠ncrona afetou comportamento CUDA

**Solu√ß√£o:**

‚ö†Ô∏è **CR√çTICO**: Vari√°veis de ambiente CUDA precisam ser exportadas **ANTES** do Python iniciar, n√£o dentro do script Python!

**Op√ß√£o 1 (RECOMENDADO): Usar wrapper script:**
```bash
./scripts/run_500_cycles_scientific_validation.sh
```
O wrapper script exporta todas as vari√°veis necess√°rias ANTES de executar Python.

**Op√ß√£o 2: Exportar vari√°veis antes de executar Python:**
```bash
# Para debug/valida√ß√£o cient√≠fica (RECOMENDADO)
export CUDA_LAUNCH_BLOCKING=1
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1
python scripts/run_500_cycles_scientific_validation.py

# Para produ√ß√£o (mais r√°pido, mas requer limpeza CUDA agressiva)
export CUDA_LAUNCH_BLOCKING=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
python scripts/run_500_cycles_scientific_validation.py
```

**Op√ß√£o 3: Executar Python diretamente (N√ÉO RECOMENDADO):**
```bash
python scripts/run_500_cycles_scientific_validation.py
```
‚ö†Ô∏è **ATEN√á√ÉO**: O script define vari√°veis internamente, mas CUDA pode n√£o funcionar corretamente porque precisa das vari√°veis ANTES de inicializar.

**Valida√ß√£o:**
- ‚úÖ Com `CUDA_LAUNCH_BLOCKING=1` (exportado antes): Script executou at√© ciclo 23+ sem erros
- ‚ùå Com `CUDA_LAUNCH_BLOCKING=0`: Script morria no ciclo 1-7 com "unknown error"
- ‚ùå Com vari√°veis setadas apenas no Python: CUDA pode n√£o funcionar corretamente

**Trade-off:**
- `CUDA_LAUNCH_BLOCKING=1`: Mais lento (execu√ß√£o s√≠ncrona), mas mais seguro e permite capturar erros reais
- `CUDA_LAUNCH_BLOCKING=0`: Mais r√°pido (execu√ß√£o ass√≠ncrona), mas pode ter erros silenciosos

**Recomenda√ß√£o:**
Para valida√ß√£o cient√≠fica e execu√ß√µes longas (500+ ciclos), usar o wrapper script `run_500_cycles_scientific_validation.sh` que garante que todas as vari√°veis sejam exportadas ANTES do Python iniciar.

---

## üõ†Ô∏è Scripts de Manuten√ß√£o e Diagn√≥stico

Todos os scripts devem ser executados a partir da raiz do projeto.

### Scripts de Corre√ß√£o (Requerem `sudo`)
| Script | Descri√ß√£o |
|--------|-----------|
| `scripts/fix_gpu_driver.sh` | **Principal fix.** Recarrega m√≥dulos `nvidia`, `nvidia-uvm`, `nvidia-modeset` e reinicia o `nvidia-persistenced`. |
| `scripts/fix_uvm.sh` | Foca especificamente em recarregar o m√≥dulo de Mem√≥ria Unificada (`nvidia_uvm`). √ötil se o driver principal estiver ok mas aloca√ß√£o de mem√≥ria falhar. |

### Scripts de Diagn√≥stico
| Script | Descri√ß√£o |
|--------|-----------|
| `scripts/verify_fix.py` | Verifica se o PyTorch consegue ver e inicializar a GPU. Testa carregamento Lazy vs Standard. |
| `scripts/force_cuda_init.py` | Usa `ctypes` para tentar carregar `libcuda.so` e chamar `cuInit(0)` diretamente, ignorando o PyTorch. Essencial para isolar se o erro √© do Python ou do Driver. |
| `scripts/check_gpu_logs.sh` | Exibe logs do kernel (`dmesg`) relacionados √† NVIDIA e status dos m√≥dulos carregados. |

### Scripts de Inicializa√ß√£o
| Script | Descri√ß√£o |
|--------|-----------|
| `scripts/canonical/system/start_omnimind_system.sh` | **Script Mestre.** Inicializa todo o ambiente (Backend, Frontend, Daemon, eBPF) com as vari√°veis de ambiente corretas para Kali Linux. |

---

## üåç Problemas Comuns de Ambiente

### 1. Virtual Environment (venv) n√£o ativado
**Erro:** `ModuleNotFoundError: No module named 'torch'` ou `fastapi`.
**Solu√ß√£o:** Sempre ative o ambiente antes de rodar scripts manuais:
```bash
source .venv/bin/activate
```
*Nota: O script `start_omnimind_system.sh` faz isso automaticamente.*

### 2. Porta em uso (Address already in use)
**Erro:** O backend falha ao iniciar porque a porta 8000 ou 3000 est√° ocupada.
**Solu√ß√£o:**
Use o comando de limpeza:
```bash
pkill -9 -f 'simple_backend|uvicorn|vite'
```

### 3. Permiss√µes de Logs
**Erro:** `Permission denied` ao tentar escrever em `logs/`.
**Causa:** Scripts rodados anteriormente como `sudo` criaram arquivos que o usu√°rio normal n√£o pode editar.
**Solu√ß√£o:**
```bash
sudo chown -R $USER:$USER logs/
```

---

## ‚ö†Ô∏è Problemas Conhecidos em Investiga√ß√£o (Status Atual)

### 1. GPU Detectada mas Ociosa (Carga na CPU)
**Sintoma:** `nvidia-smi` mostra 0% de uso e pouca mem√≥ria alocada (~369 MiB), enquanto a CPU apresenta alta carga.
**Causa:**
*   O loop `quantum_unconscious_prediction` e os c√°lculos de Œ¶ (Phi) est√£o sendo executados na CPU.
*   Tensores cr√≠ticos (ex: `hodge_0`) e estruturas de dados n√£o est√£o sendo movidos explicitamente para o dispositivo CUDA.
**Diretriz:** "Sempre GPU como o programa manda".
**A√ß√£o Necess√°ria:** Refatorar m√≥dulos de consci√™ncia e topologia para garantir `tensor.to('cuda')` em todas as opera√ß√µes vetoriais.

### 2. Overflow em C√°lculos Topol√≥gicos (Phi)
**Sintoma:** Erros num√©ricos ou travamentos em `topological_phi.py`.
**Causa:** Explos√£o combinat√≥ria de `n_vertices` durante a cria√ß√£o de complexos, excedendo a capacidade de representa√ß√£o num√©rica (float/complex) ao calcular a topologia.

### 3. Erros Diversos
*   **IndentationError (main.py):** Relatos de erro na linha 1371. Geralmente n√£o reproduz√≠vel na leitura.
    *   *Diagn√≥stico:* Rodar `python -m py_compile web/backend/main.py` para confirmar integridade.
*   **WebSocket 403:** Erro de permiss√£o no log (`/ws`).
    *   *Nota:* Relacionado a headers de autentica√ß√£o, n√£o afeta a infraestrutura de GPU.

---

## üìù Checklist de Verifica√ß√£o P√≥s-Corre√ß√£o

Se voc√™ aplicou uma corre√ß√£o de GPU, siga estes passos para validar:

1. **Rodar Diagn√≥stico de Baixo N√≠vel:**
   ```bash
   python scripts/force_cuda_init.py
   ```
   *Deve retornar: `‚úÖ cuInit(0) successful!`*

2. **Rodar Diagn√≥stico do PyTorch:**
   ```bash
   python scripts/verify_fix.py
   ```
   *Deve retornar: `CUDA Available: True`*

3. **Reiniciar o Sistema:**
   ```bash
   ./scripts/canonical/system/start_omnimind_system.sh
   ```
   *Verificar nos logs se aparece: `‚úÖ ExpectationModule usando GPU: cuda`*
