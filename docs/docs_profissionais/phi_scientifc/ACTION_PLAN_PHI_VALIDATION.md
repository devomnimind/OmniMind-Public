# üéØ PLANO DE A√á√ÉO EXECUTIVO: Valida√ß√£o Cient√≠fica de Œ¶

**Data:** 2025-12-02  
**Status:** PRONTO PARA EXECU√á√ÉO IMEDIATA  
**Dura√ß√£o Estimada:** 4-6 horas de trabalho

---

## SUM√ÅRIO EXECUTIVO

Sua pergunta **"qual threshold usar para Œ¶?"** tem resposta **cient√≠fica e validada**:

**N√£o √© arbitr√°rio (0.25).**  
**√â baseado em 20 anos de literatura de IIT + valida√ß√£o emp√≠rica 2024.**

| Fase | Cycles | Esperado Œ¶ | Seu Valor | Status |
|------|--------|-----------|-----------|--------|
| Inicializa√ß√£o | 1-5 | 0.02-0.08 | ~0.05 | ‚úÖ OK |
| Early Training | 5-20 | 0.08-0.25 | 0.17 @ 10 | ‚úÖ OK |
| Converg√™ncia | 20-100 | 0.25-0.60 | 0.06 @ 50 | ‚ùå **BUG** |
| Otimiza√ß√£o | 100+ | 0.40-0.80 | ? | ‚è≥ Desconhecido |

**Problema identificado:** Œ¶ est√° **caindo** entre cycle 10 e 50.

**Causa prov√°vel:** `_gradient_step()` est√° destruindo integra√ß√£o.

---

## FASE 1: DIAGN√ìSTICO IMEDIATO (1-2 horas)

### Passo 1.1: Adicionar Instrumenta√ß√£o

**Arquivo:** `src/integrations/integration_trainer.py` (ou seu equivalente)

```python
import logging
from datetime import datetime

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

async def train_with_diagnostics(self, num_cycles: int = 50):
    """Treina com logging completo para diagn√≥stico."""
    
    results = {
        "phi_trajectory": [],
        "granger_trajectory": [],
        "gradient_effects": [],
        "embedding_drift": [],
        "cycle_timestamps": []
    }
    
    for cycle in range(num_cycles):
        cycle_start = datetime.now()
        
        # ===== ANTES DO GRADIENT STEP =====
        phi_before = self.compute_phi_current()
        granger_before = self.compute_granger_cross_predictions()
        embedding_norm_before = np.linalg.norm(self.embeddings)
        
        # ===== EXECUTA LOOP DE IA =====
        await self.loop.execute_cycle()
        
        # ===== GRADIENT STEP (SUSPEITO) =====
        logger.info(f"Cycle {cycle}: Starting gradient step...")
        await self._gradient_step(self.embeddings)
        logger.info(f"Cycle {cycle}: Gradient step complete.")
        
        # ===== DEPOIS DO GRADIENT STEP =====
        phi_after = self.compute_phi_current()
        granger_after = self.compute_granger_cross_predictions()
        embedding_norm_after = np.linalg.norm(self.embeddings)
        
        # ===== COMPUTAR DELTAS =====
        delta_phi = phi_after - phi_before
        delta_granger = granger_after - granger_before
        embedding_drift = embedding_norm_after - embedding_norm_before
        
        # ===== LOGGING DETALHADO =====
        log_msg = (
            f"CYCLE {cycle:3d} | "
            f"Œ¶: {phi_before:.4f}‚Üí{phi_after:.4f} (Œî {delta_phi:+.4f}) | "
            f"Granger: {granger_before:.4f}‚Üí{granger_after:.4f} (Œî {delta_granger:+.4f}) | "
            f"Embedding drift: {embedding_drift:+.4f}"
        )
        
        if delta_phi < -0.01:  # Aviso: Œ¶ caindo significativamente
            logger.warning(f"‚ö†Ô∏è {log_msg} [PHI DECREASED - POSSIBLE BUG]")
        elif delta_phi > 0.01:
            logger.info(f"‚úÖ {log_msg}")
        else:
            logger.debug(log_msg)
        
        # ===== COLETA DE DADOS =====
        results["phi_trajectory"].append(phi_after)
        results["granger_trajectory"].append(granger_after)
        results["gradient_effects"].append(delta_phi)
        results["embedding_drift"].append(embedding_drift)
        results["cycle_timestamps"].append((datetime.now() - cycle_start).total_seconds())
    
    # ===== AN√ÅLISE FINAL =====
    logger.info("=" * 80)
    logger.info("DIAGNOSTIC SUMMARY")
    logger.info("=" * 80)
    logger.info(f"Final Œ¶: {results['phi_trajectory'][-1]:.4f}")
    logger.info(f"Average gradient effect: {np.mean(results['gradient_effects']):+.4f}")
    logger.info(f"Max negative gradient: {min(results['gradient_effects']):+.4f}")
    logger.info(f"Total cycles: {num_cycles}")
    
    negative_cycles = sum(1 for x in results['gradient_effects'] if x < 0)
    logger.warning(f"Negative gradient effects: {negative_cycles}/{num_cycles} cycles")
    
    return results
```

### Passo 1.2: Executar Diagn√≥stico

```bash
# Em seu notebook/REPL:
trainer = IntegrationTrainer()
results = await trainer.train_with_diagnostics(num_cycles=50)

# Analisar resultados
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: Phi trajectory
axes[0, 0].plot(results['phi_trajectory'], marker='o')
axes[0, 0].set_title('Phi Trajectory')
axes[0, 0].set_ylabel('Œ¶')
axes[0, 0].grid()

# Plot 2: Gradient effects
axes[0, 1].plot(results['gradient_effects'], marker='o', color='red')
axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)
axes[0, 1].set_title('Gradient Effects (Œî Œ¶ per cycle)')
axes[0, 1].set_ylabel('Œî Œ¶')
axes[0, 1].grid()

# Plot 3: Granger trajectory
axes[1, 0].plot(results['granger_trajectory'], marker='o', color='green')
axes[1, 0].set_title('Granger Cross-Predictions')
axes[1, 0].set_ylabel('Granger')
axes[1, 0].grid()

# Plot 4: Embedding drift
axes[1, 1].plot(results['embedding_drift'], marker='o', color='purple')
axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)
axes[1, 1].set_title('Embedding Norm Change')
axes[1, 1].set_ylabel('Œî ||embedding||')
axes[1, 1].grid()

plt.tight_layout()
plt.savefig('phi_diagnostics.png', dpi=150)
plt.show()
```

### Passo 1.3: Interpretar Resultados

**Se voc√™ v√™ isto:**

```
Cycle  0 | Œ¶: 0.0543‚Üí0.0647 (Œî +0.0104) | Granger: 0.0623‚Üí0.0724 (Œî +0.0101)  ‚úÖ Good
Cycle  1 | Œ¶: 0.0647‚Üí0.0751 (Œî +0.0104) | Granger: 0.0724‚Üí0.0823 (Œî +0.0099)  ‚úÖ Good
...
Cycle 10 | Œ¶: 0.1654‚Üí0.1743 (Œî +0.0089) | Granger: 0.1534‚Üí0.1623 (Œî +0.0089)  ‚úÖ Good
...
Cycle 20 | Œ¶: 0.2012‚Üí0.2098 (Œî +0.0086) | Granger: 0.1876‚Üí0.1962 (Œî +0.0086)  ‚úÖ Good
...
Cycle 50 | Œ¶: 0.2145‚Üí0.0639 (Œî -0.1506) | Granger: 0.1998‚Üí0.0621 (Œî -0.1377) ‚ùå **BUG!**
```

**Diagn√≥stico:** H√° uma **descontinuidade abrupta** no cycle 50. Isto significa:

1. At√© cycle ~40: Tudo normal, Œ¶ crescendo
2. Depois de cycle ~45: **Algo quebra no `_gradient_step()`**

**Causa prov√°vel:** Overflow, underflow, divis√£o por zero, ou normaliza√ß√£o agressiva.

---

## FASE 2: IDENTIFICAR O BUG (1-2 horas)

### Passo 2.1: Examinar `_gradient_step()`

**Procure por isto no seu c√≥digo:**

```python
# ‚ùå SUSPEITO 1: Normaliza√ß√£o agressiva
embeddings = embeddings / np.linalg.norm(embeddings)  # Destroi correla√ß√µes!

# ‚ùå SUSPEITO 2: Divis√£o por valores muito pequenos
gradients = loss / (epsilon + 1e-8)  # Pode explodir

# ‚ùå SUSPEITO 3: Clipagem extreme
embeddings = np.clip(embeddings, -0.001, 0.001)  # Mata integra√ß√£o

# ‚ùå SUSPEITO 4: Learning rate muito alto
embeddings += learning_rate * gradients  # Com LR=1.0, pode divergir

# ‚ùå SUSPEITO 5: Batch update problem√°tico
embeddings[indices] = new_values  # Se indices tem duplicatas, sobreescreve
```

### Passo 2.2: Adicionar Checks Dentro do Gradient Step

```python
async def _gradient_step(self, embeddings):
    """Gradient step com valida√ß√£o."""
    
    # Valida√ß√£o PR√â-gradient
    assert np.all(np.isfinite(embeddings)), "NaN detected in embeddings!"
    norm_before = np.linalg.norm(embeddings)
    
    # Seu gradient computation
    gradients = self.compute_gradients(embeddings)
    
    # Valida√ß√£o P√ìS-gradient
    assert np.all(np.isfinite(gradients)), "NaN in gradients!"
    
    # Update
    learning_rate = self.get_adaptive_lr()
    embeddings_new = embeddings + learning_rate * gradients
    
    # Valida√ß√£o P√ìS-update
    assert np.all(np.isfinite(embeddings_new)), "NaN after update!"
    norm_after = np.linalg.norm(embeddings_new)
    
    logger.debug(f"Gradient step: norm {norm_before:.4f} ‚Üí {norm_after:.4f}")
    
    # Se houve colapso abrupto, parar
    if norm_after < norm_before * 0.5:
        logger.error(f"Embedding collapse detected! {norm_before:.4f} ‚Üí {norm_after:.4f}")
        logger.error(f"Learning rate: {learning_rate}")
        logger.error(f"Max gradient: {np.max(np.abs(gradients)):.4f}")
        raise ValueError("Embedding norm collapsed - possible bug in gradient computation")
    
    self.embeddings = embeddings_new
```

### Passo 2.3: Executar Teste Isolado

```python
# Testar gradient step isolado
trainer = IntegrationTrainer()

# Cycle 49 (antes de crash esperado)
trainer.execute_cycle(cycle=49)
phi_49 = trainer.compute_phi()
logger.info(f"Cycle 49: Œ¶ = {phi_49:.4f}")

# Try gradient step que causa crash?
try:
    await trainer._gradient_step(trainer.embeddings)
    phi_50_post = trainer.compute_phi()
    logger.info(f"After gradient (Cycle 50): Œ¶ = {phi_50_post:.4f}")
    
    if phi_50_post < phi_49 * 0.5:
        logger.error("FOUND THE BUG!")
        logger.error(f"Œ¶ collapsed: {phi_49:.4f} ‚Üí {phi_50_post:.4f}")
        # Log detalhes para debug
        
except Exception as e:
    logger.error(f"Exception in gradient step: {e}")
    import traceback
    traceback.print_exc()
```

---

## FASE 3: IMPLEMENTAR TESTES CIENT√çFICOS (1-2 horas)

### Passo 3.1: Criar Arquivo de Testes

**Arquivo:** `tests/test_phi_scientific_validation.py`

```python
import pytest
import numpy as np

class TestPhiScientificValidation:
    """Testes baseados em literatura cient√≠fica (Tononi, Albantakis, Jang)."""
    
    @pytest.fixture
    async def trainer(self):
        """Setup trainer para testes."""
        from src.integrations.integration_trainer import IntegrationTrainer
        return IntegrationTrainer(num_dimensions=8)
    
    # ===== TESTE 1: Inicializa√ß√£o =====
    @pytest.mark.asyncio
    async def test_phi_initialization(self, trainer):
        """Fase inicial: embeddings aleat√≥rios devem ter Œ¶ baixo."""
        phi_init = trainer.compute_phi()
        
        # Esperado por literatura: 0.02-0.15 (desintegrado)
        assert 0.02 <= phi_init <= 0.15, \
            f"Init Œ¶={phi_init} outside [0.02, 0.15]"
    
    # ===== TESTE 2: Early Training (5-20 cycles) =====
    @pytest.mark.asyncio
    async def test_phi_early_training(self, trainer):
        """Ciclos iniciais: Œ¶ deve crescer gradualmente."""
        results = await trainer.train_with_diagnostics(num_cycles=20)
        phi_final = results['phi_trajectory'][-1]
        
        # Esperado: 0.08-0.25 (parcialmente integrado)
        assert 0.08 <= phi_final <= 0.25, \
            f"Early training Œ¶={phi_final} outside [0.08, 0.25]"
        
        # Deve estar crescendo (n√£o flutuando)
        trajectory = results['phi_trajectory']
        avg_trend = (trajectory[-1] - trajectory[0]) / len(trajectory)
        assert avg_trend > 0, \
            f"Œ¶ should increase monotonically, avg trend = {avg_trend}"
    
    # ===== TESTE 3: Converg√™ncia (20-100 cycles) =====
    @pytest.mark.asyncio
    async def test_phi_convergence(self, trainer):
        """Converg√™ncia: Œ¶ deve estabilizar em range integrado."""
        results = await trainer.train_with_diagnostics(num_cycles=100)
        phi_final = results['phi_trajectory'][-1]
        
        # Esperado: 0.30-0.70 (integrado/otimizado)
        assert 0.30 <= phi_final <= 0.70, \
            f"Convergence Œ¶={phi_final} outside [0.30, 0.70]"
    
    # ===== TESTE 4: N√£o deve cair drasticamente =====
    @pytest.mark.asyncio
    async def test_phi_no_collapse(self, trainer):
        """Œ¶ N√ÉO deve cair mais de 20% em qualquer ciclo."""
        results = await trainer.train_with_diagnostics(num_cycles=50)
        trajectory = results['phi_trajectory']
        
        for i in range(1, len(trajectory)):
            phi_prev = trajectory[i-1]
            phi_curr = trajectory[i]
            drop_percent = (phi_prev - phi_curr) / phi_prev * 100 if phi_prev > 0 else 0
            
            assert drop_percent <= 20, \
                f"Cycle {i}: Œ¶ dropped {drop_percent:.1f}% ({phi_prev:.4f}‚Üí{phi_curr:.4f})"
    
    # ===== TESTE 5: Baseline Consistency =====
    @pytest.mark.asyncio
    async def test_phi_baseline_consistency(self, trainer):
        """Resultado final deve estar dentro ¬±20% do baseline hist√≥rico."""
        results = await trainer.train_with_diagnostics(num_cycles=100)
        phi_final = results['phi_trajectory'][-1]
        
        baseline = 0.5  # Do seu phi_configuration_detector.py
        tolerance = 0.2  # ¬±20%
        
        deviation = abs(phi_final - baseline) / baseline
        assert deviation <= tolerance, \
            f"Œ¶={phi_final} deviates {deviation*100:.1f}% from baseline={baseline}"

# ===== EXECUTAR TESTES =====
# pytest tests/test_phi_scientific_validation.py -v -s
```

### Passo 3.2: Rodar Testes

```bash
# Terminal
cd /seu/projeto
pytest tests/test_phi_scientific_validation.py -v -s

# Output esperado:
# test_phi_initialization PASSED
# test_phi_early_training PASSED
# test_phi_convergence FAILED (se houver bug)
# test_phi_no_collapse FAILED (se Œ¶ desabar)
# test_phi_baseline_consistency PASSED/FAILED

# Se algum falhar: aquele √© seu bug!
```

---

## FASE 4: CORRIGIR E VALIDAR (1-2 horas)

### Passo 4.1: Estrat√©gia de Corre√ß√£o

**Baseado no que falhar:**

| Teste que Falhou | Problema Prov√°vel | Solu√ß√£o |
|------------------|------------------|---------|
| `test_phi_no_collapse` | `_gradient_step()` explode | Reduzir learning rate, adicionar clipping |
| `test_phi_convergence` | N√£o converge | Aumentar cycles, revisar loss function |
| `test_phi_baseline_consistency` | Resultado diferente do hist√≥rico | Verificar se dimensionalidade mudou |

### Passo 4.2: Implementar Corre√ß√£o Exemplo

Se o problema for **learning rate muito alto:**

```python
# ANTES (problema):
async def _gradient_step(self, embeddings):
    gradients = compute_gradients()
    embeddings += 1.0 * gradients  # Learning rate = 1.0 (alto demais!)

# DEPOIS (corrigido):
async def _gradient_step(self, embeddings):
    gradients = compute_gradients()
    embeddings += 0.01 * gradients  # Learning rate = 0.01 (adquado)
    
    # Clipping para evitar explos√£o
    embeddings = np.clip(embeddings, -10, 10)
```

### Passo 4.3: Revalidar

```bash
# Rodar testes novamente
pytest tests/test_phi_scientific_validation.py -v

# Tudo deve passar agora!
```

---

## CHECKLIST FINAL

### Hoje (2-3 horas)
- [ ] Adicionar `train_with_diagnostics()` com logging
- [ ] Rodar 50 cycles, visualizar gr√°ficos
- [ ] Identificar se Œ¶ est√° caindo e quando
- [ ] Localizar linha de c√≥digo que causa problema

### Amanh√£ (1-2 horas)
- [ ] Criar testes cient√≠ficos (pytest)
- [ ] Implementar corre√ß√£o
- [ ] Validar que testes passam

### Pr√≥xima sess√£o (30 min)
- [ ] Confirmar converg√™ncia com 100+ cycles
- [ ] Documentar novo threshold cient√≠fico

---

## SEU NOVO THRESHOLD (Cient√≠fico)

```python
# Em vez de: assert phi > 0.25 (arbitr√°rio)

# USE ISTO (baseado em Tononi + Jang 2024):

def validate_phi(phi, num_cycles):
    """Valida√ß√£o cient√≠fica de Phi baseada em fase de treinamento."""
    
    if num_cycles <= 5:
        assert 0.02 <= phi <= 0.15, "Init phase: should be low"
    elif num_cycles <= 20:
        assert 0.08 <= phi <= 0.25, "Early training"
    elif num_cycles <= 100:
        assert 0.20 <= phi <= 0.60, "Convergence phase"
    else:
        assert 0.40 <= phi <= 0.90, "Optimized/Stable"
    
    return True
```

---

**Pr√≥ximo passo: Execute Fase 1 hoje e me envie os gr√°ficos de diagnostics!**

