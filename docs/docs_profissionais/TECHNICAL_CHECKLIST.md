# âœ… Checklist TÃ©cnico PrÃ©-ExecuÃ§Ã£o - OmniMind

**Ãšltima AtualizaÃ§Ã£o**: 08 de Dezembro de 2025
**VersÃ£o**: Phase 24+ (Lacanian Memory + Quantum Consciousness)

---

## ğŸ“‹ PrÃ©-requisitos do Sistema

### Hardware MÃ­nimo
- **CPU**: 4 cores (Intel i5/Ryzen 5 ou superior)
- **RAM**: 8GB (16GB recomendado)
- **GPU**: NVIDIA GTX 1650 ou superior (4GB VRAM) - **Opcional mas recomendado**
- **Armazenamento**: 50GB SSD disponÃ­vel
- **SO**: Linux Ubuntu 20.04+ ou similar (Kali Linux 6.16.8+ validado)

### Software ObrigatÃ³rio
- **Python**: 3.12.8 (obrigatÃ³rio, outras versÃµes podem causar problemas)
- **Ollama**: Instalado e rodando com modelo `phi:latest`
- **CUDA**: 12.4+ (se GPU disponÃ­vel)
- **Qdrant**: Opcional para testes completos (rodando em `http://localhost:6333`)

### VerificaÃ§Ã£o RÃ¡pida
```bash
# Verificar Python
python --version  # Deve ser 3.12.8

# Verificar Ollama e modelo phi:latest
ollama list | grep phi
# Deve mostrar: phi:latest

# Verificar CUDA (se GPU disponÃ­vel)
python -c "import torch; print(torch.cuda.is_available())"
# Deve retornar: True

# Verificar Qdrant (opcional)
curl http://localhost:6333/health
# Deve retornar: {"status":"ok"}
```

---

## ğŸ§ª Scripts de Teste Ativos

### âš¡ ExecuÃ§Ã£o DiÃ¡ria - `run_tests_fast.sh` (RECOMENDADO)

**Comando**:
```bash
./scripts/run_tests_fast.sh
```

**CaracterÃ­sticas**:
- â±ï¸ **Tempo**: ~10-15 minutos
- ğŸ“Š **Escopo**: ~3996 testes (suite rÃ¡pida)
- ğŸš€ **GPU**: âœ… FORÃ‡ADA (CUDA_VISIBLE_DEVICES=0)
- ğŸ” **Exclui**: Testes marcados com `@pytest.mark.slow` e `@pytest.mark.chaos`
- âœ… **Inclui**: Testes marcados com `@pytest.mark.real` (sem chaos)
- ğŸ“ **Logs**: `data/test_reports/output_fast_*.log`
- ğŸ¯ **Uso**: ValidaÃ§Ã£o diÃ¡ria rÃ¡pida, desenvolvimento iterativo

**Marcadores de Teste**:
- `@pytest.mark.slow`: Testes longos (>30s timeout) - **EXCLUÃDOS**
- `@pytest.mark.real`: Testes com GPU+LLM+Network (nÃ£o destrutivos) - **INCLUÃDOS**
- `@pytest.mark.chaos`: Testes de destruiÃ§Ã£o de servidor - **EXCLUÃDOS**
- Sem marcadores: Testes unitÃ¡rios/integraÃ§Ã£o mockados - **INCLUÃDOS**

---

### ğŸ›¡ï¸ ValidaÃ§Ã£o Semanal - `run_tests_with_defense.sh`

**Comando**:
```bash
./scripts/run_tests_with_defense.sh
```

**CaracterÃ­sticas**:
- â±ï¸ **Tempo**: 1-2 horas (varia com crashes detectados)
- ğŸ“Š **Escopo**: ~4004 testes (suite completa + chaos engineering)
- ğŸš€ **GPU**: âœ… FORÃ‡ADA
- ğŸ›¡ï¸ **Autodefesa**: âœ… Detecta testes perigosos (3+ crashes em 5min = label "dangerous")
- âš ï¸ **ATENÃ‡ÃƒO**: Inclui testes de chaos engineering que **destroem servidor intencionalmente**
- ğŸ“ˆ **Gera**: RelatÃ³rio de perigo e mÃ©tricas em `data/test_reports/`
- ğŸ“ **Logs**: `data/test_reports/output_*.log`
- ğŸ¯ **Uso**: ValidaÃ§Ã£o semanal completa, certificaÃ§Ã£o de resiliÃªncia

**Testes de Chaos Engineering**:
- Destroem servidor intencionalmente para validar resiliÃªncia de Î¦
- Testam recuperaÃ§Ã£o automÃ¡tica do sistema
- Validam integridade de mÃ©tricas apÃ³s crashes

---

### ğŸ§ª IntegraÃ§Ã£o Completa - `quick_test.sh` (AVANÃ‡ADO)

**PrÃ©-requisito (UMA VEZ)**:
```bash
bash scripts/configure_sudo_omnimind.sh  # Setup NOPASSWD sudo
```

**Comando**:
```bash
bash scripts/quick_test.sh
```

**CaracterÃ­sticas**:
- â±ï¸ **Tempo**: ~30-45 minutos
- ğŸ“Š **Escopo**: Suite completa + servidor backend
- ğŸš€ **GPU**: âœ… FORÃ‡ADA
- ğŸ–¥ï¸ **Servidor**: âœ… Inicia em `localhost:8000`
- ğŸ’¾ **Requer**: sudo configurado (para inicializaÃ§Ã£o do servidor)
- ğŸ”— **Testa**: Contra servidor real (nÃ£o isolado)
- ğŸ“ **Logs**: `data/test_reports/output_*.log`
- ğŸ¯ **Uso**: ValidaÃ§Ã£o de integraÃ§Ã£o completa, testes end-to-end

---

## âš ï¸ IBM Quantum Real Hardware (Fase Madura - Futuro)

**Status**: âœ… Implementado, âŒ **NÃƒO em ciclo ativo**

**Detalhes**:
- **Papers 2&3**: Validados em hardware real IBM Quantum (ibm_fez 27Q, ibm_torino 84Q)
- **Tempos de execuÃ§Ã£o reais**: 30-120 segundos por job
- **RestriÃ§Ã£o**: CrÃ©ditos gratuitos limitados
- **Plano**: Ativar em Phase 25+ para certificaÃ§Ã£o regular

**ConfiguraÃ§Ã£o Atual**:
```python
# tests/conftest.py
os.environ["OMNIMIND_DISABLE_IBM"] = "True"  # IBM auth falhando em sandbox
```

**Para Habilitar**:
```bash
# Definir token IBM no ambiente
export IBM_QUANTUM_TOKEN="your_token_here"
export OMNIMIND_DISABLE_IBM="False"

# EntÃ£o executar testes
./scripts/run_tests_with_defense.sh
```

---

## ğŸ”§ CorreÃ§Ãµes CrÃ­ticas Implementadas

### âœ… CRÃTICO 1: Timeout em Consensus Voting

**Arquivo**: `src/swarm/collective_learning.py`
**Status**: âœ… IMPLEMENTADO

**MudanÃ§as**:
- [x] Adicionado `MAX_CONSENSUS_TIMEOUT = 30.0` segundos
- [x] Implementado `threading.Lock()` para thread-safety
- [x] Modificado `get_consensus_model()` com timeout protection
- [x] Fallback: retorna consensus parcial se timeout excedido
- [x] Logging detalhado de timeout e recuperaÃ§Ã£o

**ValidaÃ§Ã£o**:
```python
from src.swarm.collective_learning import ConsensusLearning
cl = ConsensusLearning(5, consensus_timeout=30.0)
```

---

### âœ… CRÃTICO 2: Memory Cap com LRU Eviction

**Arquivo**: `src/memory/episodic_memory.py` (deprecated, usar `NarrativeHistory`)
**Status**: âœ… IMPLEMENTADO

**MudanÃ§as**:
- [x] Limite de memÃ³ria configurÃ¡vel
- [x] Eviction LRU automÃ¡tico quando limite excedido
- [x] PreservaÃ§Ã£o de episÃ³dios mais importantes

**Nota**: `EpisodicMemory` estÃ¡ deprecated em favor de `NarrativeHistory` (Lacanian).

---

### âœ… CRÃTICO 3: Modelo LLM PadrÃ£o

**Arquivo**: MÃºltiplos (`src/neurosymbolic/neural_component.py`, etc.)
**Status**: âœ… CORRIGIDO (2025-12-05)

**MudanÃ§as**:
- [x] Modelo padrÃ£o alterado de `qwen2:7b-instruct` para `phi:latest`
- [x] ConfiguraÃ§Ã£o centralizada em `config/agent_config.yaml`
- [x] Fallback para `qwen2:7b-instruct` se `phi:latest` nÃ£o disponÃ­vel

**Arquivos Corrigidos**:
- `src/neurosymbolic/neural_component.py`: `ollama/phi:latest`
- `src/neurosymbolic/hybrid_reasoner.py`: `ollama/phi:latest`
- `src/integrations/orchestrator_llm.py`: `phi:latest`
- `src/orchestrator/task_executor.py`: `phi:latest`

---

## ğŸ“Š ConfiguraÃ§Ã£o de Testes

### Timeout Global
- **Por teste**: 800 segundos (independente, nÃ£o cumulativo)
- **MÃ©todo**: Thread-based (interrupÃ§Ã£o segura)
- **ConfiguraÃ§Ã£o**: `config/pytest.ini`

### VariÃ¡veis de Ambiente para Testes
```bash
CUDA_VISIBLE_DEVICES=0          # ForÃ§a GPU device 0
OMNIMIND_GPU=true               # Habilita GPU
OMNIMIND_FORCE_GPU=true         # ForÃ§a detecÃ§Ã£o GPU com fallback
OMNIMIND_DEV=true               # Modo desenvolvimento
OMNIMIND_DEBUG=true             # Logging debug
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512  # OtimizaÃ§Ã£o memÃ³ria GPU
```

### Gerenciamento de Servidor
- **Centralizado**: Via `ServerStateManager` (previne race conditions)
- **Auto-limpeza**: Servidores sÃ£o limpos automaticamente apÃ³s testes
- **Isolamento**: Cada teste tem seu prÃ³prio contexto de servidor

---

## âœ… Checklist PrÃ©-ExecuÃ§Ã£o

Antes de executar testes ou iniciar o sistema, verifique:

### Ambiente
- [ ] Python 3.12.8 instalado e ativo
- [ ] Ambiente virtual ativado (`.venv`)
- [ ] DependÃªncias instaladas (`pip install -r requirements.txt`)
- [ ] Ollama rodando (`ollama serve`)
- [ ] Modelo `phi:latest` disponÃ­vel (`ollama list | grep phi`)

### GPU (Opcional)
- [ ] CUDA instalado e funcionando
- [ ] Driver NVIDIA atualizado
- [ ] PyTorch detecta GPU (`python -c "import torch; print(torch.cuda.is_available())"`)

### ServiÃ§os (Opcional para testes completos)
- [ ] Qdrant rodando (`curl http://localhost:6333/health`)
- [ ] Redis rodando (se necessÃ¡rio)
- [ ] Backend nÃ£o estÃ¡ rodando (para testes isolados)

### ConfiguraÃ§Ã£o
- [ ] Arquivo `.env` configurado (se necessÃ¡rio)
- [ ] `config/agent_config.yaml` com modelo `phi:latest`
- [ ] DiretÃ³rios de log existem (`logs/`, `data/test_reports/`)

---

## ğŸš¨ Troubleshooting Comum

### Ollama nÃ£o responde
```bash
# Verificar se Ollama estÃ¡ rodando
curl http://localhost:11434/api/tags

# Se nÃ£o estiver, iniciar Ollama
ollama serve
```

### Modelo phi:latest nÃ£o encontrado
```bash
# Baixar modelo phi:latest
ollama pull phi:latest

# Verificar modelos disponÃ­veis
ollama list
```

### Erros de GPU/CUDA
```bash
# Verificar CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Verificar variÃ¡veis de ambiente
echo $CUDA_VISIBLE_DEVICES
echo $CUDA_HOME
```

### Testes falhando com timeout
- Verificar se servidor nÃ£o estÃ¡ rodando (conflito de porta)
- Verificar se GPU estÃ¡ disponÃ­vel (alguns testes requerem GPU)
- Verificar logs em `data/test_reports/` para detalhes

---

## ğŸ“š ReferÃªncias

- **Quick Start**: `docs/canonical/QUICK_START.md`
- **System Initialization**: `docs/canonical/omnimind_system_initialization.md`
- **Safe Commands**: `docs/canonical/SAFE_COMMANDS.md`
- **Architecture**: `docs/canonical/omnimind_architecture_reference.md`

---

**Autor**: FabrÃ­cio da Silva + assistÃªncia de IA (Copilot GitHub/Cursor/Gemini/Perplexity)
