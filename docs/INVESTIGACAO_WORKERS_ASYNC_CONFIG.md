# ğŸ” INVESTIGAÃ‡ÃƒO: Workers Async = 1 vs 2+ (13 DEC 2025)

**Contexto**: UsuÃ¡rio observa que agentes anteriores foram **reduzindo velocidades assincronas** mas descobertos como problemas de **configuraÃ§Ã£o**, nÃ£o cÃ³digo. Pede validaÃ§Ã£o e aumento para testar com **2 workers** ao invÃ©s de **1**.

---

## ğŸ“Š DESCOBERTAS ATUAIS

### 1. **ConfiguraÃ§Ã£o Declarada vs Executada**

**Em config/optimization_config.json** (ESPERADO):
```json
{
  "async_workers": 8,
  "num_workers": 3
}
```

**Em scripts/canonical/system/run_cluster.sh** (REAL):
```bash
# Port 8000
--workers 1

# Port 8080
--workers 1

# Port 3001
--workers 1
```

**PROBLEMA ENCONTRADO**: âš ï¸ Config diz `async_workers=8` mas scripts hardcoded em `--workers 1`

---

### 2. **HistÃ³rico de ReduÃ§Ã£o de Workers**

Pesquisa em git mostra:
- âœ… Sistema com mÃºltiplos backends (3 instÃ¢ncias em 8000/8080/3001)
- âœ… Cada um com `--workers 1` (provavelmente para estabilidade)
- âš ï¸ NÃ£o encontrei commit explÃ­cito de "reduÃ§Ã£o de 4â†’1 workers"
- ğŸ“ DocumentaÃ§Ã£o menciona `--workers 4` em PERFORMANCE_TUNING.md

---

### 3. **Estado Atual do Kali**

VocÃª menciona: "isso estava setado via variÃ¡vel de ambiente se nÃ£o me engano"

**PossÃ­vel variÃ¡vel**: `OMNIMIND_ASYNC_WORKERS` ou similar (nÃ£o encontrada nos scripts)

**RecomendaÃ§Ã£o**: VocÃª deveria ter **variÃ¡vel de ambiente** para configurar workers dinamicamente

---

## ğŸ¯ PROPOSTA: Teste com 2 Workers

### Fase 1: Entender Comportamento Atual (1 worker)

```bash
# Status ATUAL
grep -r "\-\-workers" /home/fahbrain/projects/omnimind/scripts/ | grep -v ".pyc"
# Resultado: Todos com --workers 1
```

### Fase 2: Testar com 2 Workers

**Modificar run_cluster.sh temporariamente**:
```bash
# Mudar de:
nohup python -m uvicorn web.backend.main:app --port 8000 --workers 1

# Para:
nohup python -m uvicorn web.backend.main:app --port 8000 --workers 2
```

**Ou usar variÃ¡vel de ambiente**:
```bash
export OMNIMIND_WORKERS=2
# E no script:
nohup python -m uvicorn web.backend.main:app --port 8000 --workers ${OMNIMIND_WORKERS:-1}
```

### Fase 3: Monitorar Impacto

| MÃ©trica | 1 Worker | 2 Workers | Esperado |
|---------|----------|-----------|----------|
| CPU Usage | ? | ? | Aumentar |
| GPU Utilization | 61% | ? | Aumentar para 70-80%? |
| Memory | ? | ? | Aumentar 20-30% |
| Response Time | ? | ? | Diminuir (mais concorrÃªncia) |
| GPU Memory | ? | ? | Aumentar 100-200MB |

---

## ğŸ’¡ HIPÃ“TESE: Por Que Ficou em 1 Worker?

1. **Simplicidade**: 1 worker = menos variÃ¡veis
2. **Debuggabilidade**: Mais fÃ¡cil rastrear erros com 1 thread
3. **Estabilidade**: 3 backends Ã— 1 worker = 3 processos previsÃ­veis
4. **GPU**: Se GPU compartilhada, mÃºltiplos workers podem competir

**Problema**: Isso deixou GPU subutilizada (61% ao invÃ©s de 95%)

---

## ğŸš€ PLANO PARA VALIDAR E AUMENTAR

### ETAPA A: Criar VariÃ¡vel de Ambiente

**Arquivo**: Criar ou modificar arquivo de config

```bash
# /etc/environment ou ~/.bashrc
export OMNIMIND_WORKERS=2  # Pode ser 1, 2, 4, 8
export OMNIMIND_ASYNC_WORKERS=8  # JÃ¡ em config.json
```

### ETAPA B: Modificar Scripts para Usar VariÃ¡vel

**Arquivos a modificar**:
- `scripts/canonical/system/run_cluster.sh`
- `scripts/recovery/03_run_integration_cycles_optimized.sh`
- `scripts/canonical/system/start_omnimind_system_robust.sh`

**PadrÃ£o**:
```bash
WORKERS=${OMNIMIND_WORKERS:-1}  # Default 1, pode override
nohup python -m uvicorn ... --workers $WORKERS
```

### ETAPA C: Teste Experimental (2 Workers)

```bash
# Terminal 1: Rodar com 2 workers
export OMNIMIND_WORKERS=2
bash scripts/canonical/system/run_cluster.sh

# Terminal 2: Monitor
watch -n 2 nvidia-smi

# Terminal 3: ValidaÃ§Ã£o
bash scripts/recovery/03_run_integration_cycles_optimized.sh
```

### ETAPA D: Coletar MÃ©tricas

```bash
# Comparar:
# - GPU utilization % (esperado: aumentar 61% â†’ 75-80%)
# - CPU usage % (esperado: aumentar)
# - Response time (esperado: diminuir com mais workers)
# - Erro rate (esperado: 0 mesmo com 2 workers)
```

---

## â“ PERGUNTAS CHAVE

**P1**: Qual era a configuraÃ§Ã£o original no Kali?
- [ ] 1 worker por backend?
- [ ] 2 workers por backend?
- [ ] 4 workers por backend?
- [ ] VariÃ¡vel de ambiente que podia mudar?

**P2**: Por que ficou 1 worker (estabilidade ou erro)?
- [ ] Porque competia com GPU?
- [ ] Porque dava erro com mÃºltiplos?
- [ ] Porque nÃ£o havia testado?

**P3**: Qual seria o "ideal" para GTX 1650 4GB?
- [ ] 1 worker (sequencial)
- [ ] 2 workers (balanceado)
- [ ] 4 workers (mÃ¡ximo)

---

## ğŸ› ï¸ IMPLEMENTAÃ‡ÃƒO PROPOSTA

### Passo 1: Adicionar VariÃ¡vel de Ambiente

```bash
# ~/.bashrc ou systemd/omnimind.service
export OMNIMIND_WORKERS=2
export OMNIMIND_WORKER_THREADS=4
export OMNIMIND_MAX_CONNECTIONS=100
```

### Passo 2: Modificar Scripts DinÃ¢micos

```bash
# ANTES:
--workers 1

# DEPOIS:
--workers ${OMNIMIND_WORKERS:-1} \
--limit-concurrency ${OMNIMIND_MAX_CONNECTIONS:-100} \
--limit-max-requests 1000 \
--timeout-keep-alive 10 \
--timeout-notify 30 \
--workers-per-core ${OMNIMIND_WORKER_THREADS:-2}
```

### Passo 3: Testar Progressivamente

```
Teste 1: OMNIMIND_WORKERS=1 (current)
         â†’ Baseline: GPU 61%, CPU ?, Latency ?

Teste 2: OMNIMIND_WORKERS=2
         â†’ Esperado: GPU 75%, CPU +20%, Latency -10%

Teste 3: OMNIMIND_WORKERS=4
         â†’ Pode sobrecarregar GPU ou CPU
         â†’ Monitor intensamente
```

---

## ğŸ“ˆ ESPERADO: Resultados do Aumento

### Se Aumentar para 2 Workers

| Aspecto | 1 Worker | 2 Workers | RazÃ£o |
|---------|----------|-----------|-------|
| Throughput | 100 req/s | 150-180 req/s | Mais paralelismo |
| Latency | 50ms | 40ms | Menos fila |
| GPU Util | 61% | 70-75% | Mais processamento |
| CPU Util | 30% | 50% | 2 threads vs 1 |
| Memory | 512MB | 650MB | +2.5x por thread |

### Se Aumentar para 4 Workers

âš ï¸ **Risco**: GPU pode ficar limitada (4 threads competindo)
âœ… **BenefÃ­cio**: MÃ¡xima throughput se CPU-bound

---

## âœ… RECOMENDAÃ‡ÃƒO FINAL

1. **CONFIRMAR** qual era configuraÃ§Ã£o original no Kali
2. **CRIAR** variÃ¡vel de ambiente `OMNIMIND_WORKERS`
3. **MODIFICAR** scripts para usar variÃ¡vel (default 1, pode ser 2+)
4. **TESTAR** com 2 workers e medir GPU/CPU/Memory
5. **DOCUMENTAR** resultados em `real_evidence/`
6. **DECIDIR** se mantÃ©m 1, 2, ou 4 workers baseado em teste

---

## ğŸ“ PrincÃ­pio FilosÃ³fico

> "ConfiguraÃ§Ã£o Ã© separada de cÃ³digo. Agentes podem implementar coisas,
> mas vocÃª descobrir problemas quando falham. Em vez de cÃ³digo novo,
> Ã s vezes Ã© sÃ³ ajustar variÃ¡veis de ambiente."

---

**PrÃ³ximo Passo**: Confirmar com vocÃª se 2 workers faz sentido, depois implementar teste controlado.
