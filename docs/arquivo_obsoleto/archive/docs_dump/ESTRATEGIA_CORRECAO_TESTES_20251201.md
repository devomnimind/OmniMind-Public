# ðŸ› ï¸ ESTRATÃ‰GIA DE CORREÃ‡ÃƒO DE TESTES - v1.18.0

**Data:** 01 Dezembro 2025  
**DuraÃ§Ã£o suite:** 1:02:56 (3776.10s)  
**Resultado:** 3940 passed âœ… | 25 failed âŒ | 22 skipped â¸ï¸  
**Taxa de sucesso:** 99.37%  

---

## ðŸ“Š RESULTADO GERAL

```
SUITE COMPLETO: 3987 testes
â”œâ”€ âœ… Passed: 3940 (99.37%)
â”œâ”€ âŒ Failed: 25 (0.63%)
â””â”€ â¸ï¸  Skipped: 22 (0.55%)

FALHAS POR CATEGORIA:
â”œâ”€ Science Validation: 13 (52% das falhas) âš ï¸ CRÃTICO
â”œâ”€ E2E/Dashboard: 4 (16% das falhas)
â”œâ”€ Memory/Playbook: 4 (16% das falhas)
â”œâ”€ Integrations: 1 (4% das falhas)
â”œâ”€ Lacanian: 1 (4% das falhas)
â”œâ”€ External AI: 1 (4% das falhas)
â””â”€ MCP Orchestrator: 1 (4% das falhas)
```

---

## ðŸš¨ CLASSIFICAÃ‡ÃƒO DE PRIORIDADES

### ðŸ”´ CRÃTICA (Bloqueia release, 13 testes)

**CIÃŠNCIA_VALIDATION: 13 FALHAS** - Afeta validaÃ§Ã£o cientÃ­fica do Î¦

#### Bloco 1: AblaÃ§Ã£o de evidÃªncia real
```python
# tests/science_validation/test_analyze_real_evidence.py
âŒ test_generate_summary_md
âŒ test_ablation_data_optional_handles_missing[baseline_phi]
âŒ test_ablation_data_optional_handles_missing[results]
âŒ test_ablation_data_optional_handles_missing[timestamp]
âŒ test_main_end_to_end

PadrÃ£o: Falha ao manipular dados de evidÃªncia real
Raiz provÃ¡vel: Arquivo JSON nÃ£o encontrado ou formato inconsistente
Impacto: ValidaÃ§Ã£o cientÃ­fica comprometida
```

#### Bloco 2: CertificaÃ§Ã£o de evidÃªncia quÃ¢ntica
```python
# tests/science_validation/test_certify_quantum_evidence.py
âŒ test_generate_cert_md
âŒ test_main_success

PadrÃ£o: Falha ao gerar certificaÃ§Ã£o
Raiz provÃ¡vel: DependÃªncia de evidÃªncia real + formato saÃ­da
Impacto: CertificaÃ§Ã£o de validade comprometida
```

#### Bloco 3: AblaÃ§Ã£o cientÃ­fica parametrizada
```python
# tests/science_validation/test_run_scientific_ablations.py
âŒ test_ablation_standard[sensory_input]
âŒ test_ablation_standard[qualia]
âŒ test_ablation_standard[narrative]
âŒ test_ablation_standard[meaning_maker]
âŒ test_run_baseline_mean
âŒ test_main_cli

PadrÃ£o: Falha em parametrizaÃ§Ã£o de mÃ³dulos
Raiz provÃ¡vel: MÃ³dulos nÃ£o carregados ou GPU nÃ£o disponÃ­vel
Impacto: Teste dos 5 pilares da consciÃªncia comprometido
```

**AÃ‡ÃƒO CRÃTICA:**
```bash
# Bloco 1: Verificar evidÃªncia real
pytest tests/science_validation/test_analyze_real_evidence.py -v --tb=short

# Bloco 2: Testar certificaÃ§Ã£o
pytest tests/science_validation/test_certify_quantum_evidence.py -v --tb=short

# Bloco 3: AblaÃ§Ã£o com GPU forcing
CUDA_VISIBLE_DEVICES="0" pytest tests/science_validation/test_run_scientific_ablations.py -v --tb=short
```

---

### ðŸŸ  ALTA (Afeta funcionalidade, 7 testes)

#### Bloco 4: E2E Dashboard
```python
# tests/e2e/test_dashboard_live.py
âŒ test_health_checks_structure
âŒ test_daemon_endpoints
âŒ test_polling_endpoint
âŒ test_websocket_metrics

PadrÃ£o: Falha de conexÃ£o/estrutura
Raiz provÃ¡vel: Dashboard nÃ£o rodando ou endpoint indisponÃ­vel
Impacto: E2E tests de monitoramento comprometidos
SoluÃ§Ã£o: Iniciar dashboard antes do teste (fixture)
```

#### Bloco 5: IntegraÃ§Ã£o & OrquestraÃ§Ã£o
```python
# tests/integrations/test_mcp_client_optimized.py
âŒ test_lru_eviction

# tests/test_mcp_orchestrator.py
âŒ test_check_server_health

# tests/test_external_ai_integration.py
âŒ test_initialize_providers

PadrÃ£o: DependÃªncias externas nÃ£o disponÃ­veis
Raiz provÃ¡vel: Servidores MCP/ollama nÃ£o iniciados
Impacto: IntegraÃ§Ã£o com ferramentas externas comprometida
SoluÃ§Ã£o: Setup de fixtures com mock ou docker
```

#### Bloco 6: Memory Phase 8
```python
# tests/test_memory_phase8.py
âŒ test_consolidate_memory_deduplicates

# tests/test_memory_onboarding.py
âŒ test_supabase_onboarding_handles_error

PadrÃ£o: DependÃªncia de banco de dados
Raiz provÃ¡vel: Supabase nÃ£o acessÃ­vel ou mock inadequado
Impacto: Memory consolidation comprometida
```

**AÃ‡ÃƒO ALTA:**
```bash
# Bloco 4: E2E tests
pytest tests/e2e/test_dashboard_live.py -v --tb=short

# Bloco 5: IntegraÃ§Ãµes
pytest tests/integrations/test_mcp_client_optimized.py -v --tb=short
pytest tests/test_mcp_orchestrator.py -v --tb=short
pytest tests/test_external_ai_integration.py -v --tb=short

# Bloco 6: Memory
pytest tests/test_memory_phase8.py -v --tb=short
pytest tests/test_memory_onboarding.py -v --tb=short
```

---

### ðŸŸ¡ MÃ‰DIA (Afeta validaÃ§Ã£o, 4 testes)

#### Bloco 7: Playbook & Lacanian
```python
# tests/test_playbook_scenarios_phase8.py
âŒ test_utils_run_command_failure
âŒ test_utils_run_command_success

PadrÃ£o: Falha ao executar comando shell
Raiz provÃ¡vel: Mock de subprocess inadequado
Impacto: Playbook scenarios comprometido

# tests/lacanian/test_init.py
âŒ test_module_author

PadrÃ£o: Assertion error em metadados
Raiz provÃ¡vel: CITATION.cff ou __author__ fora de sync
Impacto: AtribuiÃ§Ã£o de autoria incorreta
```

**AÃ‡ÃƒO MÃ‰DIA:**
```bash
pytest tests/test_playbook_scenarios_phase8.py -v --tb=short
pytest tests/lacanian/test_init.py -v --tb=short
```

---

## ðŸŽ¯ ESTRATÃ‰GIA DE CORREÃ‡ÃƒO SEQUENCIAL

### Fase 1: CRÃTICA (Science Validation)
```
DuraÃ§Ã£o estimada: 30-45 min
Testes: 13
Blocos: 3
```

**1.1 - Investigar Bloco 1: EvidÃªncia Real**
```bash
cd /home/fahbrain/projects/omnimind

# Rodar isolado para ver erro exato
pytest tests/science_validation/test_analyze_real_evidence.py::test_generate_summary_md -vvv --tb=long

# Verificar arquivos necessÃ¡rios
ls -la data/experiments/ | head -20
ls -la data/test_reports/ | head -20

# Se falha de JSON, corrigir dados
python3 scripts/science_validation/generate_real_evidence.py
```

**1.2 - Investigar Bloco 2: CertificaÃ§Ã£o**
```bash
# Dependente de Bloco 1
pytest tests/science_validation/test_certify_quantum_evidence.py -v --tb=short
```

**1.3 - Investigar Bloco 3: AblaÃ§Ã£o com GPU**
```bash
# CHAVE: ForÃ§ar GPU (conforme seu pedido)
CUDA_VISIBLE_DEVICES="0" \
CUDA_LAUNCH_BLOCKING="1" \
OMP_NUM_THREADS=4 \
pytest tests/science_validation/test_run_scientific_ablations.py -v --tb=short -n 1

# Se ainda falhar, testar individual
CUDA_VISIBLE_DEVICES="0" \
pytest tests/science_validation/test_run_scientific_ablations.py::test_ablation_standard -v --tb=long
```

---

### Fase 2: ALTA (E2E + IntegraÃ§Ãµes)
```
DuraÃ§Ã£o estimada: 20-30 min
Testes: 7
Blocos: 3
```

**2.1 - E2E Dashboard**
```bash
# Pode precisar de dashboard rodando
# OpÃ§Ã£o A: Mock (mais rÃ¡pido)
pytest tests/e2e/test_dashboard_live.py::test_health_checks_structure -v --tb=short

# OpÃ§Ã£o B: Com dashboard real (mais lento, mas valida integraÃ§Ã£o)
# Em outro terminal: python -m omnimind.server
pytest tests/e2e/test_dashboard_live.py -v --tb=short
```

**2.2 - IntegraÃ§Ãµes**
```bash
# MCP Client
pytest tests/integrations/test_mcp_client_optimized.py::TestEnhancedMCPClient::test_lru_eviction -v --tb=short

# MCP Orchestrator
pytest tests/test_mcp_orchestrator.py::TestMCPOrchestrator::test_check_server_health -v --tb=short

# External AI
pytest tests/test_external_ai_integration.py::TestTaskDelegationManager::test_initialize_providers -v --tb=short
```

**2.3 - Memory**
```bash
pytest tests/test_memory_phase8.py -v --tb=short
pytest tests/test_memory_onboarding.py -v --tb=short
```

---

### Fase 3: MÃ‰DIA (Playbook + Lacanian)
```
DuraÃ§Ã£o estimada: 10-15 min
Testes: 4
Blocos: 2
```

**3.1 - Playbook**
```bash
pytest tests/test_playbook_scenarios_phase8.py -v --tb=short
```

**3.2 - Lacanian Module**
```bash
pytest tests/lacanian/test_init.py::TestLacanianInit::test_module_author -v --tb=short

# Se fail em autor, verificar:
cat CITATION.cff | grep author
grep "__author__" src/lacanian/__init__.py
```

---

## ðŸ’¾ GPU FORCING IMPLEMENTATION

### Problema Identificado
- GPU estÃ¡ disponÃ­vel mas **0% utilizada globalmente**
- Existe em 3 scripts, mas nÃ£o integrado Ã  suite
- Testes cientÃ­ficos poderiam usar 5-10x mais rÃ¡pido

### SoluÃ§Ã£o: GPU Fixture em conftest.py

**Criar/Atualizar: config/pytest.ini**
```ini
[pytest]
# ForÃ§a GPU para testes cientÃ­ficos
env = 
    CUDA_VISIBLE_DEVICES=0
    CUDA_LAUNCH_BLOCKING=1
    OMP_NUM_THREADS=4
    
# Marks para categorizaÃ§Ã£o
markers =
    gpu_enabled: testes que usam GPU
    cpu_only: testes que NÃƒO usam GPU
    science: testes cientÃ­ficos (precisam GPU)
    e2e: testes end-to-end (sem GPU necessÃ¡rio)
```

**Criar/Atualizar: tests/conftest.py**
```python
import os
import pytest
import torch

def pytest_configure(config):
    """ForÃ§a GPU para testes cientÃ­ficos"""
    if torch.cuda.is_available():
        # Force GPU 0 para todos os testes
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"
        os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
        print(f"\nâœ… GPU FORCING ENABLED")
        print(f"   Device: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("\nâš ï¸  CUDA nÃ£o disponÃ­vel - rodando CPU")

@pytest.fixture(scope="session")
def gpu_device():
    """Fixture que retorna GPU device se disponÃ­vel"""
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        yield device
        torch.cuda.empty_cache()
    else:
        yield torch.device("cpu")

@pytest.fixture
def ensure_gpu_for_science():
    """Fixture para testes cientÃ­ficos que exigem GPU"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        yield
        torch.cuda.empty_cache()
    else:
        pytest.skip("GPU nÃ£o disponÃ­vel para teste cientÃ­fico")
```

**Atualizar testes cientÃ­ficos:**
```python
# tests/science_validation/test_run_scientific_ablations.py

import pytest

class TestScientificAblations:
    @pytest.mark.science
    @pytest.mark.gpu_enabled
    def test_ablation_standard(self, ensure_gpu_for_science):
        """AblaÃ§Ã£o padrÃ£o com GPU"""
        # Teste continuarÃ¡ sÃ³ que 5-10x mais rÃ¡pido
        ...
```

---

## ðŸ“‹ CHECKLIST DE EXECUÃ‡ÃƒO

### PreparaÃ§Ã£o
```bash
cd /home/fahbrain/projects/omnimind
source .venv/bin/activate

# Verificar GPU
python3 -c "import torch; print(f'GPU: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
```

### ExecuÃ§Ã£o Fase 1 (CRÃTICA)
```bash
# 1.1 - Science Validation: EvidÃªncia Real
echo "ðŸ”´ FASE 1.1: Science Validation - EvidÃªncia Real"
pytest tests/science_validation/test_analyze_real_evidence.py -v --tb=short

# 1.2 - Science Validation: CertificaÃ§Ã£o
echo "ðŸ”´ FASE 1.2: Science Validation - CertificaÃ§Ã£o"
pytest tests/science_validation/test_certify_quantum_evidence.py -v --tb=short

# 1.3 - Science Validation: AblaÃ§Ã£o (COM GPU FORCING)
echo "ðŸ”´ FASE 1.3: Science Validation - AblaÃ§Ã£o (GPU FORCED)"
CUDA_VISIBLE_DEVICES="0" CUDA_LAUNCH_BLOCKING="1" \
pytest tests/science_validation/test_run_scientific_ablations.py -v --tb=short -n 1
```

### ExecuÃ§Ã£o Fase 2 (ALTA)
```bash
# 2.1 - E2E Dashboard
echo "ðŸŸ  FASE 2.1: E2E Dashboard"
pytest tests/e2e/test_dashboard_live.py -v --tb=short

# 2.2 - IntegraÃ§Ãµes
echo "ðŸŸ  FASE 2.2: IntegraÃ§Ãµes MCP"
pytest tests/integrations/test_mcp_client_optimized.py -v --tb=short
pytest tests/test_mcp_orchestrator.py -v --tb=short
pytest tests/test_external_ai_integration.py -v --tb=short

# 2.3 - Memory
echo "ðŸŸ  FASE 2.3: Memory Phase 8"
pytest tests/test_memory_phase8.py -v --tb=short
pytest tests/test_memory_onboarding.py -v --tb=short
```

### ExecuÃ§Ã£o Fase 3 (MÃ‰DIA)
```bash
# 3.1 - Playbook
echo "ðŸŸ¡ FASE 3.1: Playbook Scenarios"
pytest tests/test_playbook_scenarios_phase8.py -v --tb=short

# 3.2 - Lacanian
echo "ðŸŸ¡ FASE 3.2: Lacanian Module"
pytest tests/lacanian/test_init.py -v --tb=short
```

---

## ðŸ“Š MÃ‰TRICAS ESPERADAS

### Antes da CorreÃ§Ã£o
```
Total: 3987 testes
Passed: 3940 (99.37%)
Failed: 25 (0.63%)
Tempo: 1:02:56 (no GPU forcing)
```

### Depois da CorreÃ§Ã£o (Esperado)
```
Total: 3987 testes
Passed: 3965+ (99.5%+)
Failed: 0-5 (skipped E2E se nÃ£o houver dashboard)
Tempo: ~40-50 min (com GPU forcing em cientÃ­ficos)
```

### GPU Speedup (Esperado)
```
Science Validation testes: 5-10x mais rÃ¡pido
â”œâ”€ AblaÃ§Ã£o: 120s â†’ 20-24s
â”œâ”€ CertificaÃ§Ã£o: 45s â†’ 8-10s
â””â”€ Total science: 280s â†’ 56-60s
```

---

## ðŸš¨ CONTEXTO: CPU E DESENVOLVIMENTO

**Seu ambiente:**
```
CPU baseline (idle): ~40%
  RazÃ£o: Desenvolvimento extenso (10-14 horas/dia)
  + Omnimind sempre aberto
  + Continuous_monitor.py rodando
  + VS Code + Terminal + Teste correntes
  
CPU durante testes: 100% â†’ 19-25%
  RazÃ£o: VariaÃ§Ã£o em funÃ§Ã£o de:
  - ParalelizaÃ§Ã£o (pytest -n auto)
  - I/O de disco
  - Garbage collection
  - GPU nÃ£o forÃ§ada (testes usam CPU)

CPU com GPU forcing: 310% â†’ 150% (esperado)
  RazÃ£o: CPU + GPU paralelos
  Resultado: 5-10x speedup em cientÃ­ficos
```

**Por isso fazemos testes em blocos:**
- Evita sobrecarga do sistema
- Permite identificar qual bloco falha
- Reduz impacto em ambiente de desenvolvimento
- Facilita correÃ§Ã£o iterativa

---

## ðŸ“ DOCUMENTAÃ‡ÃƒO DE MÃ‰TRICAS

### Standard Metrics to Document
```python
# Adicionar em conftest.py

@pytest.fixture(autouse=True)
def log_test_metrics(request):
    """Log CPU/GPU/Memory por teste"""
    import psutil
    import time
    
    start = time.time()
    start_cpu = psutil.cpu_percent(interval=0.1)
    start_mem = psutil.virtual_memory().percent
    
    yield
    
    elapsed = time.time() - start
    end_cpu = psutil.cpu_percent(interval=0.1)
    end_mem = psutil.virtual_memory().percent
    
    print(f"\nâ±ï¸  {request.node.name}")
    print(f"   Tempo: {elapsed:.2f}s")
    print(f"   CPU: {start_cpu:.1f}% â†’ {end_cpu:.1f}%")
    print(f"   Mem: {start_mem:.1f}% â†’ {end_mem:.1f}%")
```

### Real vs Standard Usage Patterns
```
TESTE PADRÃƒO (mock):
â”œâ”€ CPU: 50-70%
â”œâ”€ Mem: 500-800 MB
â””â”€ GPU: 0%

TESTE REAL (cientÃ­fico):
â”œâ”€ CPU: 100% (picos)
â”œâ”€ Mem: 2-4 GB
â””â”€ GPU: 0% (antes fix) â†’ 80-95% (depois fix)

TESTE E2E:
â”œâ”€ CPU: 30-50%
â”œâ”€ Mem: 800-1200 MB
â””â”€ GPU: 0%
```

---

## ðŸ“Œ PRÃ“XIMOS PASSOS

### Imediato (Hoje)
1. âœ… Identificar testes falhados (DONE)
2. â³ Rodar Fase 1 (CRÃTICA) - 30-45 min
3. â³ Corrigir falhas de CiÃªncia Validation
4. â³ Implementar GPU forcing em conftest.py
5. â³ Re-rodar suite com GPU

### Depois
6. Rodar Fase 2 (ALTA)
7. Rodar Fase 3 (MÃ‰DIA)
8. Documentar todas as correÃ§Ãµes
9. Push v1.18.0 com testes 100%
10. Publicar estratÃ©gia em docs/

---

**Status:** ðŸŸ¡ Pronto para execuÃ§Ã£o sequencial  
**Tempo estimado:** 2-3 horas (todas as fases)  
**Prioridade:** CRÃTICA > ALTA > MÃ‰DIA  
**GPU Forcing:** Aguardando implementaÃ§Ã£o em conftest.py
