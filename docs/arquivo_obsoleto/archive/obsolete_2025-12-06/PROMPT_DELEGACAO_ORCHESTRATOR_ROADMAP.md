# üéØ PROMPT DE DELEGA√á√ÉO - ORCHESTRATOR ROADMAP (SE√á√ïES 4, 5, 8)

**Data**: 6 de dezembro de 2025
**Para**: Agente Remoto / Equipe de Desenvolvimento
**Contexto**: Implementa√ß√£o das 3 se√ß√µes pendentes do Orchestrator (PR #82)
**Tempo Estimado**: 150-180 horas de desenvolvimento + testes
**Prioridade**: ALTA

---

## üìã INSTRU√á√ÉO GERAL

Este prompt descreve a implementa√ß√£o de 3 componentes cr√≠ticos do Orchestrator:
1. **SE√á√ÉO 4**: Power States (Ociosidade e Otimiza√ß√£o)
2. **SE√á√ÉO 5**: Permission Matrix (Autonomia)
3. **SE√á√ÉO 8**: Sandbox de Auto-Melhoria

Cada se√ß√£o √© independente mas se beneficiam da integra√ß√£o. Recomenda-se implementar na ordem: 4 ‚Üí 5 ‚Üí 8.

---

## ‚úÖ PR√â-REQUISITOS

Antes de come√ßar, validar:

```bash
# 1. Verificar que PR #82 foi merged
git log --oneline | grep "orchestrator" | head -5

# 2. Verificar arquivos essenciais existem
ls -la src/orchestrator/agent_registry.py
ls -la src/orchestrator/event_bus.py
ls -la src/orchestrator/circuit_breaker.py
ls -la tests/orchestrator/

# 3. Validar testes passam
pytest tests/orchestrator/ -v --tb=short

# 4. Validar produ√ß√£o est√° saud√°vel
curl -s http://127.0.0.1:8000/health | python -m json.tool
```

Se tudo estiver OK, prosseguir com as se√ß√µes abaixo.

---

# üîß SE√á√ÉO 4: POWER STATES (Ociosidade e Otimiza√ß√£o)

## üìä Especifica√ß√£o

### Estados de Energia

```python
class PowerState(Enum):
    """Estados de energia do sistema."""
    IDLE = "idle"          # Repouso total, apenas servi√ßos cr√≠ticos
    STANDBY = "standby"    # Preparado, servi√ßos leves
    ACTIVE = "active"      # Opera√ß√£o normal, todos os servi√ßos
    CRITICAL = "critical"  # Emergencial, m√°ximos recursos
```

### Categoriza√ß√£o de Servi√ßos

```python
SERVICE_CATEGORIES = {
    "critical": [
        "security",        # Sempre ativo (seguran√ßa)
        "metacognition",   # Sempre ativo (auto-awareness)
    ],
    "essential": [
        "orchestrator",    # Coordena√ß√£o central
    ],
    "standard": [
        "code",           # Desenvolvimento
        "architect",      # Arquitetura
        "reviewer",       # Code review
    ],
    "optional": [
        "psychoanalyst",  # An√°lise profunda
        "debug",          # Debugging
    ]
}
```

### Consumo de Recursos por Estado

| Estado | CPU | Mem√≥ria | Status | Tempo Ativa√ß√£o |
|--------|-----|---------|--------|----------------|
| IDLE | <5% | 100MB | Apenas cr√≠ticos | N/A |
| STANDBY | 10% | 256MB | Cr√≠ticos + essenciais | ~5s (warm cache) |
| ACTIVE | 30% | 512MB | Todos os servi√ßos | ~2s |
| CRITICAL | 100% | 1024MB | M√°ximos recursos | <1s (pr√©-carregado) |

### Transi√ß√µes de Estado

```
IDLE
  ‚Üì (usuario solicita a√ß√£o)
STANDBY
  ‚Üì (detec√ß√£o de carga)
ACTIVE
  ‚Üì (amea√ßa detectada)
CRITICAL
  ‚Üë (crise resolvida)
ACTIVE
  ‚Üë (inatividade por N minutos)
STANDBY
  ‚Üë (modo economizador ativo)
IDLE
```

---

## üíª Implementa√ß√£o Detalhada

### Arquivo: `src/orchestrator/power_manager.py`

```python
"""
Power Manager para Orchestrator.

Responsabilidades:
1. Gerenciar transi√ß√µes entre estados
2. Ativar/desativar servi√ßos por categoria
3. Monitorar tempo de inatividade
4. Coordenar preheating
"""

from enum import Enum
from typing import Dict, List, Set
from datetime import datetime, timedelta
import asyncio
import logging

logger = logging.getLogger(__name__)

class PowerState(Enum):
    """Estados de energia do Orchestrator."""
    IDLE = "idle"
    STANDBY = "standby"
    ACTIVE = "active"
    CRITICAL = "critical"

class ServiceCategory(Enum):
    """Categorias de servi√ßo."""
    CRITICAL = "critical"
    ESSENTIAL = "essential"
    STANDARD = "standard"
    OPTIONAL = "optional"

class PowerManager:
    """Gerencia estados de energia e recursos do Orchestrator."""

    def __init__(self, orchestrator):
        """
        Inicializa Power Manager.

        Args:
            orchestrator: Inst√¢ncia do OrchestratorAgent
        """
        self.orchestrator = orchestrator
        self.current_state = PowerState.ACTIVE
        self.last_activity = datetime.now()
        self.idle_threshold = timedelta(minutes=5)  # Entrar em IDLE ap√≥s 5min

        # Mapeamento de qual estado ativa quais categorias
        self.state_services = {
            PowerState.IDLE: [ServiceCategory.CRITICAL],
            PowerState.STANDBY: [
                ServiceCategory.CRITICAL,
                ServiceCategory.ESSENTIAL,
            ],
            PowerState.ACTIVE: [
                ServiceCategory.CRITICAL,
                ServiceCategory.ESSENTIAL,
                ServiceCategory.STANDARD,
            ],
            PowerState.CRITICAL: [
                ServiceCategory.CRITICAL,
                ServiceCategory.ESSENTIAL,
                ServiceCategory.STANDARD,
                ServiceCategory.OPTIONAL,
            ],
        }

        # Mapeamento de agentes para categorias
        self.agent_categories = {
            "security": ServiceCategory.CRITICAL,
            "metacognition": ServiceCategory.CRITICAL,
            "orchestrator": ServiceCategory.ESSENTIAL,
            "code": ServiceCategory.STANDARD,
            "architect": ServiceCategory.STANDARD,
            "reviewer": ServiceCategory.STANDARD,
            "psychoanalyst": ServiceCategory.OPTIONAL,
            "debug": ServiceCategory.OPTIONAL,
        }

    async def record_activity(self):
        """Registra atividade recente."""
        self.last_activity = datetime.now()

        # Se estava em IDLE/STANDBY, transicionar para ACTIVE
        if self.current_state in [PowerState.IDLE, PowerState.STANDBY]:
            await self.transition_to(PowerState.ACTIVE)

    async def transition_to(self, target_state: PowerState):
        """
        Transi√ß√£o para estado alvo.

        Args:
            target_state: Estado desejado
        """
        if target_state == self.current_state:
            return

        logger.info(f"Power transition: {self.current_state.value} ‚Üí {target_state.value}")

        # Desativar agentes n√£o necess√°rios
        agents_to_deactivate = self._get_agents_to_deactivate(target_state)
        for agent_name in agents_to_deactivate:
            await self.orchestrator._deactivate_agent(agent_name)

        # Ativar agentes necess√°rios
        agents_to_activate = self._get_agents_to_activate(target_state)
        for agent_name in agents_to_activate:
            await self.orchestrator._ensure_agent_active(agent_name)

        self.current_state = target_state
        logger.info(f"Power state: {target_state.value} (Active agents: {self._get_active_agents()})")

    def _get_agents_to_deactivate(self, target_state: PowerState) -> Set[str]:
        """Retorna agentes que devem ser desativados."""
        current_active = set(self.agent_categories.keys())

        # Agentes que devem ficar ativos no estado alvo
        target_categories = self.state_services[target_state]
        target_active = {
            name for name, cat in self.agent_categories.items()
            if cat in target_categories
        }

        return current_active - target_active

    def _get_agents_to_activate(self, target_state: PowerState) -> Set[str]:
        """Retorna agentes que devem ser ativados."""
        target_categories = self.state_services[target_state]
        return {
            name for name, cat in self.agent_categories.items()
            if cat in target_categories
        }

    def _get_active_agents(self) -> List[str]:
        """Retorna lista de agentes ativos."""
        target_categories = self.state_services[self.current_state]
        return [
            name for name, cat in self.agent_categories.items()
            if cat in target_categories
        ]

    async def monitor_idle_time(self):
        """
        Monitora tempo de inatividade.
        Deve ser executado periodicamente (recomendado: a cada 30 segundos).
        """
        while True:
            await asyncio.sleep(30)

            # Se em ACTIVE e sem atividade por muito tempo
            if self.current_state == PowerState.ACTIVE:
                time_since_activity = datetime.now() - self.last_activity

                if time_since_activity > self.idle_threshold:
                    # Transi√ß√£o para STANDBY
                    await self.transition_to(PowerState.STANDBY)

                    # Se continuar inativo
                    await asyncio.sleep(60)  # Esperar mais 1 minuto

                    if datetime.now() - self.last_activity > self.idle_threshold + timedelta(minutes=1):
                        # Transi√ß√£o para IDLE
                        await self.transition_to(PowerState.IDLE)

    async def preheat_agents(self, agent_names: List[str]):
        """
        Aquece agentes antes de uso (compila√ß√£o, cache aquecimento, etc).

        Args:
            agent_names: Lista de nomes de agentes a aquecer
        """
        logger.info(f"Preheating agents: {agent_names}")

        for agent_name in agent_names:
            agent = self.orchestrator.registry.get_agent(agent_name)
            if agent and hasattr(agent, 'preheat'):
                try:
                    await agent.preheat()
                    logger.info(f"‚úÖ Preheated: {agent_name}")
                except Exception as e:
                    logger.error(f"‚ùå Preheat failed for {agent_name}: {e}")

    def get_metrics(self) -> Dict:
        """Retorna m√©tricas de consumo de energia."""
        return {
            "current_state": self.current_state.value,
            "active_agents": self._get_active_agents(),
            "time_since_activity_seconds": (datetime.now() - self.last_activity).total_seconds(),
            "estimated_memory_mb": self._estimate_memory(),
            "estimated_cpu_percent": self._estimate_cpu(),
        }

    def _estimate_memory(self) -> float:
        """Estima consumo de mem√≥ria baseado no estado."""
        mapping = {
            PowerState.IDLE: 100,
            PowerState.STANDBY: 256,
            PowerState.ACTIVE: 512,
            PowerState.CRITICAL: 1024,
        }
        return mapping.get(self.current_state, 512)

    def _estimate_cpu(self) -> float:
        """Estima consumo de CPU baseado no estado."""
        mapping = {
            PowerState.IDLE: 5,
            PowerState.STANDBY: 10,
            PowerState.ACTIVE: 30,
            PowerState.CRITICAL: 100,
        }
        return mapping.get(self.current_state, 30)
```

### Integra√ß√£o em `src/agents/orchestrator_agent.py`

```python
# No __init__
self.power_manager = PowerManager(self)

# No m√©todo start()
asyncio.create_task(self.power_manager.monitor_idle_time())

# Ao receber requisi√ß√£o (em qualquer handler)
await self.power_manager.record_activity()

# Novo endpoint de API
@router.get("/power-state")
async def get_power_state():
    return {
        "state": orchestrator.power_manager.get_metrics()
    }
```

---

## üß™ Testes Necess√°rios

**Arquivo**: `tests/orchestrator/test_power_manager.py`

```python
"""
Testes para PowerManager.

Cobertura:
- Transi√ß√µes entre estados
- Ativa√ß√£o/desativa√ß√£o de agentes
- Monitoramento de inatividade
- Preheating de agentes
- Estimativas de consumo
"""

import pytest
import asyncio
from datetime import datetime, timedelta
from src.orchestrator.power_manager import (
    PowerManager,
    PowerState,
    ServiceCategory,
)

@pytest.fixture
def mock_orchestrator():
    """Mock do OrchestratorAgent."""
    class MockOrchestrator:
        def __init__(self):
            self.active_agents = set()
            self.registry = MockRegistry()

        async def _deactivate_agent(self, name: str):
            self.active_agents.discard(name)

        async def _ensure_agent_active(self, name: str):
            self.active_agents.add(name)

    class MockRegistry:
        def get_agent(self, name: str):
            return MockAgent()

    class MockAgent:
        async def preheat(self):
            pass

    return MockOrchestrator()

@pytest.mark.asyncio
async def test_transition_idle_to_active(mock_orchestrator):
    """Testa transi√ß√£o de IDLE para ACTIVE."""
    pm = PowerManager(mock_orchestrator)
    pm.current_state = PowerState.IDLE

    await pm.transition_to(PowerState.ACTIVE)

    assert pm.current_state == PowerState.ACTIVE
    assert "security" in mock_orchestrator.active_agents
    assert "code" in mock_orchestrator.active_agents

@pytest.mark.asyncio
async def test_transition_active_to_idle(mock_orchestrator):
    """Testa transi√ß√£o de ACTIVE para IDLE."""
    pm = PowerManager(mock_orchestrator)
    pm.current_state = PowerState.ACTIVE
    mock_orchestrator.active_agents = {"security", "code", "architect"}

    await pm.transition_to(PowerState.IDLE)

    assert pm.current_state == PowerState.IDLE
    assert "code" not in mock_orchestrator.active_agents
    assert "architect" not in mock_orchestrator.active_agents
    assert "security" in mock_orchestrator.active_agents

@pytest.mark.asyncio
async def test_record_activity_activates_from_idle(mock_orchestrator):
    """Testa que atividade ativa agentes."""
    pm = PowerManager(mock_orchestrator)
    pm.current_state = PowerState.IDLE

    await pm.record_activity()

    assert pm.current_state == PowerState.ACTIVE

@pytest.mark.asyncio
async def test_idle_timeout(mock_orchestrator):
    """Testa transi√ß√£o autom√°tica para IDLE ap√≥s timeout."""
    pm = PowerManager(mock_orchestrator)
    pm.current_state = PowerState.ACTIVE
    pm.idle_threshold = timedelta(seconds=1)
    pm.last_activity = datetime.now() - timedelta(seconds=2)

    # Simular monitoramento (em produ√ß√£o roda em background)
    if datetime.now() - pm.last_activity > pm.idle_threshold:
        await pm.transition_to(PowerState.STANDBY)

    assert pm.current_state == PowerState.STANDBY

@pytest.mark.asyncio
async def test_preheat_agents(mock_orchestrator):
    """Testa preheating de agentes."""
    pm = PowerManager(mock_orchestrator)

    # Deve completar sem exce√ß√£o
    await pm.preheat_agents(["code", "architect"])

def test_metrics_idle(mock_orchestrator):
    """Testa m√©tricas em estado IDLE."""
    pm = PowerManager(mock_orchestrator)
    pm.current_state = PowerState.IDLE

    metrics = pm.get_metrics()

    assert metrics["current_state"] == "idle"
    assert metrics["estimated_memory_mb"] == 100
    assert metrics["estimated_cpu_percent"] == 5

def test_metrics_critical(mock_orchestrator):
    """Testa m√©tricas em estado CRITICAL."""
    pm = PowerManager(mock_orchestrator)
    pm.current_state = PowerState.CRITICAL

    metrics = pm.get_metrics()

    assert metrics["current_state"] == "critical"
    assert metrics["estimated_memory_mb"] == 1024
    assert metrics["estimated_cpu_percent"] == 100

# ... mais testes para cobertura completa
```

---

## üìã Checklist de Implementa√ß√£o

- [ ] Criar `src/orchestrator/power_manager.py` com classe PowerManager
- [ ] Definir enums PowerState e ServiceCategory
- [ ] Implementar l√≥gica de transi√ß√£o entre estados
- [ ] Implementar categoriza√ß√£o de agentes
- [ ] Implementar monitoramento de inatividade
- [ ] Implementar preheating de agentes
- [ ] Implementar m√©tricas de consumo
- [ ] Integrar PowerManager em OrchestratorAgent
- [ ] Criar endpoints de API para power state
- [ ] Criar suite de testes (20+ testes)
- [ ] Validar com black, flake8, mypy
- [ ] Testar em produ√ß√£o por 48h
- [ ] Documentar em docs/

---

# üîí SE√á√ÉO 5: PERMISSION MATRIX (Autonomia)

## üìä Especifica√ß√£o

### Matriz de Permiss√µes

```python
class PermissionLevel(Enum):
    """N√≠veis de permiss√£o."""
    AUTOMATIC = 1          # Sem aprova√ß√£o, executado imediatamente
    MANUAL_REVIEW = 2      # Requer aprova√ß√£o humana
    EMERGENCY_ONLY = 3     # Apenas em modo emergencial
    ESCALATE = 0           # Sempre escala para humano
```

### A√ß√µes com Permiss√µes

```python
NORMAL_PERMISSIONS = {
    # Delega√ß√£o (n√≠vel 1 - autom√°tico)
    "delegate_task": {
        "level": PermissionLevel.AUTOMATIC,
        "requires_approval": False,
        "max_retries": 3,
        "timeout_seconds": 300,
    },

    # Leitura (n√≠vel 1 - autom√°tico)
    "read_logs": {
        "level": PermissionLevel.AUTOMATIC,
        "requires_approval": False,
    },

    # Modifica√ß√£o de c√≥digo (n√≠vel 2 - revis√£o)
    "modify_code": {
        "level": PermissionLevel.MANUAL_REVIEW,
        "requires_approval": True,
        "approval_timeout": 3600,
        "dry_run_first": True,
    },

    # Restart de servi√ßo (n√≠vel 2 - revis√£o)
    "restart_service": {
        "level": PermissionLevel.MANUAL_REVIEW,
        "requires_approval": True,
        "backup_first": True,
    },

    # Block port (n√≠vel 2-3 - emergencial)
    "block_port": {
        "level": PermissionLevel.MANUAL_REVIEW,
        "requires_approval": True,
        "emergency_auto": True,  # Auto em emerg√™ncia
    },

    # Modificar configura√ß√£o (n√≠vel 3 - emergencial)
    "modify_config": {
        "level": PermissionLevel.EMERGENCY_ONLY,
        "requires_approval": True,
    },
}

EMERGENCY_PERMISSIONS = {
    # Em emerg√™ncia, algumas a√ß√µes s√£o autom√°ticas
    "block_port": {
        "level": PermissionLevel.AUTOMATIC,
        "requires_approval": False,
    },
    "isolate_component": {
        "level": PermissionLevel.AUTOMATIC,
        "requires_approval": False,
    },
    "escalate_to_human": {
        "level": PermissionLevel.AUTOMATIC,
        "requires_approval": False,
    },
}
```

### Sistema de Confian√ßa

```python
class TrustLevel(Enum):
    """N√≠veis de confian√ßa (0.0 a 1.0)."""
    UNTRUSTED = 0.2        # Requer aprova√ß√£o em tudo
    LOW = 0.4              # Requer aprova√ß√£o em a√ß√µes de risco
    MEDIUM = 0.6           # Aprova√ß√£o em a√ß√µes cr√≠ticas
    HIGH = 0.8             # Poucas restri√ß√µes
    MAXIMUM = 1.0          # Confian√ßa total (emerg√™ncia)
```

---

## üíª Implementa√ß√£o Detalhada

### Arquivo: `src/orchestrator/permission_matrix.py`

```python
"""
Permission Matrix para Orchestrator.

Responsabilidades:
1. Definir quais a√ß√µes o Orchestrator pode fazer autonomamente
2. Gerenciar n√≠veis de confian√ßa
3. Rastrear decis√µes e aprova√ß√µes
4. Auditar a√ß√µes executadas
"""

from enum import Enum
from typing import Dict, Any, List, Optional
from datetime import datetime
import logging
import json

logger = logging.getLogger(__name__)

class PermissionLevel(Enum):
    """N√≠veis de permiss√£o."""
    AUTOMATIC = 1
    MANUAL_REVIEW = 2
    EMERGENCY_ONLY = 3
    ESCALATE = 0

class TrustLevel(Enum):
    """N√≠veis de confian√ßa."""
    UNTRUSTED = 0.2
    LOW = 0.4
    MEDIUM = 0.6
    HIGH = 0.8
    MAXIMUM = 1.0

class Decision(Enum):
    """Resultado de uma decis√£o."""
    APPROVED = "approved"
    DENIED = "denied"
    ESCALATED = "escalated"
    PENDING = "pending"

class PermissionMatrix:
    """Gerencia permiss√µes do Orchestrator."""

    # Permiss√µes em modo normal
    NORMAL_PERMISSIONS = {
        "delegate_task": {
            "level": PermissionLevel.AUTOMATIC,
            "requires_approval": False,
            "audit": True,
        },
        "read_logs": {
            "level": PermissionLevel.AUTOMATIC,
            "requires_approval": False,
            "audit": False,
        },
        "modify_code": {
            "level": PermissionLevel.MANUAL_REVIEW,
            "requires_approval": True,
            "audit": True,
            "dry_run_first": True,
        },
        "restart_service": {
            "level": PermissionLevel.MANUAL_REVIEW,
            "requires_approval": True,
            "audit": True,
        },
        "block_port": {
            "level": PermissionLevel.MANUAL_REVIEW,
            "requires_approval": True,
            "audit": True,
            "emergency_auto": True,
        },
        "modify_config": {
            "level": PermissionLevel.MANUAL_REVIEW,
            "requires_approval": True,
            "audit": True,
        },
    }

    # Permiss√µes em modo emergencial
    EMERGENCY_PERMISSIONS = {
        "block_port": {
            "level": PermissionLevel.AUTOMATIC,
            "requires_approval": False,
            "audit": True,
        },
        "isolate_component": {
            "level": PermissionLevel.AUTOMATIC,
            "requires_approval": False,
            "audit": True,
        },
        "escalate_to_human": {
            "level": PermissionLevel.AUTOMATIC,
            "requires_approval": False,
            "audit": True,
        },
    }

    def __init__(self):
        """Inicializa matriz de permiss√µes."""
        self.emergency_mode = False
        self.trust_level = TrustLevel.MEDIUM
        self.approval_pending: Dict[str, Dict[str, Any]] = {}
        self.decision_history: List[Dict[str, Any]] = []

    def set_emergency_mode(self, enabled: bool, reason: str = ""):
        """Define modo emergencial."""
        self.emergency_mode = enabled
        if enabled:
            self.trust_level = TrustLevel.MAXIMUM
            logger.critical(f"üö® EMERGENCY MODE ENABLED: {reason}")
        else:
            self.trust_level = TrustLevel.MEDIUM
            logger.info("‚úÖ Emergency mode disabled")

    async def can_execute(self, action: str, context: Dict[str, Any] = None) -> bool:
        """
        Verifica se a√ß√£o pode ser executada.

        Args:
            action: Nome da a√ß√£o
            context: Contexto da a√ß√£o (opcional)

        Returns:
            True se pode executar, False caso contr√°rio
        """
        context = context or {}

        # Selecionar matriz correta
        perms = self.EMERGENCY_PERMISSIONS if self.emergency_mode else self.NORMAL_PERMISSIONS

        if action not in perms:
            logger.warning(f"Unknown action: {action}")
            return False

        perm = perms[action]

        # Se a√ß√£o autom√°tica
        if perm["level"] == PermissionLevel.AUTOMATIC:
            logger.info(f"‚úÖ Auto-approved: {action}")
            await self._audit_decision(action, Decision.APPROVED, context)
            return True

        # Se a√ß√£o escalada
        if perm["level"] == PermissionLevel.ESCALATE:
            logger.warning(f"‚ö†Ô∏è Escalating: {action}")
            await self._audit_decision(action, Decision.ESCALATED, context)
            await self.escalate_to_human(action, context)
            return False

        # Se requer aprova√ß√£o
        if perm["requires_approval"]:
            logger.info(f"‚è≥ Waiting approval: {action}")
            decision = await self._request_approval(action, context)
            return decision == Decision.APPROVED

        # Default: permitir
        logger.info(f"‚úÖ Permitted: {action}")
        await self._audit_decision(action, Decision.APPROVED, context)
        return True

    async def _request_approval(self, action: str, context: Dict[str, Any]) -> Decision:
        """
        Solicita aprova√ß√£o para a√ß√£o.

        Em produ√ß√£o, isso se conectaria a sistema de aprova√ß√£o humana.
        Por enquanto, simular com timeout.
        """
        request_id = f"{action}_{datetime.now().timestamp()}"

        self.approval_pending[request_id] = {
            "action": action,
            "context": context,
            "timestamp": datetime.now(),
            "status": Decision.PENDING,
        }

        logger.info(f"Approval request {request_id} created")

        # TODO: Integrar com sistema de aprova√ß√£o humana
        # Por enquanto, timeout de 1 hora
        # await asyncio.sleep(3600)
        # return Decision.PENDING

        # Simular: aprovar automaticamente para testes
        self.approval_pending[request_id]["status"] = Decision.APPROVED
        return Decision.APPROVED

    async def escalate_to_human(self, action: str, context: Dict[str, Any]):
        """
        Escala a√ß√£o para humano.

        Envia notifica√ß√£o e espera interven√ß√£o.
        """
        logger.critical(f"üö® ESCALATING TO HUMAN: {action}")

        # TODO: Integrar com sistema de notifica√ß√£o
        # Enviar email, SMS, push notification, etc.

        await self._audit_decision(action, Decision.ESCALATED, context)

    async def _audit_decision(self, action: str, decision: Decision, context: Dict[str, Any]):
        """Registra decis√£o em auditoria."""
        record = {
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "decision": decision.value,
            "trust_level": self.trust_level.name,
            "emergency_mode": self.emergency_mode,
            "context": context,
        }

        self.decision_history.append(record)

        # Salvar em arquivo
        try:
            with open("logs/permission_decisions.jsonl", "a") as f:
                f.write(json.dumps(record) + "\n")
        except Exception as e:
            logger.error(f"Error saving audit: {e}")

    def get_decision_history(self, action: Optional[str] = None) -> List[Dict[str, Any]]:
        """Retorna hist√≥rico de decis√µes."""
        if action:
            return [d for d in self.decision_history if d["action"] == action]
        return self.decision_history

    def explain_decision(self, action: str, context: Dict[str, Any]) -> str:
        """Explica por que uma decis√£o foi tomada."""
        perms = self.EMERGENCY_PERMISSIONS if self.emergency_mode else self.NORMAL_PERMISSIONS

        if action not in perms:
            return f"Action '{action}' is not recognized."

        perm = perms[action]
        mode = "EMERGENCY" if self.emergency_mode else "NORMAL"

        explanation = f"""
Decision Explanation
====================
Action: {action}
Mode: {mode}
Trust Level: {self.trust_level.name}
Permission Level: {perm['level'].name}
Requires Approval: {perm.get('requires_approval', False)}

Reasoning:
- In {mode} mode, '{action}' has permission level {perm['level'].name}
- Current trust level is {self.trust_level.name}
- Decision: {"AUTO-APPROVED" if not perm.get('requires_approval') else "REQUIRES APPROVAL"}

Context:
{json.dumps(context, indent=2)}
        """

        return explanation.strip()
```

### Integra√ß√£o em `src/agents/orchestrator_agent.py`

```python
# No __init__
self.permission_matrix = PermissionMatrix()

# Antes de executar a√ß√£o cr√≠tica
async def execute_action(self, action: str, context: Dict[str, Any]):
    if await self.permission_matrix.can_execute(action, context):
        # Executar a√ß√£o
        logger.info(f"Executing: {action}")
        # ... implementa√ß√£o ...
    else:
        logger.warning(f"Action denied: {action}")

# Novo endpoint de API
@router.get("/permissions/explain/{action}")
async def explain_permission(action: str):
    return {
        "explanation": orchestrator.permission_matrix.explain_decision(action, {})
    }

# Quando entrar em modo emergencial
async def _handle_crisis(self, reason: str):
    self.permission_matrix.set_emergency_mode(True, reason)
    # Agora a√ß√µes autom√°ticas s√£o expandidas
```

---

## üß™ Testes Necess√°rios

**Arquivo**: `tests/orchestrator/test_permission_matrix.py`

```python
"""
Testes para PermissionMatrix.

Cobertura:
- Permiss√µes em modo normal
- Permiss√µes em modo emergencial
- Aprova√ß√µes e escala√ß√£o
- Hist√≥rico de auditoria
- Explicabilidade de decis√µes
"""

import pytest
from src.orchestrator.permission_matrix import (
    PermissionMatrix,
    PermissionLevel,
    TrustLevel,
    Decision,
)

@pytest.mark.asyncio
async def test_automatic_permission():
    """Testa que a√ß√µes autom√°ticas s√£o aprovadas."""
    pm = PermissionMatrix()

    result = await pm.can_execute("delegate_task", {"target": "code"})

    assert result is True

@pytest.mark.asyncio
async def test_requires_approval():
    """Testa que a√ß√µes que requerem aprova√ß√£o retornam False."""
    pm = PermissionMatrix()

    result = await pm.can_execute("modify_code", {"file": "test.py"})

    # Retorna False porque n√£o h√° aprova√ß√£o (simular timeout)
    # Em produ√ß√£o seria True ap√≥s aprova√ß√£o humana

@pytest.mark.asyncio
async def test_emergency_mode_auto_approves():
    """Testa que modo emergencial aprova automaticamente."""
    pm = PermissionMatrix()
    pm.set_emergency_mode(True, "Security threat")

    result = await pm.can_execute("block_port", {"port": 8080})

    assert result is True
    assert pm.emergency_mode is True

@pytest.mark.asyncio
async def test_emergency_mode_escalation():
    """Testa que escala√ß√£o funciona em emerg√™ncia."""
    pm = PermissionMatrix()
    pm.set_emergency_mode(True, "Critical threat")

    result = await pm.can_execute("escalate_to_human", {})

    # Deve escalar (retorna False)
    assert result is False

def test_audit_trail():
    """Testa que decis√µes s√£o auditadas."""
    pm = PermissionMatrix()

    # Simular decis√£o
    import asyncio
    asyncio.run(pm.can_execute("delegate_task", {"target": "security"}))

    history = pm.get_decision_history()

    assert len(history) > 0
    assert history[0]["action"] == "delegate_task"
    assert history[0]["decision"] == "approved"

def test_explain_decision():
    """Testa explica√ß√£o de decis√£o."""
    pm = PermissionMatrix()

    explanation = pm.explain_decision("modify_code", {"file": "test.py"})

    assert "modify_code" in explanation
    assert "REQUIRES APPROVAL" in explanation

def test_permission_level_normal():
    """Testa que normal mode tem permiss√µes corretas."""
    pm = PermissionMatrix()
    pm.emergency_mode = False

    # delegate_task deve ser autom√°tico
    assert pm.NORMAL_PERMISSIONS["delegate_task"]["level"] == PermissionLevel.AUTOMATIC

    # modify_code deve requer aprova√ß√£o
    assert pm.NORMAL_PERMISSIONS["modify_code"]["requires_approval"] is True

def test_permission_level_emergency():
    """Testa que emergency mode tem permiss√µes expandidas."""
    pm = PermissionMatrix()
    pm.set_emergency_mode(True, "Test")

    # Em modo emergencial, block_port √© autom√°tico
    assert pm.EMERGENCY_PERMISSIONS["block_port"]["level"] == PermissionLevel.AUTOMATIC

# ... mais testes
```

---

## üìã Checklist de Implementa√ß√£o

- [ ] Criar `src/orchestrator/permission_matrix.py` com classe PermissionMatrix
- [ ] Definir enums PermissionLevel, TrustLevel, Decision
- [ ] Implementar l√≥gica de permiss√µes normais e emergenciais
- [ ] Implementar sistema de aprova√ß√£o (com timeout)
- [ ] Implementar escala√ß√£o para humano
- [ ] Implementar auditoria imut√°vel
- [ ] Implementar explicabilidade de decis√µes
- [ ] Integrar PermissionMatrix em OrchestratorAgent
- [ ] Criar endpoints de API para permissions
- [ ] Criar suite de testes (25+ testes)
- [ ] Validar com black, flake8, mypy
- [ ] Testar em produ√ß√£o por 48h
- [ ] Documentar em docs/

---

# üß¨ SE√á√ÉO 8: SANDBOX DE AUTO-MELHORIA (Self-Improvement)

## üìä Especifica√ß√£o

### Fluxo de Auto-Melhoria

```
1. Detectar Oportunidade
   ‚îî‚îÄ M√©tricas mostram gap de performance
   ‚îî‚îÄ Hist√≥rico sugere padr√£o de falha

2. Propor Mudan√ßa
   ‚îî‚îÄ AutopoieticManager gera c√≥digo
   ‚îî‚îÄ Define novo algoritmo/estrat√©gia

3. [NOVO] Criar Sandbox
   ‚îî‚îÄ Clonar estado do Orchestrator
   ‚îî‚îÄ Preparar ambiente isolado
   ‚îî‚îÄ Aquecimento de cache

4. [NOVO] Aplicar Mudan√ßa
   ‚îî‚îÄ Instalar c√≥digo na c√≥pia
   ‚îî‚îÄ Inicializar novos componentes
   ‚îî‚îÄ Valida√ß√£o de sintaxe

5. [NOVO] Testar em Sandbox
   ‚îî‚îÄ Executar suite de valida√ß√£o
   ‚îî‚îÄ Comparar m√©tricas (antes vs depois)
   ‚îî‚îÄ Verificar regress√µes

6. [NOVO] Decidir
   ‚îú‚îÄ Se melhoria > threshold (ex: +5%)
   ‚îÇ  ‚îî‚îÄ Aplicar em produ√ß√£o
   ‚îÇ  ‚îî‚îÄ Registrar em history
   ‚îÇ  ‚îî‚îÄ Auditar mudan√ßa
   ‚îî‚îÄ Sen√£o
     ‚îî‚îÄ Descartar
     ‚îî‚îÄ Arquivar em an√°lise
     ‚îî‚îÄ Usar para futuro learning

7. [NOVO] Rollback Autom√°tico
   ‚îî‚îÄ Se degrada√ß√£o detectada
   ‚îî‚îÄ Reverter para vers√£o anterior
   ‚îî‚îÄ Alertar humano
```

---

## üíª Implementa√ß√£o Detalhada

### Arquivo: `src/orchestrator/sandbox.py`

```python
"""
Sandbox para Auto-Melhoria do Orchestrator.

Responsabilidades:
1. Clonar estado de forma segura
2. Aplicar mudan√ßas em isolamento
3. Validar antes de aplicar em produ√ß√£o
4. Rastrear evolu√ß√£o do sistema
"""

from typing import Dict, Any, Callable, Optional, List
from dataclasses import dataclass
from enum import Enum
import asyncio
import logging
import copy
import json
from datetime import datetime

logger = logging.getLogger(__name__)

class TestResult(Enum):
    """Resultado de teste no sandbox."""
    PASSED = "passed"
    FAILED = "failed"
    DEGRADED = "degraded"

@dataclass
class SandboxMetrics:
    """M√©tricas do sandbox."""
    response_time_ms: float
    success_rate: float
    memory_mb: float
    cpu_percent: float
    phi_score: float
    timestamp: str

@dataclass
class ImprovementProposal:
    """Proposta de melhoria."""
    id: str
    description: str
    change_code: str
    expected_improvement_percent: float
    timestamp: str
    created_by: str  # "autopoietic_manager", "human", etc.

class OrchestratorSandbox:
    """Sandbox para testar mudan√ßas antes de aplicar em produ√ß√£o."""

    def __init__(self, orchestrator):
        """
        Inicializa sandbox.

        Args:
            orchestrator: Inst√¢ncia do OrchestratorAgent a clonar
        """
        self.orchestrator = orchestrator
        self.sandbox_orchestrator: Optional[Any] = None
        self.baseline_metrics: Optional[SandboxMetrics] = None
        self.test_metrics: Optional[SandboxMetrics] = None
        self.improvement_history: List[Dict[str, Any]] = []
        self.improvement_threshold = 0.05  # Melhoria m√≠nima de 5%
        self.regression_threshold = 0.02  # Regress√£o m√°xima de 2%

    async def clone_orchestrator(self) -> Any:
        """
        Clona o Orchestrator para sandbox.

        Usa deep copy para isolamento total.

        Returns:
            C√≥pia isolada do Orchestrator
        """
        try:
            logger.info("Cloning orchestrator for sandbox...")

            # Deep copy do orchestrator
            self.sandbox_orchestrator = copy.deepcopy(self.orchestrator)

            # Isolar recursos
            # (em produ√ß√£o, usar processo separado ou container)

            logger.info("‚úÖ Orchestrator cloned successfully")
            return self.sandbox_orchestrator

        except Exception as e:
            logger.error(f"‚ùå Failed to clone orchestrator: {e}")
            raise

    async def apply_change(self, proposal: ImprovementProposal) -> bool:
        """
        Aplica mudan√ßa ao sandbox.

        Args:
            proposal: Proposta de melhoria

        Returns:
            True se aplica√ß√£o foi bem-sucedida
        """
        if not self.sandbox_orchestrator:
            await self.clone_orchestrator()

        try:
            logger.info(f"Applying change {proposal.id}: {proposal.description}")

            # Validar sintaxe da mudan√ßa
            compile(proposal.change_code, "<string>", "exec")

            # Aplicar mudan√ßa ao sandbox
            exec_globals = {"orchestrator": self.sandbox_orchestrator}
            exec(proposal.change_code, exec_globals)

            logger.info(f"‚úÖ Change applied: {proposal.id}")
            return True

        except SyntaxError as e:
            logger.error(f"‚ùå Syntax error in change: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Failed to apply change: {e}")
            return False

    async def run_validation_tests(self) -> Dict[str, Any]:
        """
        Executa suite de testes de valida√ß√£o.

        Testa:
        - Funcionalidade b√°sica
        - Performance
        - Regress√µes
        - Memory leaks

        Returns:
            Resultados dos testes
        """
        if not self.sandbox_orchestrator:
            raise ValueError("Sandbox not initialized")

        logger.info("Running validation tests...")

        results = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "test_cases": [],
        }

        # Testes b√°sicos de funcionalidade
        test_cases = [
            ("Basic initialization", self._test_initialization),
            ("Agent registry", self._test_agent_registry),
            ("Event bus", self._test_event_bus),
            ("Circuit breaker", self._test_circuit_breaker),
            ("Security handlers", self._test_security_handlers),
            ("Performance", self._test_performance),
        ]

        for test_name, test_func in test_cases:
            try:
                result = await test_func()
                results["test_cases"].append({
                    "name": test_name,
                    "result": "passed",
                    "details": result,
                })
                results["passed_tests"] += 1
            except Exception as e:
                logger.error(f"Test failed: {test_name}: {e}")
                results["test_cases"].append({
                    "name": test_name,
                    "result": "failed",
                    "error": str(e),
                })
                results["failed_tests"] += 1

            results["total_tests"] += 1

        return results

    async def compare_metrics(self, baseline: SandboxMetrics, current: SandboxMetrics) -> Dict[str, Any]:
        """
        Compara m√©tricas baseline vs atual.

        Args:
            baseline: M√©tricas antes da mudan√ßa
            current: M√©tricas ap√≥s a mudan√ßa

        Returns:
            An√°lise de melhoria/degrada√ß√£o
        """
        comparison = {
            "response_time_improvement_percent": (
                (baseline.response_time_ms - current.response_time_ms) /
                baseline.response_time_ms * 100
            ),
            "success_rate_improvement_percent": (
                (current.success_rate - baseline.success_rate) * 100
            ),
            "memory_improvement_percent": (
                (baseline.memory_mb - current.memory_mb) /
                baseline.memory_mb * 100
            ),
            "phi_score_improvement_percent": (
                (current.phi_score - baseline.phi_score) /
                baseline.phi_score * 100
            ) if baseline.phi_score > 0 else 0,
        }

        # Calcular score geral
        overall_improvement = (
            comparison["response_time_improvement_percent"] * 0.3 +
            comparison["success_rate_improvement_percent"] * 0.3 +
            comparison["memory_improvement_percent"] * 0.2 +
            comparison["phi_score_improvement_percent"] * 0.2
        ) / 100

        comparison["overall_improvement_percent"] = overall_improvement * 100

        return comparison

    async def execute_improvement(self, proposal: ImprovementProposal) -> bool:
        """
        Executa fluxo completo de melhoria.

        Args:
            proposal: Proposta de melhoria

        Returns:
            True se melhoria foi aplicada em produ√ß√£o
        """
        logger.info(f"Starting improvement process: {proposal.id}")

        try:
            # 1. Clone
            await self.clone_orchestrator()

            # 2. Medir baseline
            self.baseline_metrics = await self._measure_performance(self.sandbox_orchestrator)
            logger.info(f"Baseline metrics: {self.baseline_metrics}")

            # 3. Aplicar mudan√ßa
            if not await self.apply_change(proposal):
                logger.error("Failed to apply change")
                return False

            # 4. Executar testes
            test_results = await self.run_validation_tests()
            if test_results["failed_tests"] > 0:
                logger.error(f"Tests failed: {test_results['failed_tests']}")
                return False

            # 5. Medir performance
            self.test_metrics = await self._measure_performance(self.sandbox_orchestrator)
            logger.info(f"Test metrics: {self.test_metrics}")

            # 6. Comparar
            comparison = await self.compare_metrics(self.baseline_metrics, self.test_metrics)
            logger.info(f"Comparison: {comparison}")

            # 7. Decidir
            if comparison["overall_improvement_percent"] > self.improvement_threshold * 100:
                logger.info(f"‚úÖ Improvement approved: {comparison['overall_improvement_percent']:.2f}%")

                # Aplicar em produ√ß√£o
                success = await self.apply_change(proposal)
                if success:
                    self._record_improvement(proposal, comparison)
                    return True
            else:
                logger.info(f"‚ùå Improvement insufficient: {comparison['overall_improvement_percent']:.2f}%")
                return False

        except Exception as e:
            logger.error(f"Error during improvement execution: {e}")
            return False

        finally:
            # Limpar sandbox
            await self._cleanup_sandbox()

    async def _measure_performance(self, orchestrator) -> SandboxMetrics:
        """Mede performance do orchestrator."""
        # TODO: Implementar medi√ß√£o real
        return SandboxMetrics(
            response_time_ms=50.0,
            success_rate=0.95,
            memory_mb=512.0,
            cpu_percent=30.0,
            phi_score=0.85,
            timestamp=datetime.now().isoformat(),
        )

    async def _test_initialization(self) -> Dict[str, Any]:
        """Testa inicializa√ß√£o."""
        # TODO: Implementar teste real
        return {"initialized": True}

    async def _test_agent_registry(self) -> Dict[str, Any]:
        """Testa AgentRegistry."""
        # TODO: Implementar teste real
        return {"agents_registered": 7}

    async def _test_event_bus(self) -> Dict[str, Any]:
        """Testa EventBus."""
        # TODO: Implementar teste real
        return {"events_processed": 100}

    async def _test_circuit_breaker(self) -> Dict[str, Any]:
        """Testa CircuitBreaker."""
        # TODO: Implementar teste real
        return {"breakers_healthy": True}

    async def _test_security_handlers(self) -> Dict[str, Any]:
        """Testa Security Handlers."""
        # TODO: Implementar teste real
        return {"handlers_responsive": True}

    async def _test_performance(self) -> Dict[str, Any]:
        """Testa performance."""
        # TODO: Implementar teste real
        return {"response_time_ms": 50}

    async def _cleanup_sandbox(self):
        """Limpa sandbox."""
        self.sandbox_orchestrator = None
        logger.info("Sandbox cleaned up")

    def _record_improvement(self, proposal: ImprovementProposal, comparison: Dict[str, Any]):
        """Registra melhoria aprovada."""
        record = {
            "proposal_id": proposal.id,
            "description": proposal.description,
            "improvement_percent": comparison["overall_improvement_percent"],
            "timestamp": datetime.now().isoformat(),
            "status": "applied",
        }

        self.improvement_history.append(record)

        # Salvar em arquivo
        try:
            with open("logs/improvements.jsonl", "a") as f:
                f.write(json.dumps(record) + "\n")
        except Exception as e:
            logger.error(f"Error saving improvement: {e}")

    def get_improvement_history(self) -> List[Dict[str, Any]]:
        """Retorna hist√≥rico de melhorias."""
        return self.improvement_history
```

### Integra√ß√£o em `src/agents/orchestrator_agent.py`

```python
# No __init__
self.sandbox = OrchestratorSandbox(self)

# Quando AutopoieticManager prop√µe melhoria
async def apply_autopoietic_improvement(self, proposal: ImprovementProposal):
    success = await self.sandbox.execute_improvement(proposal)
    if success:
        logger.info(f"‚úÖ Improvement applied: {proposal.id}")
        await self.event_bus.publish(
            OrchestratorEvent(
                type="improvement_applied",
                severity="HIGH",
                message=f"Self-improvement executed: {proposal.description}"
            )
        )

# Novo endpoint de API
@router.get("/improvements/history")
async def get_improvement_history():
    return {
        "history": orchestrator.sandbox.get_improvement_history()
    }
```

---

## üß™ Testes Necess√°rios

**Arquivo**: `tests/orchestrator/test_sandbox.py`

```python
"""
Testes para OrchestratorSandbox.

Cobertura:
- Clonagem de orchestrator
- Aplica√ß√£o de mudan√ßas
- Execu√ß√£o de testes
- Compara√ß√£o de m√©tricas
- Fluxo completo de melhoria
"""

import pytest
from src.orchestrator.sandbox import (
    OrchestratorSandbox,
    SandboxMetrics,
    ImprovementProposal,
)
from datetime import datetime

@pytest.fixture
def mock_orchestrator():
    """Mock do OrchestratorAgent."""
    class MockOrchestrator:
        def __init__(self):
            self.name = "orchestrator"
            self.phi_score = 0.85

    return MockOrchestrator()

@pytest.fixture
def sandbox(mock_orchestrator):
    """Fixture do sandbox."""
    return OrchestratorSandbox(mock_orchestrator)

@pytest.mark.asyncio
async def test_clone_orchestrator(sandbox):
    """Testa clonagem de orchestrator."""
    cloned = await sandbox.clone_orchestrator()

    assert cloned is not None
    assert cloned.name == "orchestrator"

@pytest.mark.asyncio
async def test_apply_valid_change(sandbox):
    """Testa aplica√ß√£o de mudan√ßa v√°lida."""
    await sandbox.clone_orchestrator()

    proposal = ImprovementProposal(
        id="test-1",
        description="Test improvement",
        change_code="orchestrator.test_var = 123",
        expected_improvement_percent=10,
        timestamp=datetime.now().isoformat(),
        created_by="test",
    )

    result = await sandbox.apply_change(proposal)

    assert result is True

@pytest.mark.asyncio
async def test_apply_invalid_change(sandbox):
    """Testa aplica√ß√£o de mudan√ßa inv√°lida."""
    await sandbox.clone_orchestrator()

    proposal = ImprovementProposal(
        id="test-2",
        description="Invalid change",
        change_code="this is not valid python !!!",
        expected_improvement_percent=10,
        timestamp=datetime.now().isoformat(),
        created_by="test",
    )

    result = await sandbox.apply_change(proposal)

    assert result is False

@pytest.mark.asyncio
async def test_compare_metrics():
    """Testa compara√ß√£o de m√©tricas."""
    sandbox = OrchestratorSandbox(None)

    baseline = SandboxMetrics(
        response_time_ms=100.0,
        success_rate=0.90,
        memory_mb=512.0,
        cpu_percent=50.0,
        phi_score=0.80,
        timestamp=datetime.now().isoformat(),
    )

    improved = SandboxMetrics(
        response_time_ms=80.0,  # 20% melhor
        success_rate=0.95,  # 5% melhor
        memory_mb=400.0,  # 22% melhor
        cpu_percent=40.0,  # 20% melhor
        phi_score=0.90,  # 12% melhor
        timestamp=datetime.now().isoformat(),
    )

    comparison = await sandbox.compare_metrics(baseline, improved)

    assert comparison["response_time_improvement_percent"] > 0
    assert comparison["success_rate_improvement_percent"] > 0
    assert comparison["overall_improvement_percent"] > 0

# ... mais testes
```

---

## üìã Checklist de Implementa√ß√£o

- [ ] Criar `src/orchestrator/sandbox.py` com classe OrchestratorSandbox
- [ ] Implementar clonagem de orchestrator (deep copy)
- [ ] Implementar aplica√ß√£o de mudan√ßas
- [ ] Implementar medi√ß√£o de performance
- [ ] Implementar suite de testes validat√≥rios
- [ ] Implementar compara√ß√£o de m√©tricas
- [ ] Implementar fluxo completo de melhoria
- [ ] Implementar rollback autom√°tico
- [ ] Implementar hist√≥rico de melhorias
- [ ] Integrar Sandbox em OrchestratorAgent
- [ ] Criar endpoints de API para sandbox
- [ ] Criar suite de testes (20+ testes)
- [ ] Validar com black, flake8, mypy
- [ ] Testar em produ√ß√£o por 72h (mais longo que outras se√ß√µes)
- [ ] Documentar em docs/

---

# üì¶ RESUMO DE ENTREGA

## Arquivos a Criar/Modificar

```
NEW FILES:
- src/orchestrator/power_manager.py (300-400 linhas)
- src/orchestrator/permission_matrix.py (400-500 linhas)
- src/orchestrator/sandbox.py (500-600 linhas)
- tests/orchestrator/test_power_manager.py (250-300 linhas)
- tests/orchestrator/test_permission_matrix.py (300-350 linhas)
- tests/orchestrator/test_sandbox.py (250-300 linhas)

MODIFIED FILES:
- src/agents/orchestrator_agent.py (adicionar integra√ß√µes)
- src/orchestrator/__init__.py (export de classes novas)

DOCUMENTATION:
- docs/ORCHESTRATOR_POWER_STATES.md
- docs/ORCHESTRATOR_PERMISSIONS.md
- docs/ORCHESTRATOR_SANDBOX.md
- docs/ORCHESTRATOR_ROADMAP.md (atualizar com status)
```

## M√©tricas de Sucesso

| M√©trica | Meta |
|---------|------|
| Cobertura de Testes | ‚â•95% |
| Black Compliance | 100% |
| Flake8 Errors | 0 |
| MyPy Errors | 0 |
| Documenta√ß√£o | ‚â•90% das fun√ß√µes |
| Integra√ß√£o com Git | 100% (todos os arquivos tracked) |

---

## Pr√≥ximas Etapas

1. **Implementar Se√ß√£o 4** (40-50h)
   - Criar PowerManager
   - Integrar em OrchestratorAgent
   - Testar transi√ß√µes

2. **Implementar Se√ß√£o 5** (50-60h)
   - Criar PermissionMatrix
   - Sistema de aprova√ß√µes
   - Auditoria

3. **Implementar Se√ß√£o 8** (60-70h)
   - Criar OrchestratorSandbox
   - Fluxo de melhoria
   - Valida√ß√£o robusta

---

**Documento preparado**: 6 de dezembro de 2025
**Vers√£o**: 1.0
**Status**: üü¢ PRONTO PARA IMPLEMENTA√á√ÉO
