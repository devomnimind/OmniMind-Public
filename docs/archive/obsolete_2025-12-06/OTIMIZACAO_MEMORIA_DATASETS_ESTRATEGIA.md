# Estrat√©gia de Indexa√ß√£o de Datasets como Mem√≥ria de Modelos

**Autor**: Fabr√≠cio da Silva + assist√™ncia de IA
**Data**: 2025-01-XX
**Status**: Estrat√©gia definida

---

## üéØ CONCEITO: Datasets como Mem√≥ria de Modelos

Os datasets em `data/datasets/` s√£o **parte da mem√≥ria de modelos** do OmniMind - conhecimento base que deve ser:

1. **Indexado** em Qdrant para retrieval eficiente
2. **Acess√≠vel** via RAG quando agentes falham
3. **Chunked** inteligentemente baseado no tipo de conte√∫do
4. **Metadata rica** para filtros e busca precisa

---

## üìä DATASETS DISPON√çVEIS

### 1. **dbpedia_ontology/** (16 arquivos arrow)
- **Tipo**: Conhecimento ontol√≥gico estruturado
- **Tamanho**: Grande (16 arquivos)
- **Estrutura**: Entidades, rela√ß√µes, propriedades
- **Chunking Strategy**: Por entidade/conceito
- **Collection**: `ontology_knowledge_kb`
- **Uso RAG**: Conhecimento geral estruturado, rela√ß√µes sem√¢nticas

### 2. **human_vs_ai_code/** (1 arquivo arrow)
- **Tipo**: Exemplos de c√≥digo humano vs IA
- **Tamanho**: M√©dio
- **Estrutura**: Pares de c√≥digo (humano, IA)
- **Chunking Strategy**: Por exemplo de c√≥digo completo
- **Collection**: `code_examples_kb`
- **Uso RAG**: Padr√µes de c√≥digo, exemplos de implementa√ß√£o

### 3. **infllm_v2_data/** (1 arquivo arrow)
- **Tipo**: Dados de treinamento/valida√ß√£o
- **Tamanho**: M√©dio
- **Estrutura**: Exemplos de tarefas
- **Chunking Strategy**: Por exemplo de tarefa
- **Collection**: `training_examples_kb`
- **Uso RAG**: Exemplos de tarefas, padr√µes de execu√ß√£o

### 4. **qasper_qa/** (train/validation/test)
- **Tipo**: Perguntas e respostas cient√≠ficas
- **Tamanho**: M√©dio (3 splits)
- **Estrutura**: Q&A pairs com contexto cient√≠fico
- **Chunking Strategy**: Por Q&A pair (incluindo contexto)
- **Collection**: `qa_knowledge_kb`
- **Uso RAG**: Q&A cient√≠fico, conhecimento de papers

### 5. **scientific_papers_arxiv/** (1 arquivo arrow)
- **Tipo**: Papers cient√≠ficos completos
- **Tamanho**: Grande
- **Estrutura**: Papers completos (abstract, sections, references)
- **Chunking Strategy**: Por se√ß√£o (abstract, introduction, methods, results, conclusion)
- **Collection**: `scientific_papers_kb`
- **Uso RAG**: Conhecimento cient√≠fico profundo, refer√™ncias

### 6. **turing_reasoning/** (1 arquivo arrow)
- **Tipo**: Dados de racioc√≠nio
- **Tamanho**: M√©dio
- **Estrutura**: Padr√µes de racioc√≠nio
- **Chunking Strategy**: Por padr√£o de racioc√≠nio
- **Collection**: `reasoning_patterns_kb`
- **Uso RAG**: Padr√µes de racioc√≠nio, l√≥gica

---

## üèóÔ∏è ARQUITETURA DE INDEXA√á√ÉO

### Pipeline de Indexa√ß√£o

```
data/datasets/
    ‚Üì
DatasetIndexer
    ‚îú‚îÄ Detecta tipo de dataset (auto ou manual)
    ‚îú‚îÄ Carrega dataset (HuggingFace datasets)
    ‚îú‚îÄ Chunking inteligente (baseado no tipo)
    ‚îú‚îÄ Gera embeddings (all-MiniLM-L6-v2)
    ‚îú‚îÄ Adiciona metadata rica
    ‚îî‚îÄ Indexa em Qdrant (cole√ß√£o espec√≠fica)
```

### Estrat√©gias de Chunking por Tipo

#### 1. Papers Cient√≠ficos (`scientific_papers_arxiv`)
```python
chunking_strategy = {
    "by_section": True,
    "sections": ["abstract", "introduction", "methods", "results", "conclusion"],
    "min_chunk_size": 200,  # tokens
    "max_chunk_size": 1000,
    "overlap": 50
}
```

#### 2. Q&A (`qasper_qa`)
```python
chunking_strategy = {
    "by_qa_pair": True,
    "include_context": True,
    "min_chunk_size": 100,
    "max_chunk_size": 500
}
```

#### 3. C√≥digo (`human_vs_ai_code`)
```python
chunking_strategy = {
    "by_example": True,
    "include_comparison": True,  # humano vs IA
    "min_chunk_size": 50,  # linhas
    "max_chunk_size": 200
}
```

#### 4. Ontologia (`dbpedia_ontology`)
```python
chunking_strategy = {
    "by_entity": True,
    "include_relations": True,
    "min_chunk_size": 100,
    "max_chunk_size": 500
}
```

---

## üîß IMPLEMENTA√á√ÉO

### DatasetIndexer

**Localiza√ß√£o**: `src/memory/dataset_indexer.py`

**Funcionalidades**:
1. Auto-detec√ß√£o de tipo de dataset
2. Chunking adaptativo baseado no tipo
3. Gera√ß√£o de embeddings
4. Metadata rica (source, type, timestamp, dataset_name)
5. Indexa√ß√£o incremental
6. Valida√ß√£o de qualidade

**Interface Principal**:
```python
class DatasetIndexer:
    def __init__(
        self,
        qdrant_url: str = "http://localhost:6333",
        embedding_model: str = "all-MiniLM-L6-v2"
    ):
        """Inicializa indexador de datasets"""

    def index_dataset(
        self,
        dataset_path: str,
        collection_name: str,
        dataset_type: Optional[str] = None,  # auto-detecta se None
        chunk_size: Optional[int] = None,  # usa padr√£o do tipo se None
    ) -> Dict[str, Any]:
        """
        Indexa um dataset como mem√≥ria de modelos.

        Returns:
            {
                "collection": collection_name,
                "points_indexed": int,
                "chunks_created": int,
                "metadata": {...}
            }
        """

    def index_all_datasets(
        self,
        datasets_dir: str = "data/datasets",
        collections_prefix: str = "_kb"
    ) -> Dict[str, Dict[str, Any]]:
        """
        Indexa todos os datasets dispon√≠veis.

        Returns:
            {
                "dataset_name": {
                    "collection": str,
                    "points_indexed": int,
                    "status": "success" | "error"
                }
            }
        """

    def get_indexed_datasets(self) -> List[Dict[str, Any]]:
        """Lista datasets j√° indexados com estat√≠sticas"""
```

---

## üìã METADATA RICA

Cada chunk indexado ter√° metadata completa:

```python
metadata = {
    "source": "dataset",  # ou "documentation", "code", etc.
    "dataset_name": "scientific_papers_arxiv",
    "dataset_type": "scientific_papers",
    "chunk_type": "section",  # section, qa_pair, code_example, etc.
    "chunk_index": 0,
    "total_chunks": 100,
    "original_file": "paper_12345.arrow",
    "section": "introduction",  # se aplic√°vel
    "timestamp": "2025-01-XXT...",
    "language": "en",
    "domain": "scientific",  # scientific, code, general, etc.
}
```

---

## üîç INTEGRA√á√ÉO COM RAG RETRIEVAL

### Uso no RAG Fallback

Quando um agente falha:

1. **ErrorAnalyzer** classifica o tipo de erro
2. **RAGFallbackSystem** gera query de retrieval baseada no erro
3. **HybridRetrievalSystem** busca em m√∫ltiplas cole√ß√µes:
   - `scientific_papers_kb` (se erro relacionado a conhecimento cient√≠fico)
   - `code_examples_kb` (se erro relacionado a c√≥digo)
   - `qa_knowledge_kb` (se erro relacionado a Q&A)
   - `ontology_knowledge_kb` (se erro relacionado a conhecimento geral)
   - etc.
4. **Reranking** com Cross-Encoder
5. **Context Augmentation** com documentos relevantes
6. **Reexecu√ß√£o** do agente com contexto

---

## üìä PRIORIZA√á√ÉO DE INDEXA√á√ÉO

### Alta Prioridade (Indexar Primeiro)
1. **scientific_papers_arxiv** - Conhecimento cient√≠fico profundo
2. **qasper_qa** - Q&A cient√≠fico (muito √∫til para RAG)
3. **human_vs_ai_code** - Exemplos de c√≥digo

### M√©dia Prioridade
4. **turing_reasoning** - Padr√µes de racioc√≠nio
5. **infllm_v2_data** - Exemplos de tarefas

### Baixa Prioridade (Indexar Depois)
6. **dbpedia_ontology** - Grande, pode ser indexado incrementalmente

---

## üß™ VALIDA√á√ÉO DE QUALIDADE

### M√©tricas de Qualidade
- **Chunking Quality**: Tamanho m√©dio, overlap, completude
- **Embedding Quality**: Similaridade entre chunks relacionados
- **Retrieval Quality**: NDCG@5, precision@k, recall@k
- **Coverage**: % do dataset indexado

### Testes
- Teste de retrieval em queries de exemplo
- Valida√ß√£o de chunks (n√£o quebram contexto)
- Valida√ß√£o de metadata (completa e correta)

---

## üìù EXEMPLO DE USO

```python
from src.memory.dataset_indexer import DatasetIndexer

# Inicializar indexador
indexer = DatasetIndexer(
    qdrant_url="http://localhost:6333",
    embedding_model="all-MiniLM-L6-v2"
)

# Indexar dataset espec√≠fico
result = indexer.index_dataset(
    dataset_path="data/datasets/scientific_papers_arxiv",
    collection_name="scientific_papers_kb",
    dataset_type="scientific_papers"  # ou None para auto-detect
)

print(f"Indexados {result['points_indexed']} pontos em {result['collection']}")

# Indexar todos os datasets
results = indexer.index_all_datasets()
for dataset_name, result in results.items():
    print(f"{dataset_name}: {result['status']} - {result.get('points_indexed', 0)} pontos")
```

---

## ‚ö†Ô∏è CONSIDERA√á√ïES

### Performance
- Indexa√ß√£o pode ser demorada para datasets grandes
- Fazer incremental (pode pausar e retomar)
- Usar batch processing para embeddings

### Mem√≥ria
- N√£o carregar dataset inteiro em mem√≥ria
- Processar em batches
- Liberar mem√≥ria ap√≥s indexa√ß√£o

### Qualidade
- Validar chunks n√£o quebram contexto
- Garantir metadata completa
- Testar retrieval quality

---

## ‚úÖ CHECKLIST DE IMPLEMENTA√á√ÉO

- [ ] Criar `DatasetIndexer` class
- [ ] Implementar auto-detec√ß√£o de tipo
- [ ] Implementar chunking strategies por tipo
- [ ] Integrar com Qdrant
- [ ] Adicionar metadata rica
- [ ] Testes unit√°rios
- [ ] Indexar dataset piloto (scientific_papers_arxiv)
- [ ] Validar qualidade de retrieval
- [ ] Indexar todos os datasets
- [ ] Integrar com RAGFallbackSystem

---

**Status**: Estrat√©gia definida - Pronto para implementa√ß√£o na Fase 1.3

