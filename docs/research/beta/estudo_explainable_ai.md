# üî¨ Estudo Cient√≠fico: Explainable AI (XAI) para OmniMind
## Fase Beta - An√°lise Avan√ßada e Prototipagem

**Projeto:** OmniMind - Sistema de IA Aut√¥nomo  
**Categoria:** Interpretabilidade e Transpar√™ncia  
**Status:** Beta - An√°lise e Prototipagem  
**Data:** Novembro 2025  
**Hardware Base:** NVIDIA GTX 1650 (4GB VRAM), Intel i5, 24GB RAM

---

## üìã Resumo Executivo

Este estudo desenvolve estrat√©gias de **Explainable AI (XAI)** para tornar as decis√µes do OmniMind transparentes, interpret√°veis e audit√°veis, essencial para confian√ßa, debugging e compliance regulat√≥rio.

### üéØ Objetivos da Pesquisa

1. **Implementar** mecanismos de explicabilidade em decis√µes complexas
2. **Desenvolver** visualiza√ß√µes de attention para interpretabilidade
3. **Criar** explica√ß√µes em linguagem natural para a√ß√µes do sistema
4. **Estabelecer** m√©tricas de confian√ßa e incerteza

### üîç Gap Identificado

**Situa√ß√£o Atual:**
- ‚úÖ Sistema funcional com m√∫ltiplos agentes
- ‚úÖ Decision trees com explicabilidade b√°sica
- ‚ùå Decis√µes de LLM s√£o "black boxes"
- ‚ùå Sem visualiza√ß√£o de racioc√≠nio interno
- ‚ùå Explica√ß√µes t√©cnicas, n√£o humanas
- ‚ùå Aus√™ncia de m√©tricas de confian√ßa

**Impacto:**
- Dificuldade de debugging em decis√µes erradas
- Impossibilidade de auditoria regulat√≥ria
- Baixa confian√ßa de usu√°rios finais
- Problemas de compliance (LGPD, GDPR)

---

## üèóÔ∏è Fundamenta√ß√£o Te√≥rica

### 1. Attention Visualization

#### 1.1 Attention Heatmaps

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
import numpy as np

class AttentionVisualizer:
    """Visualiza attention weights para interpretabilidade"""
    
    def __init__(self):
        self.attention_cache: Dict[str, torch.Tensor] = {}
        
    def register_attention_hook(
        self,
        model: nn.Module,
        layer_name: str
    ) -> None:
        """Registra hook para capturar attention weights"""
        
        def hook_fn(module, input, output):
            # Output de MultiheadAttention: (output, attention_weights)
            if isinstance(output, tuple) and len(output) == 2:
                attention_weights = output[1]
                self.attention_cache[layer_name] = attention_weights.detach()
        
        # Encontra layer e registra hook
        for name, module in model.named_modules():
            if name == layer_name and isinstance(module, nn.MultiheadAttention):
                module.register_forward_hook(hook_fn)
    
    def visualize_attention(
        self,
        layer_name: str,
        tokens: List[str],
        save_path: str = None
    ) -> None:
        """Cria heatmap de attention"""
        
        if layer_name not in self.attention_cache:
            raise ValueError(f"No attention cached for layer {layer_name}")
        
        # Attention weights: [batch, num_heads, seq_len, seq_len]
        attention = self.attention_cache[layer_name][0]  # Primeiro batch
        
        # M√©dia sobre heads
        avg_attention = attention.mean(dim=0).cpu().numpy()
        
        # Cria heatmap
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            avg_attention,
            xticklabels=tokens,
            yticklabels=tokens,
            cmap='viridis',
            cbar_kws={'label': 'Attention Weight'}
        )
        plt.title(f'Attention Heatmap - {layer_name}')
        plt.xlabel('Key Tokens')
        plt.ylabel('Query Tokens')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        else:
            plt.show()
    
    def get_top_attended_tokens(
        self,
        layer_name: str,
        query_token_idx: int,
        top_k: int = 5
    ) -> List[Tuple[int, float]]:
        """Retorna top-k tokens mais atendidos para query espec√≠fico"""
        
        attention = self.attention_cache[layer_name][0]
        avg_attention = attention.mean(dim=0)  # M√©dia sobre heads
        
        # Attention do query token
        query_attention = avg_attention[query_token_idx]
        
        # Top-k
        top_values, top_indices = torch.topk(query_attention, k=top_k)
        
        return list(zip(
            top_indices.tolist(),
            top_values.tolist()
        ))

class GradCAM:
    """Gradient-weighted Class Activation Mapping para CNNs"""
    
    def __init__(self, model: nn.Module, target_layer: str):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        
        self._register_hooks()
    
    def _register_hooks(self) -> None:
        """Registra hooks para gradients e activations"""
        
        def forward_hook(module, input, output):
            self.activations = output.detach()
        
        def backward_hook(module, grad_input, grad_output):
            self.gradients = grad_output[0].detach()
        
        # Registra hooks na target layer
        for name, module in self.model.named_modules():
            if name == self.target_layer:
                module.register_forward_hook(forward_hook)
                module.register_backward_hook(backward_hook)
    
    def generate_cam(
        self,
        input_tensor: torch.Tensor,
        target_class: int
    ) -> np.ndarray:
        """Gera Class Activation Map"""
        
        # Forward pass
        output = self.model(input_tensor)
        
        # Backward pass para target class
        self.model.zero_grad()
        output[0, target_class].backward()
        
        # Calcula weights (global average pooling de gradients)
        weights = torch.mean(self.gradients, dim=[2, 3], keepdim=True)
        
        # Weighted combination de activations
        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)
        
        # ReLU (remover ativa√ß√µes negativas)
        cam = torch.relu(cam)
        
        # Normaliza
        cam = cam - cam.min()
        cam = cam / (cam.max() + 1e-8)
        
        return cam[0, 0].cpu().numpy()
    
    def visualize_cam(
        self,
        original_image: np.ndarray,
        cam: np.ndarray,
        alpha: float = 0.5
    ) -> np.ndarray:
        """Sobrep√µe CAM na imagem original"""
        
        # Resize CAM para tamanho da imagem
        cam_resized = cv2.resize(cam, (original_image.shape[1], original_image.shape[0]))
        
        # Converte para heatmap
        heatmap = cv2.applyColorMap(
            (cam_resized * 255).astype(np.uint8),
            cv2.COLORMAP_JET
        )
        
        # Sobrep√µe
        overlay = cv2.addWeighted(
            original_image,
            alpha,
            heatmap,
            1 - alpha,
            0
        )
        
        return overlay
```

#### 1.2 Decision Tree Interpretability

```python
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree

class InterpretableDecisionTree:
    """Decision tree com explica√ß√µes humanas"""
    
    def __init__(self, max_depth: int = 5):
        self.tree = DecisionTreeClassifier(
            max_depth=max_depth,
            min_samples_split=10,
            min_samples_leaf=5
        )
        self.feature_names: List[str] = []
        self.class_names: List[str] = []
    
    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        feature_names: List[str],
        class_names: List[str]
    ) -> None:
        """Treina √°rvore"""
        
        self.feature_names = feature_names
        self.class_names = class_names
        self.tree.fit(X, y)
    
    def explain_prediction(
        self,
        sample: np.ndarray,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """Explica predi√ß√£o para amostra espec√≠fica"""
        
        # Predi√ß√£o
        prediction = self.tree.predict([sample])[0]
        probabilities = self.tree.predict_proba([sample])[0]
        
        # Caminho da decis√£o
        decision_path = self.tree.decision_path([sample])
        node_indicator = decision_path.toarray()[0]
        
        # Nodes visitados
        visited_nodes = np.where(node_indicator)[0]
        
        # Explanation
        explanation = []
        for node_id in visited_nodes:
            # Informa√ß√µes do n√≥
            feature_idx = self.tree.tree_.feature[node_id]
            threshold = self.tree.tree_.threshold[node_id]
            
            # Se n√£o √© folha
            if feature_idx != -2:  # -2 indica folha
                feature_name = self.feature_names[feature_idx]
                feature_value = sample[feature_idx]
                
                # Dire√ß√£o
                if feature_value <= threshold:
                    direction = "<="
                else:
                    direction = ">"
                
                explanation.append({
                    'feature': feature_name,
                    'value': feature_value,
                    'threshold': threshold,
                    'direction': direction,
                    'rule': f"{feature_name} {direction} {threshold:.2f}"
                })
        
        # Confian√ßa
        confidence = max(probabilities)
        
        result = {
            'prediction': self.class_names[prediction],
            'confidence': confidence,
            'probabilities': dict(zip(self.class_names, probabilities)),
            'decision_path': explanation
        }
        
        if verbose:
            print(f"Prediction: {result['prediction']}")
            print(f"Confidence: {confidence:.2%}")
            print("\nDecision Path:")
            for step in explanation:
                print(f"  - {step['rule']}")
        
        return result
    
    def get_feature_importance(self) -> Dict[str, float]:
        """Retorna import√¢ncia de features"""
        
        importances = self.tree.feature_importances_
        
        return dict(zip(self.feature_names, importances))
    
    def export_rules(self) -> str:
        """Exporta regras em texto"""
        
        rules = export_text(
            self.tree,
            feature_names=self.feature_names
        )
        
        return rules
```

### 2. Natural Language Explanations

#### 2.1 Template-Based Explanations

```python
from jinja2 import Template
from typing import Any

class NaturalLanguageExplainer:
    """Gera explica√ß√µes em linguagem natural"""
    
    def __init__(self):
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, Template]:
        """Carrega templates de explica√ß√£o"""
        
        return {
            'decision': Template(
                "Decidi {{ action }} porque {{ reason }}. "
                "Esta decis√£o tem {{ confidence }}% de confian√ßa."
            ),
            
            'comparison': Template(
                "Escolhi {{ option_a }} em vez de {{ option_b }} porque "
                "{{ option_a }} tem {{ advantage }} enquanto {{ option_b }} "
                "tem {{ disadvantage }}."
            ),
            
            'reasoning_chain': Template(
                "Meu racioc√≠nio foi: "
                "{% for step in steps %}"
                "{{ loop.index }}. {{ step }}. "
                "{% endfor %}"
                "Portanto, {{ conclusion }}."
            ),
            
            'uncertainty': Template(
                "N√£o tenho certeza sobre {{ topic }} porque {{ reasons }}. "
                "Minha confian√ßa √© apenas {{ confidence }}%. "
                "Para melhorar, preciso de {{ requirements }}."
            ),
            
            'learning': Template(
                "Aprendi que {{ lesson }} ap√≥s {{ experience }}. "
                "Isso mudou meu entendimento de {{ concept }} "
                "porque {{ impact }}."
            )
        }
    
    def explain_decision(
        self,
        action: str,
        reasoning: Dict[str, Any],
        confidence: float
    ) -> str:
        """Explica decis√£o em linguagem natural"""
        
        # Extrai raz√µes do reasoning
        reasons = []
        
        if 'feature_importance' in reasoning:
            for feature, importance in sorted(
                reasoning['feature_importance'].items(),
                key=lambda x: x[1],
                reverse=True
            )[:3]:  # Top 3 features
                reasons.append(f"{feature} era {importance:.1%} importante")
        
        if 'constraints' in reasoning:
            for constraint in reasoning['constraints']:
                reasons.append(f"precisava satisfazer {constraint}")
        
        reason_text = ", ".join(reasons) if reasons else "era a melhor op√ß√£o"
        
        return self.templates['decision'].render(
            action=action,
            reason=reason_text,
            confidence=int(confidence * 100)
        )
    
    def explain_comparison(
        self,
        chosen_option: str,
        rejected_option: str,
        comparison: Dict[str, Any]
    ) -> str:
        """Explica por que uma op√ß√£o foi escolhida sobre outra"""
        
        return self.templates['comparison'].render(
            option_a=chosen_option,
            option_b=rejected_option,
            advantage=comparison['advantage'],
            disadvantage=comparison['disadvantage']
        )
    
    def explain_reasoning_chain(
        self,
        steps: List[str],
        conclusion: str
    ) -> str:
        """Explica cadeia de racioc√≠nio"""
        
        return self.templates['reasoning_chain'].render(
            steps=steps,
            conclusion=conclusion
        )
    
    def explain_uncertainty(
        self,
        topic: str,
        confidence: float,
        missing_info: List[str]
    ) -> str:
        """Explica incerteza"""
        
        reasons = "faltam informa√ß√µes sobre " + ", ".join(missing_info)
        requirements = ", ".join(missing_info)
        
        return self.templates['uncertainty'].render(
            topic=topic,
            reasons=reasons,
            confidence=int(confidence * 100),
            requirements=requirements
        )

class CounterfactualExplainer:
    """Gera explica√ß√µes contrafactuais"""
    
    def generate_counterfactual(
        self,
        original_input: Dict[str, Any],
        original_output: str,
        desired_output: str,
        model: Any
    ) -> Dict[str, Any]:
        """Gera explica√ß√£o contrafactual"""
        
        # Encontra mudan√ßa m√≠nima para obter desired_output
        counterfactual_input = self._search_counterfactual(
            original_input,
            desired_output,
            model
        )
        
        # Identifica diferen√ßas
        differences = self._compute_differences(
            original_input,
            counterfactual_input
        )
        
        # Gera explica√ß√£o
        explanation = (
            f"Se {differences}, "
            f"o resultado seria {desired_output} "
            f"em vez de {original_output}."
        )
        
        return {
            'counterfactual_input': counterfactual_input,
            'changes_required': differences,
            'explanation': explanation
        }
    
    def _search_counterfactual(
        self,
        original: Dict[str, Any],
        target: str,
        model: Any
    ) -> Dict[str, Any]:
        """Busca input contrafactual"""
        
        # Implementa√ß√£o simplificada
        # Em produ√ß√£o: usar algoritmos de otimiza√ß√£o (gradient descent, genetic algorithms)
        
        candidate = original.copy()
        
        # Tenta modifica√ß√µes incrementais
        for key in original.keys():
            if isinstance(original[key], (int, float)):
                # Varia valor
                for delta in [-0.1, -0.5, -1.0, 0.1, 0.5, 1.0]:
                    candidate[key] = original[key] + delta
                    
                    if model.predict(candidate) == target:
                        return candidate
                
                # Restaura valor original
                candidate[key] = original[key]
        
        return candidate
    
    def _compute_differences(
        self,
        original: Dict[str, Any],
        counterfactual: Dict[str, Any]
    ) -> str:
        """Computa diferen√ßas entre inputs"""
        
        diffs = []
        
        for key in original.keys():
            if original[key] != counterfactual[key]:
                diffs.append(
                    f"{key} fosse {counterfactual[key]} "
                    f"(em vez de {original[key]})"
                )
        
        return " e ".join(diffs) if diffs else "nada mudasse"
```

### 3. Uncertainty Quantification

#### 3.1 Bayesian Neural Networks

```python
import torch.nn.functional as F

class BayesianLinear(nn.Module):
    """Camada linear Bayesiana"""
    
    def __init__(self, in_features: int, out_features: int):
        super().__init__()
        
        # Weight parameters
        self.weight_mu = nn.Parameter(
            torch.randn(out_features, in_features) * 0.1
        )
        self.weight_rho = nn.Parameter(
            torch.randn(out_features, in_features) * 0.1
        )
        
        # Bias parameters
        self.bias_mu = nn.Parameter(torch.zeros(out_features))
        self.bias_rho = nn.Parameter(torch.zeros(out_features))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass com sampling"""
        
        # Sample weights
        weight_sigma = torch.log1p(torch.exp(self.weight_rho))
        weight = self.weight_mu + weight_sigma * torch.randn_like(weight_sigma)
        
        # Sample bias
        bias_sigma = torch.log1p(torch.exp(self.bias_rho))
        bias = self.bias_mu + bias_sigma * torch.randn_like(bias_sigma)
        
        return F.linear(x, weight, bias)

class UncertaintyEstimator:
    """Estima incerteza de predi√ß√µes"""
    
    def __init__(self, model: nn.Module, num_samples: int = 100):
        self.model = model
        self.num_samples = num_samples
    
    def predict_with_uncertainty(
        self,
        x: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Predi√ß√£o com estimativa de incerteza"""
        
        self.model.train()  # Enable dropout/sampling
        
        predictions = []
        
        # Monte Carlo sampling
        for _ in range(self.num_samples):
            with torch.no_grad():
                pred = self.model(x)
                predictions.append(pred)
        
        predictions = torch.stack(predictions)
        
        # Mean prediction
        mean_pred = predictions.mean(dim=0)
        
        # Epistemic uncertainty (variance)
        epistemic = predictions.var(dim=0)
        
        # Aleatoric uncertainty (data noise)
        # Simplifica√ß√£o: usa vari√¢ncia m√©dia
        aleatoric = epistemic.mean()
        
        return mean_pred, epistemic, aleatoric
    
    def get_confidence_interval(
        self,
        x: torch.Tensor,
        confidence_level: float = 0.95
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Calcula intervalo de confian√ßa"""
        
        self.model.train()
        predictions = []
        
        for _ in range(self.num_samples):
            with torch.no_grad():
                pred = self.model(x)
                predictions.append(pred)
        
        predictions = torch.stack(predictions)
        
        # Quantis para intervalo
        alpha = (1 - confidence_level) / 2
        lower = torch.quantile(predictions, alpha, dim=0)
        upper = torch.quantile(predictions, 1 - alpha, dim=0)
        
        return lower, upper
```

#### 3.2 Calibration Metrics

```python
class CalibrationMetrics:
    """M√©tricas de calibra√ß√£o de confian√ßa"""
    
    def expected_calibration_error(
        self,
        confidences: np.ndarray,
        predictions: np.ndarray,
        labels: np.ndarray,
        num_bins: int = 10
    ) -> float:
        """Calcula Expected Calibration Error (ECE)"""
        
        bin_boundaries = np.linspace(0, 1, num_bins + 1)
        ece = 0.0
        
        for i in range(num_bins):
            # Samples neste bin
            in_bin = (
                (confidences > bin_boundaries[i]) &
                (confidences <= bin_boundaries[i + 1])
            )
            
            if in_bin.sum() > 0:
                # Acur√°cia no bin
                bin_accuracy = (predictions[in_bin] == labels[in_bin]).mean()
                
                # Confian√ßa m√©dia no bin
                bin_confidence = confidences[in_bin].mean()
                
                # Peso do bin
                bin_weight = in_bin.sum() / len(confidences)
                
                # Contribui√ß√£o para ECE
                ece += bin_weight * abs(bin_accuracy - bin_confidence)
        
        return ece
    
    def plot_reliability_diagram(
        self,
        confidences: np.ndarray,
        predictions: np.ndarray,
        labels: np.ndarray,
        num_bins: int = 10
    ) -> None:
        """Plota diagrama de confiabilidade"""
        
        bin_boundaries = np.linspace(0, 1, num_bins + 1)
        bin_centers = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2
        
        accuracies = []
        avg_confidences = []
        
        for i in range(num_bins):
            in_bin = (
                (confidences > bin_boundaries[i]) &
                (confidences <= bin_boundaries[i + 1])
            )
            
            if in_bin.sum() > 0:
                accuracies.append(
                    (predictions[in_bin] == labels[in_bin]).mean()
                )
                avg_confidences.append(confidences[in_bin].mean())
            else:
                accuracies.append(0)
                avg_confidences.append(bin_centers[i])
        
        plt.figure(figsize=(8, 8))
        plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')
        plt.bar(
            bin_centers,
            accuracies,
            width=1/num_bins,
            alpha=0.7,
            label='Actual Accuracy'
        )
        plt.xlabel('Confidence')
        plt.ylabel('Accuracy')
        plt.title('Reliability Diagram')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
```

---

## üìä An√°lise de Viabilidade

### Computational Overhead

**M√©todos de XAI:**

| M√©todo | VRAM Overhead | Lat√™ncia | Qualidade |
|--------|--------------|----------|-----------|
| Attention Visualization | ~50MB | +20ms | Alta |
| GradCAM | ~100MB | +50ms | M√©dia |
| Decision Trees | 0MB (CPU) | +5ms | Alta |
| Natural Language | 0MB (CPU) | +30ms | M√©dia |
| Uncertainty (MC) | 0MB | +500ms (100 samples) | Alta |

### Use Cases

```python
class XAIOrchestrator:
    """Orquestra m√©todos de XAI baseado em contexto"""
    
    def __init__(self):
        self.attention_viz = AttentionVisualizer()
        self.nl_explainer = NaturalLanguageExplainer()
        self.uncertainty_estimator = UncertaintyEstimator(model, 50)
        
    def explain(
        self,
        decision: Dict[str, Any],
        explanation_type: str = "auto"
    ) -> Dict[str, Any]:
        """Gera explica√ß√£o apropriada"""
        
        explanations = {}
        
        # Sempre inclui confian√ßa
        uncertainty = self.uncertainty_estimator.predict_with_uncertainty(
            decision['input']
        )
        explanations['confidence'] = {
            'mean': uncertainty[0],
            'epistemic': uncertainty[1],
            'aleatoric': uncertainty[2]
        }
        
        # Explica√ß√£o natural
        if explanation_type in ["auto", "natural"]:
            explanations['natural_language'] = \
                self.nl_explainer.explain_decision(
                    decision['action'],
                    decision['reasoning'],
                    uncertainty[0]
                )
        
        # Visualiza√ß√µes (se aplic√°vel)
        if 'attention' in decision and explanation_type in ["auto", "visual"]:
            explanations['attention_heatmap'] = \
                self.attention_viz.visualize_attention(
                    decision['layer'],
                    decision['tokens']
                )
        
        return explanations
```

---

## üéØ Roadmap de Implementa√ß√£o

### Fase 1: Attention Visualization (1-2 semanas)

**Entreg√°veis:**
```python
# src/explainability/attention_viz.py
class AttentionVisualizer:
    """Visualiza√ß√£o de attention"""
```

### Fase 2: Natural Language Explanations (2 semanas)

**Entreg√°veis:**
```python
# src/explainability/nl_explainer.py
class NaturalLanguageExplainer:
    """Explica√ß√µes em linguagem natural"""
```

### Fase 3: Uncertainty Quantification (2-3 semanas)

**Entreg√°veis:**
```python
# src/explainability/uncertainty.py
class UncertaintyEstimator:
    """Estimativa de incerteza"""
```

### Fase 4: Integration (1 semana)

**Entreg√°veis:**
```python
# src/explainability/xai_orchestrator.py
class XAIOrchestrator:
    """Orquestrador de XAI"""
```

---

## üìà M√©tricas de Sucesso

| M√©trica | Target | Medi√ß√£o |
|---------|--------|---------|
| ECE (Calibration) | <0.1 | Calibration tests |
| Explanation Quality | >80% human approval | User studies |
| Latency Overhead | <100ms | Benchmarks |
| Coverage | 100% decisions | Code coverage |

---

## üìö Refer√™ncias

1. **Ribeiro, M. T., et al. (2016).** "Why Should I Trust You?" *KDD 2016*
2. **Selvaraju, R. R., et al. (2017).** "Grad-CAM: Visual Explanations from Deep Networks" *ICCV 2017*
3. **Guo, C., et al. (2017).** "On Calibration of Modern Neural Networks" *ICML 2017*

---

## ‚úÖ Conclus√µes

### Conclus√µes da Fase Beta

1. ‚úÖ **Viabilidade:** XAI √© essencial e implement√°vel
2. ‚úÖ **Overhead:** Aceit√°vel (<100ms)
3. ‚úÖ **Compliance:** Atende LGPD/GDPR

### Pr√≥ximos Passos

- [ ] Implementar AttentionVisualizer
- [ ] Desenvolver NaturalLanguageExplainer
- [ ] Criar UncertaintyEstimator
- [ ] Integrar com agents existentes

---

**Status:** üìã Beta - Pronto para Implementa√ß√£o  
**Prioridade:** Alta (Compliance)  
**Aprova√ß√£o:** Aguardando decis√£o
