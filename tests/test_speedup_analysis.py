#!/usr/bin/env python3
"""
Teste de Performance Otimizada - Speedup Real da Vetoriza√ß√£o

Este teste demonstra o verdadeiro speedup da vetoriza√ß√£o usando:
- Muitos m√≥dulos (20-50) para mostrar benef√≠cio da paraleliza√ß√£o
- Compara√ß√£o justa: mesmo workload computacional
- M√©tricas detalhadas de performance
"""

import asyncio
import logging
import time
from typing import Dict, List

import numpy as np
import torch

from src.consciousness.shared_workspace import SharedWorkspace, CrossPredictionMetrics

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def test_real_speedup():
    """Teste de speedup real com muitos m√≥dulos."""
    logger.info("üöÄ TESTE DE SPEEDUP REAL - Vetoriza√ß√£o com Muitos M√≥dulos")
    logger.info("=" * 80)

    # Testar com diferentes n√∫meros de m√≥dulos
    module_counts = [4, 10, 20, 30]  # Escalar para ver quando compensa

    results = {}

    for n_modules in module_counts:
        logger.info(f"\nüî¨ Testando com {n_modules} m√≥dulos")
        logger.info("-" * 50)

        # Criar workspace
        workspace = SharedWorkspace(embedding_dim=256, max_history_size=2000)

        # Gerar m√≥dulos dinamicamente
        modules = [f"module_{i:02d}" for i in range(n_modules)]

        # Gerar dados sint√©ticos com rela√ß√µes causais
        np.random.seed(42)
        n_timesteps = 100  # Suficiente para causalidade

        logger.info(f"üìä Gerando dados para {n_modules} m√≥dulos...")

        for module in modules:
            for t in range(n_timesteps):
                # Embedding base com sinal causal simples
                base_embedding = np.random.randn(256) * 0.1

                # Adicionar sinal causal baseado no √≠ndice do m√≥dulo
                module_idx = int(module.split("_")[1])
                causal_signal = np.sin(t * 0.05 + module_idx * 0.1) * 0.3
                base_embedding += causal_signal

                workspace.write_module_state(module, base_embedding)

        # Avan√ßar ciclos
        for _ in range(3):
            workspace.advance_cycle()

        logger.info("‚úÖ Dados gerados. Testando performance...")

        # TESTE: Compara√ß√£o justa de performance
        history_window = 50

        # M√©todo 1: Individual (baseline) - s√≥ m√≥dulos com hist√≥rico suficiente
        logger.info("   üîÑ Executando m√©todo individual...")
        start_time = time.time()

        individual_count = 0
        # Filtrar apenas m√≥dulos com hist√≥rico suficiente (igual ao vetorizado)
        valid_modules = []
        for module in modules:
            history = workspace.get_module_history(module, history_window)
            if len(history) >= 2:
                valid_modules.append(module)

        logger.info(
            f"   üìä Usando {len(valid_modules)} m√≥dulos v√°lidos (de {len(modules)} criados)"
        )

        for i, source in enumerate(valid_modules):
            for j, target in enumerate(valid_modules):
                if i != j:  # N√£o auto-predi√ß√£o
                    pred = workspace.compute_cross_prediction(source, target, history_window)
                    individual_count += 1

        individual_time = (time.time() - start_time) * 1000

        # M√©todo 2: Vetorizado
        logger.info("   ‚ö° Executando m√©todo vetorizado...")
        start_time = time.time()

        vectorized_predictions = workspace.compute_all_cross_predictions_vectorized(
            history_window=history_window, use_gpu=torch.cuda.is_available(), force_recompute=True
        )

        vectorized_time = (time.time() - start_time) * 1000

        # Calcular m√©tricas
        real_speedup = individual_time / vectorized_time if vectorized_time > 0 else 1.0
        vectorized_count = (
            len(vectorized_predictions) * len(list(vectorized_predictions.values())[0])
            if vectorized_predictions
            else 0
        )

        # Verificar se mesma quantidade de predi√ß√µes
        if individual_count != vectorized_count:
            logger.warning(
                f"   ‚ö†Ô∏è Contagem diferente: individual={individual_count}, vetorizado={vectorized_count}"
            )
            logger.warning(
                f"   M√≥dulos v√°lidos: {len(valid_modules)}, Predi√ß√µes esperadas: {len(valid_modules) * (len(valid_modules) - 1)}"
            )

        # M√©tricas por predi√ß√£o
        individual_per_pred = individual_time / individual_count if individual_count > 0 else 0
        vectorized_per_pred = vectorized_time / vectorized_count if vectorized_count > 0 else 0

        logger.info("üìä Resultados:")
        logger.info(f"   Predi√ß√µes calculadas: {individual_count}")
        logger.info(
            f"   Individual: {individual_time:.1f}ms total ({individual_per_pred:.2f}ms/pred)"
        )
        logger.info(
            f"   Vetorizado: {vectorized_time:.1f}ms total ({vectorized_per_pred:.2f}ms/pred)"
        )
        logger.info(f"   Speedup real: {real_speedup:.2f}x")
        logger.info(f"   GPU: {'Sim' if torch.cuda.is_available() else 'N√£o'}")

        results[n_modules] = {
            "individual_time": individual_time,
            "vectorized_time": vectorized_time,
            "speedup": real_speedup,
            "predictions": individual_count,
            "individual_per_pred": individual_per_pred,
            "vectorized_per_pred": vectorized_per_pred,
        }

    # AN√ÅLISE DOS RESULTADOS
    logger.info("\nüéØ AN√ÅLISE DE SPEEDUP POR ESCALA")
    logger.info("=" * 80)

    logger.info("üìà Speedup por n√∫mero de m√≥dulos:")
    for n_modules, data in results.items():
        speedup = data["speedup"]
        status = "‚úÖ" if speedup > 1.0 else "‚ùå"
        logger.info(f"   {n_modules} m√≥dulos: {speedup:.2f}x {status}")

    # Encontrar ponto de equil√≠brio
    break_even = None
    for n_modules in sorted(results.keys()):
        if results[n_modules]["speedup"] > 1.0:
            break_even = n_modules
            break

    if break_even:
        logger.info(
            f"\nüí° PONTO DE EQUIL√çBRIO: Vetoriza√ß√£o compensa a partir de {break_even} m√≥dulos"
        )
    else:
        logger.info(f"\n‚ö†Ô∏è Vetoriza√ß√£o ainda n√£o compensa mesmo com {max(module_counts)} m√≥dulos")

    # TESTE DE CACHE OTIMIZADO
    logger.info("\nüíæ TESTE DE CACHE AVAN√áADO")
    logger.info("-" * 50)

    # Usar configura√ß√£o otimizada (20 m√≥dulos)
    n_modules = 20
    workspace = SharedWorkspace(embedding_dim=256, max_history_size=2000)
    modules = [f"module_{i:02d}" for i in range(n_modules)]

    # Gerar dados
    np.random.seed(42)
    for module in modules:
        for t in range(100):
            base_embedding = np.random.randn(256) * 0.1
            module_idx = int(module.split("_")[1])
            causal_signal = np.sin(t * 0.05 + module_idx * 0.1) * 0.3
            base_embedding += causal_signal
            workspace.write_module_state(module, base_embedding)

    # Teste de cache com diferentes cen√°rios
    cache_scenarios = [
        ("Cache frio", True),  # For√ßar rec√°lculo
        ("Cache quente", False),  # Usar cache
        ("Cache invalidado", False),  # Invalidar e usar
    ]

    cache_times = {}

    for scenario_name, force_recompute in cache_scenarios:
        if scenario_name == "Cache invalidado":
            # Invalidar alguns m√≥dulos
            if workspace._vectorized_predictor is not None:
                for i in range(0, n_modules, 5):  # Invalidar 20% dos m√≥dulos
                    workspace._vectorized_predictor.invalidate_module_cache(modules[i])
        result = workspace.compute_all_cross_predictions_vectorized(
            history_window=50, force_recompute=force_recompute
        )
        elapsed = (time.time() - start_time) * 1000

        cache_times[scenario_name] = elapsed
        logger.info(f"   {scenario_name}: {elapsed:.1f}ms")

    # Calcular efici√™ncia do cache
    cold_time = cache_times["Cache frio"]
    hot_time = cache_times["Cache quente"]
    invalidated_time = cache_times["Cache invalidado"]

    cache_speedup = cold_time / hot_time if hot_time > 0 else 1.0
    cache_overhead = invalidated_time - hot_time  # Overhead da invalida√ß√£o

    logger.info("üìä Efici√™ncia do Cache:")
    logger.info(f"   Speedup cache: {cache_speedup:.2f}x")
    logger.info(f"   Overhead invalida√ß√£o: {cache_overhead:.1f}ms")

    # RECOMENDA√á√ïES DE OTIMIZA√á√ÉO
    logger.info("\nüéØ RECOMENDA√á√ïES DE OTIMIZA√á√ÉO")
    logger.info("=" * 80)

    recommendations = []

    if break_even and break_even <= 10:
        recommendations.append("‚úÖ Vetoriza√ß√£o: Boa para sistemas com 10+ m√≥dulos")
    elif break_even and break_even > 20:
        recommendations.append("‚ö†Ô∏è Vetoriza√ß√£o: Otimizar overhead para compensar com menos m√≥dulos")
    else:
        recommendations.append("‚ùå Vetoriza√ß√£o: Overhead muito alto, precisa otimiza√ß√£o")

    if cache_speedup > 2.0:
        recommendations.append("‚úÖ Cache: Excelente performance")
    elif cache_speedup > 1.5:
        recommendations.append("‚ö†Ô∏è Cache: Funcional, mas pode melhorar")
    else:
        recommendations.append("‚ùå Cache: Overhead alto, otimizar lookups")

    if torch.cuda.is_available():
        recommendations.append("‚úÖ GPU: Dispon√≠vel e sendo utilizada")
    else:
        recommendations.append("‚ö†Ô∏è GPU: Instalar PyTorch GPU para speedup adicional")

    for rec in recommendations:
        logger.info(f"   {rec}")

    # RESULTADO FINAL
    overall_score = 0
    if break_even and break_even <= 15:
        overall_score += 1
    if cache_speedup > 1.8:
        overall_score += 1
    if torch.cuda.is_available():
        overall_score += 1

    overall_rating = overall_score / 3

    logger.info(f"\nüèÜ AVALIA√á√ÉO GERAL: {overall_score}/3 ({overall_rating:.0%})")

    if overall_rating >= 0.8:
        logger.info("üéâ Otimiza√ß√µes funcionando bem! Sistema pronto para produ√ß√£o.")
    elif overall_rating >= 0.6:
        logger.info("‚ö†Ô∏è Otimiza√ß√µes funcionais, mas precisam ajustes.")
    else:
        logger.info("‚ùå Otimiza√ß√µes precisam revis√£o significativa.")

    return {
        "results": results,
        "break_even_modules": break_even,
        "cache_speedup": cache_speedup,
        "cache_overhead": cache_overhead,
        "overall_score": overall_rating,
    }


if __name__ == "__main__":
    try:
        import torch

        result = asyncio.run(test_real_speedup())

        print("\nüìä Resumo Executivo:")
        print(f"   Ponto de equil√≠brio: {result['break_even_modules']} m√≥dulos")
        print(f"   Cache speedup: {result['cache_speedup']:.2f}x")
        print(f"   Score geral: {result['overall_score']:.0%}")

    except ImportError:
        logger.error("‚ùå PyTorch n√£o encontrado. Instale com: pip install torch")
        logger.info("üí° Performance limitada sem GPU")
