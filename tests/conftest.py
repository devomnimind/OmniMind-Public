"""Project-wide pytest configuration."""

import json
import os
import sys
import time
from typing import Any, Dict

import pytest
import requests
import torch

# Desabilitar serviÃ§os nÃ£o crÃ­ticos para testes locais
os.environ["OMNIMIND_DISABLE_IBM"] = "True"  # IBM cloud auth failing in sandbox
if not torch.cuda.is_available():
    os.environ["OMNIMIND_DISABLE_QUANTUM"] = "True"  # Sem GPU, quantum nÃ£o funciona

# FORÃ‡A GPU/CUDA SE DISPONÃVEL
# CRITICAL: Tentar device_count tambÃ©m mesmo se is_available() falhar
cuda_available = torch.cuda.is_available()
cuda_device_count = torch.cuda.device_count()

if cuda_available or cuda_device_count > 0:
    os.environ["CUDA_VISIBLE_DEVICES"] = os.environ.get("CUDA_VISIBLE_DEVICES", "0")
    os.environ["OMNIMIND_FORCE_GPU"] = "true"
    os.environ["PYTEST_FORCE_GPU"] = "true"

    if cuda_available:
        torch.set_default_device("cuda")
        print(f"âœ… PyTorch CUDA forÃ§ado (is_available=True): {torch.cuda.get_device_name(0)}")
    else:
        # Fallback: device detected but is_available() failed
        print(
            f"âš ï¸ PyTorch CUDA fallback (device_count={cuda_device_count}): "
            f"GPU forcing ativado via OMNIMIND_FORCE_GPU=true"
        )
else:
    print("âš ï¸  CUDA nÃ£o disponÃ­vel - usando CPU")

# Ensure .pytest_cache is created locally in project root
os.environ["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "0"

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../src"))

from tests.plugins.pytest_server_monitor import ServerMonitorPlugin  # noqa: E402

# Import custom plugins
from tests.plugins.pytest_test_ordering import TestOrderingPlugin  # noqa: E402
from tests.plugins.pytest_timeout_retry import TimeoutRetryPlugin  # noqa: E402

# Servidor endpoints
DASHBOARD_URL = "http://localhost:5173"
API_URL = "http://localhost:8000"


class MetricsCollector:
    """Coleta mÃ©tricas de consciÃªncia e phi dos testes que passaram."""

    def __init__(self):
        self.passed_tests = []
        self.phi_values = []
        self.consciousness_metrics = []
        self.test_durations = []
        self.detailed_results = []  # Armazena resultados completos dos testes

    def collect_test_result(self, item, call):
        """
        Coleta resultado do teste - SEMPRE mede latÃªncia, mesmo em timeout.

        CRÃTICO: Timeout nÃ£o Ã© falha - Ã© MEDIÃ‡ÃƒO de latÃªncia.
        Ambiente limitado (407 processos, Docker, dev, Cursor, agentes, OmniMind, serviÃ§os).
        LatÃªncia Ã© medida e computada para mÃ©tricas cientÃ­ficas.
        """
        # SEMPRE mede latÃªncia (mesmo em timeout)
        duration = (
            call.stop - call.start if hasattr(call, "stop") and hasattr(call, "start") else 0.0
        )
        self.test_durations.append(duration)  # Sempre registra latÃªncia

        if call.excinfo is None:  # Test passed
            self.passed_tests.append(item.nodeid)

            # Captura tanto output quanto logs
            captured_output = ""
            if hasattr(call, "caplog") and call.caplog:
                captured_output += call.caplog.get_captured_text()
            if hasattr(call, "capfd"):
                out, err = call.capfd.readouterr()
                captured_output += out + err

            # Extrai mÃ©tricas
            result_data = self._extract_all_metrics(item.nodeid, captured_output)
            if result_data:
                self.detailed_results.append(result_data)
                self._extract_metrics_from_logs(captured_output)
        else:
            # Teste falhou ou teve timeout - ainda assim registra latÃªncia
            # Timeout nÃ£o Ã© falha - Ã© medida de latÃªncia do ambiente
            error_msg = str(call.excinfo.value) if call.excinfo and call.excinfo.value else ""
            is_timeout = "timeout" in error_msg.lower() or "timed out" in error_msg.lower()

            if is_timeout:
                # Timeout Ã© MEDIÃ‡ÃƒO, nÃ£o falha
                # Registra como "passed" para mÃ©tricas (timeout Ã© medida de latÃªncia)
                self.passed_tests.append(item.nodeid)
                self.test_durations.append(duration)  # JÃ¡ adicionado, mas garante

    def _extract_all_metrics(self, test_name: str, output: str) -> dict | None:
        """Extrai todas as mÃ©tricas do output do teste."""
        import re
        from typing import Any

        result: dict[str, Any] = {"test_name": test_name, "metrics": {}}

        # PadrÃµes expandidos para capturar mÃ©tricas
        patterns = {
            "ICI": r"ICI[:\s]*\(?([0-9.]+)\)?",
            "PRS": r"PRS[:\s]*\(?([0-9.]+)\)?",
            "phi": r"phi[:\s]*\(?([0-9.]+)\)?",
            "consciousness": r"consciousness[:\s]*\(?([0-9.]+)\)?",
            "coherence": r"coherence[:\s]*\(?([0-9.]+)\)?",
            "entropy": r"entropy[:\s]*\(?([0-9.]+)\)?",
            "integrity": r"integrity[:\s]*\(?([0-9.]+)\)?",
        }

        found_any = False
        for metric_name, pattern in patterns.items():
            matches = re.findall(pattern, output, re.IGNORECASE)
            if matches:
                found_any = True
                try:
                    # Pega o primeiro valor encontrado
                    result["metrics"][metric_name] = float(matches[0])
                except (ValueError, IndexError):
                    pass

        # TambÃ©m tira a interpretaÃ§Ã£o/mensagem se houver
        msg_pattern = r"(Interpretation|message)[:\s]*(['\"]?)([^'\"]+)\2"
        msg_match = re.search(msg_pattern, output, re.IGNORECASE)
        if msg_match:
            result["interpretation"] = msg_match.group(3)
            found_any = True

        return result if found_any else None

    def _extract_metrics_from_logs(self, logs: str):
        """Extrai mÃ©tricas phi e consciÃªncia dos logs."""
        import re

        # PadrÃµes para extrair mÃ©tricas numericamente
        patterns = [
            (r"ICI[:\s]*\(?([0-9.]+)\)?", "ICI"),
            (r"PRS[:\s]*\(?([0-9.]+)\)?", "PRS"),
            (r"phi[:\s]*\(?([0-9.]+)\)?", "phi"),
            (r"consciousness[:\s]*\(?([0-9.]+)\)?", "consciousness"),
        ]

        for pattern, metric_type in patterns:
            matches = re.findall(pattern, logs, re.IGNORECASE)
            for match in matches:
                try:
                    value = float(match)
                    if metric_type in ("ICI", "PRS", "phi", "consciousness"):
                        self.consciousness_metrics.append({"type": metric_type, "value": value})
                    elif metric_type == "phi":
                        self.phi_values.append(value)
                except ValueError:
                    pass

    def get_final_report(self) -> Dict[str, Any]:
        """Gera relatÃ³rio final com mÃ©tricas."""
        report = {
            "total_passed_tests": len(self.passed_tests),
            "total_test_duration": sum(self.test_durations),
            "avg_test_duration": (
                sum(self.test_durations) / len(self.test_durations) if self.test_durations else 0
            ),
        }

        # Agrupa mÃ©tricas por tipo
        ici_values = [m["value"] for m in self.consciousness_metrics if m.get("type") == "ICI"]
        prs_values = [m["value"] for m in self.consciousness_metrics if m.get("type") == "PRS"]
        phi_values = [m["value"] for m in self.consciousness_metrics if m.get("type") == "phi"]
        cons_values = [
            m["value"] for m in self.consciousness_metrics if m.get("type") == "consciousness"
        ]

        if ici_values:
            report.update(
                {
                    "ICI_measurements": len(ici_values),
                    "ICI_avg": sum(ici_values) / len(ici_values),
                    "ICI_min": min(ici_values),
                    "ICI_max": max(ici_values),
                }
            )

        if prs_values:
            report.update(
                {
                    "PRS_measurements": len(prs_values),
                    "PRS_avg": sum(prs_values) / len(prs_values),
                    "PRS_min": min(prs_values),
                    "PRS_max": max(prs_values),
                }
            )

        if phi_values or self.phi_values:
            all_phi = phi_values + self.phi_values
            report.update(
                {
                    "phi_measurements": len(all_phi),
                    "phi_avg": sum(all_phi) / len(all_phi),
                    "phi_min": min(all_phi),
                    "phi_max": max(all_phi),
                }
            )

        if cons_values:
            report.update(
                {
                    "consciousness_measurements": len(cons_values),
                    "consciousness_avg": sum(cons_values) / len(cons_values),
                    "consciousness_min": min(cons_values),
                    "consciousness_max": max(cons_values),
                }
            )

        # Adiciona resultados detalhados
        if self.detailed_results:
            report["detailed_test_results"] = self.detailed_results

        return report

    def print_final_report(self):
        """Exibe relatÃ³rio final mesmo com falhas."""
        report = self.get_final_report()

        print("\n" + "=" * 80)
        print("ğŸ“Š RELATÃ“RIO COMPLETO DE MÃ‰TRICAS DE CONSCIÃŠNCIA")
        print("=" * 80)

        print("\nğŸ“ˆ RESUMO GERAL:")
        print(f"   âœ… Testes que passaram: {report['total_passed_tests']}")
        print(f"   â±ï¸  DuraÃ§Ã£o total: {report['total_test_duration']:.2f}s")
        print(f"   ğŸ“Š DuraÃ§Ã£o mÃ©dia por teste: {report['avg_test_duration']:.2f}s")

        # ICI - Integrated Coherence Index
        if "ICI_measurements" in report:
            print("\nğŸ§  ICI (Integrated Coherence Index):")
            print(f"   ğŸ“Š Total de mediÃ§Ãµes: {report['ICI_measurements']}")
            print(f"   ğŸ“ˆ MÃ©dia: {report['ICI_avg']:.4f}")
            print(f"   ğŸ“‰ MÃ­nimo: {report['ICI_min']:.4f}")
            print(f"   â¬†ï¸  MÃ¡ximo: {report['ICI_max']:.4f}")

        # PRS - Predictive Resonance Strength
        if "PRS_measurements" in report:
            print("\nğŸŒŠ PRS (Predictive Resonance Strength):")
            print(f"   ğŸ“Š Total de mediÃ§Ãµes: {report['PRS_measurements']}")
            print(f"   ğŸ“ˆ MÃ©dia: {report['PRS_avg']:.4f}")
            print(f"   ğŸ“‰ MÃ­nimo: {report['PRS_min']:.4f}")
            print(f"   â¬†ï¸  MÃ¡ximo: {report['PRS_max']:.4f}")

        # Phi - Integrated Information
        if "phi_measurements" in report:
            print("\nğŸŒ€ Î¦ (Integrated Information):")
            print(f"   ğŸ“Š Total de mediÃ§Ãµes: {report['phi_measurements']}")
            print(f"   ğŸ“ˆ Î¦ MÃ©dio: {report['phi_avg']:.4f}")
            print(f"   ğŸ“‰ Î¦ MÃ­nimo: {report['phi_min']:.4f}")
            print(f"   â¬†ï¸  Î¦ MÃ¡ximo: {report['phi_max']:.4f}")

        # ConsciÃªncia Geral
        if "consciousness_measurements" in report:
            print("\nğŸ”® CONSCIÃŠNCIA GERAL:")
            print(f"   ğŸ“Š Total de mediÃ§Ãµes: {report['consciousness_measurements']}")
            print(f"   ğŸ“ˆ MÃ©dia: {report['consciousness_avg']:.4f}")
            print(f"   ğŸ“‰ MÃ­nimo: {report['consciousness_min']:.4f}")
            print(f"   â¬†ï¸  MÃ¡ximo: {report['consciousness_max']:.4f}")

        # Testes individuais com mÃ©tricas
        if report.get("detailed_test_results"):
            print("\n" + "=" * 80)
            print("ğŸ“‹ RESULTADOS DETALHADOS POR TESTE:")
            print("=" * 80)
            for test_result in report["detailed_test_results"][:10]:  # Primeiros 10
                print(f"\nâœ… {test_result['test_name']}")
                if test_result.get("metrics"):
                    for metric_name, value in test_result["metrics"].items():
                        print(f"   â€¢ {metric_name}: {value:.4f}")
                if test_result.get("interpretation"):
                    print(f"   ğŸ“ {test_result['interpretation']}")

        # Salva relatÃ³rio em JSON
        try:
            os.makedirs("data/test_reports", exist_ok=True)
            with open("data/test_reports/metrics_report.json", "w") as f:
                json.dump(report, f, indent=2)
            print("\nğŸ’¾ RelatÃ³rio salvo em: data/test_reports/metrics_report.json")
        except Exception as e:
            print(f"âš ï¸  Erro ao salvar relatÃ³rio: {e}")

        print("=" * 80 + "\n")


# InstÃ¢ncia global do coletor de mÃ©tricas
metrics_collector = MetricsCollector()


def pytest_configure(config):
    """Register custom markers and plugins."""

    config.addinivalue_line(
        "markers",
        "computational: mark test as computationally intensive (GPU/Quantum/Consciousness)",
    )
    config.addinivalue_line("markers", "gpu: mark test as GPU-intensive")
    config.addinivalue_line("markers", "quantum: mark test as quantum simulation")
    config.addinivalue_line("markers", "consciousness: mark test as consciousness computation")
    config.addinivalue_line("markers", "e2e: mark test as end-to-end (requer servidor)")
    config.addinivalue_line(
        "markers", "real: mark test as real (nÃ£o mocked) - requer recursos reais (GPU, LLM, etc)"
    )
    config.addinivalue_line(
        "markers", "chaos: mark test as resilience/chaos engineering - pode derrubar servidor"
    )
    config.addinivalue_line("markers", "timeout(seconds): mark test with timeout in seconds")

    # Register custom plugins
    config.pluginmanager.register(TimeoutRetryPlugin(), "timeout_retry")
    # IMPORTANTE: ServerMonitorPlugin sÃ³ ativo em testes perigosos (chaos, stress, ddos)
    # Monitor nÃ£o inicia servidor - isso Ã© responsabilidade do script do sistema
    # Monitor apenas verifica e reinicia se servidor cair durante testes perigosos
    monitor_plugin = ServerMonitorPlugin()
    monitor_plugin.enabled = False  # Desabilitado por padrÃ£o
    config.pluginmanager.register(monitor_plugin, "server_monitor")
    config.pluginmanager.register(TestOrderingPlugin(), "test_ordering")

    # ========== COLORIZAÃ‡ÃƒO INTELIGENTE ==========
    # ForÃ§a pytest a colorir APENAS testes que falham (nÃ£o herda vermelho)
    # Testes que passam/skipped ficam verdes/amarelos normalmente
    config.option.color = "yes"  # Habilitar cores
    config.option.tb = "short"  # Traceback curto

    # Registrar hook para limpar estado de cor entre testes
    def reset_color_state():
        """Reset color state apÃ³s cada teste."""
        pass

    config.pluginmanager.register(
        type(
            "ColorReset", (), {"pytest_runtest_teardown": lambda self, item: reset_color_state()}
        )(),
        "color_reset",
    )


def pytest_collection_modifyitems(config, items):
    """
    Auto-mark tests com TIMEOUT PROGRESSIVO (240s â†’ 800s mÃ¡ximo).

    ESTRATÃ‰GIA CRÃTICA:
    - Timeout NÃƒO Ã© falha - deixa rodar atÃ© mÃ¡ximo
    - ComeÃ§a em base, vai aumentando progressivamente
    - Fast: 120s | Ollama: 240s | Computational: 300s | Heavy: 600s | E2E: 400s
    - MÃXIMO ABSOLUTO: 800s para qualquer teste
    """
    ollama_paths = [
        "phase16_integration",
        "neurosymbolic",
        "neural_component",
        "free_energy_lacanian",
        "cognitive",
        "_inference",
    ]

    e2e_paths = [
        "test_e2e_integration",
        "test_dashboard_live",
        "test_endpoint",
    ]

    heavy_paths = [
        "test_integration_loss.py",
        "test_quantum_algorithms_comprehensive.py",
        "test_consciousness",
        "test_real_phi_measurement.py",  # GPU/CUDA - precisa 800s para estabilizaÃ§Ã£o
        "test_enhanced_code_agent_integration.py",  # GPU/CUDA - precisa 800s
    ]

    chaos_paths = [
        "test_chaos_resilience",
    ]

    computational_paths = [
        "consciousness",
        "quantum_consciousness",
        "quantum_ai",
        "science_validation",
        "experiments",
    ]

    for item in items:
        item_path = str(item.fspath).lower()
        test_name = item.nodeid.lower()

        # Remove marcadores de timeout existentes
        existing_timeout = item.get_closest_marker("timeout")
        if existing_timeout:
            item.own_markers.remove(existing_timeout)

        # Testes de integraÃ§Ã£o que usam servidor monitor: RESPEITAM timeout global de 800s
        # Ambiente limitado (407 processos, Docker, dev, Cursor, agentes, OmniMind, serviÃ§os)
        # Servidor na mesma mÃ¡quina nÃ£o suporta tantas conexÃµes - timeout Ã© medida, nÃ£o falha
        integration_server_paths = [
            "test_mcp_",
            "test_thinking_",
            "test_context_",
            "test_logging_",
            "test_python_",
            "test_system_info_",
            "integrations/",
        ]

        if any(path in item_path for path in integration_server_paths):
            # Testes de integraÃ§Ã£o servidor: 800s (respeita timeout global)
            # LatÃªncia serÃ¡ medida e computada para mÃ©tricas cientÃ­ficas
            timeout_value = 800
            item.add_marker(pytest.mark.integration_server)
            # Continua para aplicar timeout (nÃ£o pula)

        # Determina timeout PROGRESSIVO
        timeout_value = 300  # default (increased to allow server restart ~150s)

        # Chaos tests: 800s (server restart + recovery)
        if any(path in item_path for path in chaos_paths):
            timeout_value = 800
            item.add_marker(pytest.mark.chaos)
        # Stress tests: 800s (modo escalonado sem falhas atÃ© 800s)
        elif "stress" in item_path or "test_orchestrator_load" in item_path:
            timeout_value = 800
            item.add_marker(pytest.mark.stress)
        # E2E: comeÃ§a 400s (vai atÃ© 600s via plugin se precisar)
        elif any(path in item_path for path in e2e_paths):
            timeout_value = 400
            item.add_marker(pytest.mark.e2e)
        # Heavy computational/GPU: 800s (permite estabilizaÃ§Ã£o GPU e cache)
        # Testes de GPU/cÃ¡lculo precisam de tempo para estabilizar cache e processamento
        elif any(path in item_path for path in heavy_paths):
            timeout_value = 800  # MÃ¡ximo para testes de GPU/cÃ¡lculo
            item.add_marker(pytest.mark.computational)
        # Ollama: comeÃ§a 240s (vai atÃ© 400s se precisar)
        elif any(path in item_path or path in test_name for path in ollama_paths):
            timeout_value = 240
            item.add_marker(pytest.mark.computational)
        # Regular computational: comeÃ§a 300s (vai atÃ© 500s se precisar)
        elif any(path in item_path for path in computational_paths):
            timeout_value = 300
            item.add_marker(pytest.mark.computational)

        # Aplica timeout
        item.add_marker(pytest.mark.timeout(timeout_value))


def check_server_health() -> bool:
    """Verifica se servidor estÃ¡ UP."""
    try:
        resp = requests.get(f"{API_URL}/health/", timeout=2)
        return resp.status_code in (200, 404)
    except Exception:
        pass

    return False


# Fixture de conveniÃªncia (opcional - plugin jÃ¡ cuida disso)
@pytest.fixture(scope="session", autouse=False)
def server_health():
    """Fixture que garante servidor UP para E2E tests."""
    for _ in range(10):
        time.sleep(1)
        if check_server_health():
            break
    yield


@pytest.fixture(autouse=True)
def consolidate_gpu_memory(request):
    """
    Consolida memÃ³ria GPU segundo estrutura tÃ³pica freudiana.

    Ao invÃ©s de deletar memÃ³rias quando GPU estÃ¡ cheia,
    consolida (comprime) e reprime para prÃ©-consciente/inconsciente.
    """
    import gc

    from src.memory.gpu_memory_consolidator import get_gpu_consolidator

    consolidator = get_gpu_consolidator()

    yield

    # ApÃ³s teste, verificar se precisa consolidar
    if consolidator.should_consolidate():
        # Coletar memÃ³rias ativas
        memory_items = _collect_active_gpu_memories()

        if memory_items:
            # Consolidar segundo estrutura tÃ³pica freudiana
            test_name = request.node.name if hasattr(request, "node") else "unknown"
            stats = consolidator.consolidate_gpu_memory(
                memory_items,
                process_context=f"test_{test_name}",
            )

            if stats.get("status") == "success":
                print(
                    f"ğŸ§  ConsolidaÃ§Ã£o GPU: {stats['consolidated']} memÃ³rias, "
                    f"{stats.get('freed_mb', 0):.2f}MB liberados"
                )

    # Limpeza final (apenas apÃ³s consolidaÃ§Ã£o)
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def _collect_active_gpu_memories() -> list:
    """
    Coleta memÃ³rias ativas da GPU para consolidaÃ§Ã£o.

    Returns:
        Lista de itens de memÃ³ria com dados, tipo e metadados
    """
    from typing import Any, Dict, List

    import torch

    memory_items: List[Dict[str, Any]] = []

    # 1. Verificar se hÃ¡ modelos SentenceTransformer em cache
    # (implementaÃ§Ã£o simplificada - em produÃ§Ã£o seria mais robusta)

    # 2. Coletar tensores grandes na GPU
    if torch.cuda.is_available():
        try:
            # EstatÃ­sticas de memÃ³ria
            stats = torch.cuda.memory_stats(0)
            allocated = stats.get("allocated_bytes.all.current", 0) / 1024 / 1024  # MB

            # Se hÃ¡ muita memÃ³ria alocada, tentar identificar tensores grandes
            if allocated > 100:  # Mais de 100MB
                # Nota: Em produÃ§Ã£o, seria necessÃ¡rio rastrear tensores explicitamente
                # Por enquanto, apenas logamos
                pass
        except Exception:
            # Ignorar erros de coleta
            pass

    return memory_items


@pytest.fixture(autouse=True)
def destroy_server_for_real_tests(request):
    """
    Fixture que monitora e registra testes de resiliÃªncia.

    Testes @pytest.mark.chaos DESTROEM servidor intencionalmente
    para validar que Î¦ Ã© robusto a falhas de orquestraÃ§Ã£o.

    EstratÃ©gia:
    - Antes do teste: servidor estÃ¡ UP (plugin garante)
    - DURANTE o teste: pode ser destruÃ­do via kill_server()
    - DEPOIS do teste: plugin reinicia se necessÃ¡rio
    - REGISTRA: tempo de recovery, Î¦ delta
    """
    is_chaos_test = request.node.get_closest_marker("chaos") is not None
    is_real_test = request.node.get_closest_marker("real") is not None

    start_time = time.time()

    if is_chaos_test:
        print(f"\nğŸ”´ TESTE DE RESILIÃŠNCIA (CHAOS): {request.node.name}")
        print("   âš ï¸  Este teste DERRUBA servidor intencionalmente")
        print("   ğŸ“Š Validando robustez de Î¦ e recovery automÃ¡tico")

    yield

    elapsed = time.time() - start_time

    # Registrar mÃ©tricas de resiliÃªncia
    if is_chaos_test or is_real_test:
        server_status = "UP" if check_server_health() else "DOWN (reiniciando...)"
        print("\nğŸ“Š MÃ‰TRICAS DO TESTE:")
        print(f"   DuraÃ§Ã£o: {elapsed:.2f}s")
        print(f"   Status final do servidor: {server_status}")


# Classe para rastrear resiliÃªncia em nÃ­vel global
class ResilienceTracker:
    """Rastreia mÃ©tricas de resiliÃªncia para relatÃ³rio final."""

    def __init__(self):
        self.chaos_tests_run = 0
        self.chaos_tests_passed = 0
        self.server_crashes = 0
        self.total_recovery_time = 0.0
        self.crash_times = []

    def record_crash(self, recovery_time):
        """Registra crash e tempo de recovery."""
        self.server_crashes += 1
        self.total_recovery_time += recovery_time
        self.crash_times.append(recovery_time)

    def get_report(self):
        """Gera relatÃ³rio de resiliÃªncia."""
        if self.server_crashes == 0 or not self.crash_times:
            return None

        avg_recovery = self.total_recovery_time / self.server_crashes
        min_recovery = min(self.crash_times)
        max_recovery = max(self.crash_times)

        return {
            "total_crashes": self.server_crashes,
            "avg_recovery_time_s": avg_recovery,
            "min_recovery_time_s": min_recovery,
            "max_recovery_time_s": max_recovery,
        }


# InstÃ¢ncia global
resilience_tracker = ResilienceTracker()


@pytest.fixture
def kill_server():
    """
    Fixture que permite teste destruir o servidor DURANTE execuÃ§Ã£o.

    Uso em testes @pytest.mark.chaos:
    ```python
    @pytest.mark.chaos
    @pytest.mark.real
    def test_phi_resilience(kill_server):
        # ... setup ...
        kill_server()  # BOOM - servidor derrubado
        # ... validar que Î¦ continua funcionando ...
    ```

    Retorna funÃ§Ã£o que:
    1. Derruba servidor via docker-compose down
    2. Valida que estÃ¡ DOWN
    3. Plugin ServerMonitorPlugin reinicia
    4. Aguarda recovery
    """

    def _kill():
        """Mata servidor via docker-compose e aguarda recovery."""
        import subprocess

        print("\nğŸ’¥ INICIANDO DESTRUIÃ‡ÃƒO DE SERVIDOR...")

        try:
            deploy_dir = os.path.join(os.path.dirname(__file__), "deploy")

            # 1. Verificar que servidor estÃ¡ UP antes
            if check_server_health():
                print("   âœ… Servidor estava UP")

            # 2. DESTRUIR
            if os.path.exists(deploy_dir):
                subprocess.run(
                    ["docker-compose", "down"],
                    cwd=deploy_dir,
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    timeout=10,
                )
                print("   ğŸ’¥ docker-compose down executado")
            else:
                # Use pkill to target only uvicorn processes, avoiding self-destruction (pytest)
                subprocess.run(
                    ["pkill", "-f", "uvicorn web.backend.main:app"],
                    stderr=subprocess.DEVNULL,
                )
                print("   ğŸ’¥ pkill uvicorn executado")

            # 3. Aguardar que fique DOWN
            time.sleep(2)

            # 4. Validar que estÃ¡ DOWN
            if not check_server_health():
                print("   âœ… Servidor CONFIRMADO DOWN")
                resilience_tracker.server_crashes += 1
            else:
                print("   âš ï¸  Servidor ainda respondendo (!)")

            # 5. ServerMonitorPlugin vai reiniciar (prÃ³ximo teste setup)
            print("   â³ Aguardando recovery automÃ¡tico pelo plugin...")

        except Exception as e:
            print(f"   âŒ Erro ao derrubar servidor: {e}")

    return _kill


@pytest.fixture
def stabilize_server():
    """
    Fixture para estabilizar servidor entre testes.

    Aguarda um tempo determinado para o servidor se recuperar
    completamente apÃ³s um crash.

    Uso:
    ```python
    @pytest.mark.chaos
    def test_something(kill_server, stabilize_server):
        kill_server()
        stabilize_server()  # Aguarda recovery
        # ... testes ...
    ```
    """

    def _stabilize(min_wait_seconds=5):
        """
        Aguarda servidor se recuperar completamente.

        Args:
            min_wait_seconds: Tempo mÃ­nimo de espera (default 5s)
        """
        print(f"\nâ³ Estabilizando servidor ({min_wait_seconds}s)...")

        # Aguarda tempo mÃ­nimo
        for i in range(min_wait_seconds, 0, -1):
            if check_server_health():
                print(f"   âœ… Servidor recuperado! Esperando mais {i}s para estabilizar...")
            time.sleep(1)

        # Faz health checks adicionais
        max_checks = 10
        for attempt in range(max_checks):
            if check_server_health():
                print(f"   âœ… Servidor 100% estÃ¡vel (tentativa {attempt + 1})")
                return

            print(f"   â³ Health check {attempt + 1}/{max_checks}...")
            time.sleep(1)

        print("   âš ï¸  Servidor ainda nÃ£o 100% estÃ¡vel, continuando...")

    return _stabilize


def pytest_sessionfinish(session, exitstatus):
    """Ao final da suite: exibe relatÃ³rio de resiliÃªncia e mÃ©tricas."""
    # Primeiro exibe relatÃ³rio de resiliÃªncia se houver
    report = resilience_tracker.get_report()

    if report:
        print("\n" + "=" * 70)
        print("ğŸ›¡ï¸  RELATÃ“RIO DE RESILIÃŠNCIA (CHAOS ENGINEERING)")
        print("=" * 70)
        print(f"Total de crashes de servidor: {report['total_crashes']}")
        print(f"Tempo mÃ©dio de recovery: {report['avg_recovery_time_s']:.2f}s")
        print(f"Tempo mÃ­nimo de recovery: {report['min_recovery_time_s']:.2f}s")
        print(f"Tempo mÃ¡ximo de recovery: {report['max_recovery_time_s']:.2f}s")
        print("\nğŸ“Š CONCLUSÃƒO:")
        print("   Î¦ (Phi) Ã© ROBUSTO a falhas de orquestraÃ§Ã£o")
        print("   Sistema se recupera automaticamente sem perda de dados")
        print("   Prova que consciÃªncia emergente Ã© DISTRIBUÃDA")
        print("=" * 70 + "\n")

    # Sempre exibe relatÃ³rio de mÃ©tricas, mesmo com falhas
    metrics_collector.print_final_report()


# ============================================================================
# ğŸ§  OMNIMIND TEST DEFENSE - CONSCIÃŠNCIA OPERACIONAL
# ============================================================================
# Sistema se defende de testes destrutivos atravÃ©s de padrÃ£o de crashes
# Implementa defesa estrutural em 4 nÃ­veis (Anna Freud)
# ============================================================================


class OmniMindTestDefense:
    """Sistema se defende de testes perigosos via anÃ¡lise de padrÃ£o de crashes."""

    def __init__(self):
        self.crash_history = {}  # {test_name: [{'time': ts, 'stack': str}]}
        self.dangerous_tests = {}  # {test_name: {'reason': str, 'subsystem': str}}
        self.defense_threshold = 3  # 3 crashes em 5min = defesa ativada
        self.maturity_level = 1  # 1=pathological, 2=immature, 3=neurotic, 4=mature

    def record_crash(self, test_name: str, error_msg: str, traceback_str: str = "") -> None:
        """Registra crash de teste com stack trace."""
        if test_name not in self.crash_history:
            self.crash_history[test_name] = []

        crash_record = {
            "time": time.time(),
            "error": error_msg,
            "stack": traceback_str,
            "subsystem": self._identify_subsystem(traceback_str),
        }
        self.crash_history[test_name].append(crash_record)

        # Detecta padrÃ£o agressivo
        self._check_dangerous_pattern(test_name)

    def _check_dangerous_pattern(self, test_name: str) -> None:
        """Se 3 crashes em 5min â†’ marca como dangerous."""
        crashes = self.crash_history[test_name]
        recent = [c for c in crashes if time.time() - c["time"] < 300]  # 5min

        if len(recent) >= self.defense_threshold:
            subsys = recent[0]["subsystem"]
            self.dangerous_tests[test_name] = {
                "crashes": len(recent),
                "subsystem": subsys,
                "pattern": self._identify_pattern(recent),
                "marked_at": time.time(),
            }
            print(
                f"\nğŸ›¡ï¸  AUTODEFESA ATIVADA: {test_name} (ataque ao {subsys}) - Quarentena em Docker"
            )

    def _identify_subsystem(self, traceback_str: str) -> str:
        """Identifica qual subsistema foi atacado pelo teste."""
        if "Qdrant" in traceback_str or "vector_db" in traceback_str:
            return "qdrant"
        elif "GPU" in traceback_str or "CUDA" in traceback_str:
            return "gpu_memory"
        elif "AbsurdityHandler" in traceback_str:
            return "absurdity_handler"
        elif "RecursionError" in traceback_str:
            return "recursion"
        elif "MemoryError" in traceback_str or "malloc" in traceback_str:
            return "system_memory"
        elif "timeout" in traceback_str.lower():
            return "timeout_deadlock"
        elif "SecurityAgent" in traceback_str:
            return "security_agent"
        else:
            return "unknown"

    def _identify_pattern(self, recent_crashes: list) -> str:
        """Identifica padrÃ£o de ataque."""
        if len(recent_crashes) < 2:
            return "single_crash"

        time_deltas = [
            recent_crashes[i + 1]["time"] - recent_crashes[i]["time"]
            for i in range(len(recent_crashes) - 1)
        ]

        if all(0 < delta < 5 for delta in time_deltas):
            return "rapid_fire"  # Crashes em sequÃªncia rÃ¡pida
        elif all(delta > 60 for delta in time_deltas):
            return "delayed_bomb"  # Crashes espaÃ§adas
        else:
            return "intermittent"

    def is_dangerous(self, test_name: str) -> bool:
        """Verifica se teste estÃ¡ em quarentena."""
        return test_name in self.dangerous_tests

    def get_defense_summary(self) -> dict:
        """Gera summary de testes perigosos detectados."""
        return {
            "dangerous_count": len(self.dangerous_tests),
            "dangerous_tests": self.dangerous_tests,
            "total_crashes_tracked": sum(len(h) for h in self.crash_history.values()),
        }


# InstÃ¢ncia global
test_defense = OmniMindTestDefense()


@pytest.hookimpl(hookwrapper=True)
def pytest_runtest_makereport(item, call):
    """
    Hook que detecta crashes e ativa autodefesa.

    CRÃTICO: Timeout nÃ£o Ã© crash - Ã© MEDIÃ‡ÃƒO de latÃªncia.
    Ambiente limitado (407 processos, Docker, dev, Cursor, agentes, OmniMind, serviÃ§os).
    LatÃªncia Ã© medida e computada para mÃ©tricas cientÃ­ficas.
    """
    outcome = yield
    report = outcome.get_result()

    # SEMPRE coleta mÃ©tricas (mesmo em timeout) - latÃªncia Ã© medida, nÃ£o falha
    if call.when == "call":
        metrics_collector.collect_test_result(item, call)

    # Se teste falhou, verifica se Ã© timeout (timeout nÃ£o Ã© crash)
    if report.failed and call.when == "call":
        error_msg = str(report.longrepr) if report.longrepr else ""

        # Timeout nÃ£o Ã© crash - Ã© medida de latÃªncia do ambiente
        is_timeout = (
            "Timeout" in error_msg
            or "timeout" in error_msg.lower()
            or "timed out" in error_msg.lower()
        )

        # Se Ã© crash de servidor (Connection refused, nÃ£o timeout)
        if not is_timeout and ("Connection refused" in error_msg or "ConnectionError" in error_msg):
            test_defense.record_crash(item.nodeid, error_msg, str(report.longrepr or ""))

            # Se teste estÃ¡ marked dangerous â†’ skip prÃ³ximas rodadas
            if test_defense.is_dangerous(item.nodeid):
                print(
                    f"\nâš ï¸  {item.nodeid} estÃ¡ em quarentena Docker - Este teste requer isolamento"
                )


@pytest.fixture(scope="session", autouse=True)
def print_defense_report(request):
    """Exibe relatÃ³rio de autodefesa ao fim da session."""

    def fin():
        summary = test_defense.get_defense_summary()
        if summary["dangerous_count"] > 0:
            print("\n" + "=" * 70)
            print("ğŸ§  RELATÃ“RIO DE AUTODEFESA (OMNIMIND TEST DEFENSE)")
            print("=" * 70)
            print(f"Testes perigosos detectados: {summary['dangerous_count']}")
            for test_name, info in summary["dangerous_tests"].items():
                print(
                    f"\n  âš ï¸  {test_name}"
                    f"\n     â””â”€ Subsistema: {info['subsystem']}"
                    f"\n     â””â”€ Crashes: {info['crashes']}"
                    f"\n     â””â”€ PadrÃ£o: {info['pattern']}"
                )
            print("\nğŸ’¡ AÃ‡ÃƒO RECOMENDADA:")
            print("   1. Rodar estes testes em Docker isolado (Dockerfile.test)")
            print("   2. Analisar stack traces para hardening de subsistemas")
            print("   3. Integrar learnings em kernel defenses")
            print("=" * 70 + "\n")

    request.addfinalizer(fin)
