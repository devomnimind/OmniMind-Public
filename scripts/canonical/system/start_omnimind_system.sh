#!/bin/bash

# Cores para output
GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${GREEN}üöÄ Iniciando Sistema OmniMind Completo...${NC}"

# üîß CR√çTICO: Ativar venv ANTES de qualquer import Python
# PROJECT_ROOT deve apontar para a raiz do projeto (1 n√≠vel acima de scripts/)
PROJECT_ROOT="$(cd "$(dirname "$0")/.." && pwd)"
if [ -f "$PROJECT_ROOT/.venv/bin/activate" ]; then
    source "$PROJECT_ROOT/.venv/bin/activate"
    echo "‚úÖ Venv ativado: $VIRTUAL_ENV"
else
    echo "‚ö†Ô∏è  Venv n√£o encontrado em $PROJECT_ROOT/.venv"
fi

# üîí SEGURAN√áA: Bloquear porta 4444 (comumente usada por malware)
# Documentado em: docs/SECURITY_PORT_4444_BLOCK.md
echo "üîí Aplicando bloqueio de seguran√ßa (porta 4444)..."
if command -v iptables &> /dev/null; then
    # Verificar se regras j√° existem
    if ! sudo iptables -C INPUT -p tcp --dport 4444 -j DROP 2>/dev/null; then
        sudo iptables -A INPUT -p tcp --dport 4444 -j DROP 2>/dev/null || true
        sudo iptables -A OUTPUT -p tcp --dport 4444 -j DROP 2>/dev/null || true
        sudo iptables -A INPUT -p udp --dport 4444 -j DROP 2>/dev/null || true
        sudo iptables -A OUTPUT -p udp --dport 4444 -j DROP 2>/dev/null || true
        echo "‚úÖ Porta 4444 bloqueada (seguran√ßa)"
    else
        echo "‚úÖ Porta 4444 j√° est√° bloqueada"
    fi
else
    echo "‚ö†Ô∏è  iptables n√£o dispon√≠vel - porta 4444 n√£o bloqueada"
fi

# üîß GPU Configuration - Kali Linux Native Paths
echo "üîß Configurando ambiente GPU (Kali Native)..."
# No Kali/Debian, CUDA √© integrado em /usr
export CUDA_HOME="/usr"
export CUDA_path="/usr"
# A libcuda.so.1 est√° em /usr/lib/x86_64-linux-gnu/
# Adicionar ao LD_LIBRARY_PATH explicitamente para garantir que PyTorch a encontre
export LD_LIBRARY_PATH="/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}"
export CUDA_VISIBLE_DEVICES="0"
export PYTORCH_CUDA_ALLOC_CONF="backend:cudaMallocAsync"
# export CUDA_LAUNCH_BLOCKING="1" # Descomente se precisar debugar inicializa√ß√£o s√≠ncrona

# Garantir permiss√£o de execu√ß√£o no run_cluster
chmod +x "$PROJECT_ROOT/scripts/canonical/system/run_cluster.sh" 2>/dev/null || true

# L√≥gica de Autentica√ß√£o Din√¢mica (Soberania Local) - UNIFICADA PARA CLUSTER
# Gera credenciais UMA VEZ e exporta para todos os subprocessos
DASH_USER=""
DASH_PASS=""
AUTH_FILE="$PROJECT_ROOT/config/dashboard_auth.json"

# 1. Tentar ler do arquivo gerado anteriormente ou preservar sess√£o
if [ -f "$AUTH_FILE" ]; then
    # Extra√ß√£o segura
    DASH_USER=$(python3 -c "import json; print(json.load(open('$AUTH_FILE')).get('user', ''))" 2>/dev/null)
    DASH_PASS=$(python3 -c "import json; print(json.load(open('$AUTH_FILE')).get('pass', ''))" 2>/dev/null)
fi

# 2. Fallback para .env
if [ -z "$DASH_USER" ] && [ -f "$PROJECT_ROOT/.env" ]; then
    DASH_USER=$(grep "^OMNIMIND_DASHBOARD_USER=" "$PROJECT_ROOT/.env" | cut -d '=' -f2 | tr -d '"' | tr -d "'")
    DASH_PASS=$(grep "^OMNIMIND_DASHBOARD_PASS=" "$PROJECT_ROOT/.env" | cut -d '=' -f2 | tr -d '"' | tr -d "'")
fi

# 3. Gerar novas se n√£o existirem (e salvar no arquivo para o backend usar a mesma)
if [ -z "$DASH_USER" ]; then
    # SOBERANIA LOCAL REAL: Gerar credenciais aleat√≥rias fortes a cada sess√£o
    # Isso garante seguran√ßa e obriga o uso correto do fluxo de autentica√ß√£o
    DASH_USER="admin"
    DASH_PASS=$(openssl rand -base64 12)

    # Salvar no JSON para persist√™ncia e leitura pelo backend
    echo "{\"user\": \"$DASH_USER\", \"pass\": \"$DASH_PASS\"}" > "$AUTH_FILE"
    echo "üîë Novas credenciais SOBERANAS geradas em $AUTH_FILE"
fi

# EXPORTAR PARA O AMBIENTE - ISSO GARANTE QUE TODOS OS BACKENDS USEM A MESMA SENHA
export OMNIMIND_DASHBOARD_USER="$DASH_USER"
export OMNIMIND_DASHBOARD_PASS="$DASH_PASS"
export OMNIMIND_DASHBOARD_AUTH_FILE="$AUTH_FILE"

echo -e "${GREEN}üîê Credenciais Unificadas do Cluster:${NC}"
echo "   User: $DASH_USER"
echo "   Pass: $DASH_PASS"

# 1. Limpeza
echo "üßπ Limpando processos antigos..."
pkill -f "python web/backend/main.py"
pkill -f "uvicorn web.backend.main:app"
pkill -f "python -m src.main"
pkill -f "vite"
pkill -f "bpftrace.*monitor_mcp_bpf" || true
sleep 2

# 2. Iniciar Backend Cluster (FASE 1: ESSENCIAIS)
echo -e "${GREEN}üîå Iniciando Backend Cluster (Fase 1: Essenciais)...${NC}"

# SEMPRE reiniciar o backend para garantir servi√ßos novos
# Mesmo que j√° esteja rodando, fazer restart para confirmar servi√ßos atualizados
if curl -s http://localhost:8000/health/ > /dev/null 2>&1; then
    echo -e "${YELLOW}‚ö†Ô∏è  Backend j√° est√° rodando na porta 8000${NC}"
    echo "   Reiniciando para garantir servi√ßos novos..."
    pkill -f "uvicorn web.backend.main:app" || true
    pkill -f "python web/backend/main.py" || true
    sleep 3
fi

"$PROJECT_ROOT/scripts/canonical/system/run_cluster.sh"

# Aguardar Backend subir
# ‚ö†Ô∏è CR√çTICO: Uvicorn + Orchestrator + SecurityAgent podem levar 30-60s
# Aumentado de 10s para 40s para garantir inicializa√ß√£o completa
echo "‚è≥ Aguardando Backend inicializar (40s - Orchestrator + SecurityAgent)..."
sleep 40

# Verificar Health Check (usando o endpoint /health/ que agora √© servido pelo router)
# Nota: O endpoint raiz /health foi removido do main.py, agora √© /health/ (com barra) ou /health (se o router permitir sem barra)
# O router tem prefix="/health" e @router.get("/"). Ent√£o √© /health/
if curl -s http://localhost:8000/health/ > /dev/null; then
    echo -e "${GREEN}‚úÖ Backend (Primary) Online!${NC}"
elif curl -s http://localhost:8000/api/v1/status > /dev/null; then
    echo -e "${GREEN}‚úÖ Backend (Primary) Online (via Status API)!${NC}"
else
    echo -e "${RED}‚ùå Falha ao conectar no Backend (Port 8000). Verifique logs/backend_8000.log${NC}"
    tail -n 10 "$PROJECT_ROOT/logs/backend_8000.log" 2>/dev/null || echo "   Log n√£o encontrado"
    exit 1
fi

# FASE 2: SECUND√ÅRIOS (ap√≥s 30s dos essenciais)
echo -e "${GREEN}‚è∞ Aguardando 30s antes de iniciar servi√ßos secund√°rios...${NC}"
echo "   (Garantindo que servi√ßos essenciais estejam totalmente inicializados)"
sleep 30

# 2.1. Iniciar MCP Servers (FASE 2: SECUND√ÅRIOS)
echo -e "${GREEN}üåê Iniciando MCP Servers...${NC}"
cd "$PROJECT_ROOT"

# Verificar se MCP Orchestrator j√° est√° rodando
if pgrep -f "run_mcp_orchestrator.py" > /dev/null || pgrep -f "mcp_orchestrator" > /dev/null; then
    echo -e "${YELLOW}‚ö†Ô∏è  MCP Orchestrator j√° est√° rodando${NC}"
    MCP_ORCHESTRATOR_PID=$(pgrep -f "run_mcp_orchestrator.py" | head -1)
    echo "   Usando PID existente: $MCP_ORCHESTRATOR_PID"
else
    # Garantir permiss√£o de execu√ß√£o
    chmod +x "$PROJECT_ROOT/scripts/canonical/system/start_mcp_servers.sh" 2>/dev/null || true
    chmod +x "$PROJECT_ROOT/scripts/canonical/system/run_mcp_orchestrator.py" 2>/dev/null || true

    # Iniciar MCP Orchestrator
    nohup python "$PROJECT_ROOT/scripts/canonical/system/run_mcp_orchestrator.py" > "$PROJECT_ROOT/logs/mcp_orchestrator.log" 2>&1 &
    MCP_ORCHESTRATOR_PID=$!
    echo $MCP_ORCHESTRATOR_PID > "$PROJECT_ROOT/logs/mcp_orchestrator.pid"
    echo "‚úì MCP Orchestrator iniciado (PID $MCP_ORCHESTRATOR_PID)"
    echo "   Log: tail -f logs/mcp_orchestrator.log"
    sleep 5
fi

# 3. Iniciar Ciclo Principal com Autopoiese (Phase 23)
echo -e "${GREEN}üîÑ Iniciando Ciclo Principal OmniMind (Fase 23: Autopoiese + Integra√ß√£o Real-time)...${NC}"
cd "$PROJECT_ROOT"
mkdir -p "$PROJECT_ROOT/logs" "$PROJECT_ROOT/data/autopoietic/synthesized_code" "$PROJECT_ROOT/data/monitor"

# Verificar se j√° est√° rodando
if [ -f "$PROJECT_ROOT/logs/main_cycle.pid" ]; then
    OLD_PID=$(cat "$PROJECT_ROOT/logs/main_cycle.pid" 2>/dev/null || echo "")
    if [ -n "$OLD_PID" ] && ps -p "$OLD_PID" > /dev/null 2>&1; then
        echo -e "${YELLOW}‚ö†Ô∏è  Ciclo Principal j√° est√° rodando (PID $OLD_PID)${NC}"
        MAIN_CYCLE_PID=$OLD_PID
    else
        # Iniciar ciclo principal em background (Rhizome + Consci√™ncia + Autopoiese)
        nohup python -m src.main > "$PROJECT_ROOT/logs/main_cycle.log" 2>&1 &
        MAIN_CYCLE_PID=$!
        echo $MAIN_CYCLE_PID > "$PROJECT_ROOT/logs/main_cycle.pid"
        echo "‚úì Ciclo Principal iniciado (PID $MAIN_CYCLE_PID)"
    fi
else
    # Iniciar ciclo principal em background (Rhizome + Consci√™ncia + Autopoiese)
    nohup python -m src.main > "$PROJECT_ROOT/logs/main_cycle.log" 2>&1 &
    MAIN_CYCLE_PID=$!
    echo $MAIN_CYCLE_PID > "$PROJECT_ROOT/logs/main_cycle.pid"
    echo "‚úì Ciclo Principal iniciado (PID $MAIN_CYCLE_PID)"
fi
echo "   Log: tail -f logs/main_cycle.log"
sleep 3

# 4. Iniciar Daemon
echo -e "${GREEN}ü§ñ Inicializando OmniMind Daemon...${NC}"
cd "$PROJECT_ROOT"

# Fazer requisi√ß√£o com as credenciais descobertas
if [ -n "$OMNIMIND_DASHBOARD_PASS" ]; then
    curl -X POST http://localhost:8000/daemon/start \
      -u "${OMNIMIND_DASHBOARD_USER}:${OMNIMIND_DASHBOARD_PASS}" \
      > "$PROJECT_ROOT/logs/daemon_start.log" 2>&1 &
    DAEMON_START_PID=$!
    echo "‚úì Daemon start request enviado (PID $DAEMON_START_PID)"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Senha n√£o encontrada, pulando inicializa√ß√£o do daemon via API${NC}"
fi
sleep 2

# 5. Iniciar Frontend
echo -e "${GREEN}üé® Iniciando Frontend...${NC}"
cd "$PROJECT_ROOT"

# Verificar se diret√≥rio frontend existe
if [ ! -d "web/frontend" ]; then
    echo -e "${RED}‚ùå Diret√≥rio web/frontend n√£o encontrado!${NC}"
    echo "   Verificando estrutura do projeto..."
    ls -la web/ 2>&1 | head -10
    FRONTEND_PID=""
else
    cd web/frontend

    # Verificar se node_modules existe, se n√£o, instalar
    if [ ! -d "node_modules" ]; then
        echo "üì¶ Instalando depend√™ncias do Frontend..."
        npm install
    fi

    # Verificar se j√° est√° rodando
    if [ -f "$PROJECT_ROOT/logs/frontend.pid" ]; then
        OLD_PID=$(cat "$PROJECT_ROOT/logs/frontend.pid" 2>/dev/null || echo "")
        if [ -n "$OLD_PID" ] && ps -p "$OLD_PID" > /dev/null 2>&1; then
            echo -e "${YELLOW}‚ö†Ô∏è  Frontend j√° est√° rodando (PID $OLD_PID)${NC}"
            FRONTEND_PID=$OLD_PID
        else
            nohup npm run dev > "$PROJECT_ROOT/logs/frontend.log" 2>&1 &
            FRONTEND_PID=$!
            echo $FRONTEND_PID > "$PROJECT_ROOT/logs/frontend.pid"
            echo "‚úì Frontend iniciado (PID $FRONTEND_PID)"
        fi
    else
        nohup npm run dev > "$PROJECT_ROOT/logs/frontend.log" 2>&1 &
        FRONTEND_PID=$!
        echo $FRONTEND_PID > "$PROJECT_ROOT/logs/frontend.pid"
        echo "‚úì Frontend iniciado (PID $FRONTEND_PID)"
    fi
fi

# Voltar para raiz do projeto
cd "$PROJECT_ROOT"

# 6. Verifica√ß√£o Final
echo -e "${GREEN}üîç Verificando status do sistema...${NC}"
sleep 5

if [ -n "$FRONTEND_PID" ] && ps -p $FRONTEND_PID > /dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ Frontend rodando (PID $FRONTEND_PID)${NC}"
    echo "   Acesse: http://localhost:3000"
else
    echo -e "${RED}‚ùå Frontend falhou ao iniciar. Verifique logs/frontend.log${NC}"
    if [ -f "$PROJECT_ROOT/logs/frontend.log" ]; then
        tail -n 20 "$PROJECT_ROOT/logs/frontend.log"
    else
        echo "   Arquivo de log n√£o encontrado"
    fi
fi

# FASE 3: MONITORAMENTO (ap√≥s 15s dos servi√ßos principais)
# Aguardar estabiliza√ß√£o completa antes de iniciar servi√ßos de monitoramento
echo -e "${GREEN}‚è∞ Aguardando 15s antes de iniciar servi√ßos de monitoramento...${NC}"
echo "   (Garantindo que todos os servi√ßos principais estejam totalmente est√°veis)"
sleep 15

# 7. Iniciar Observer Service (FASE 3: MONITORAMENTO - ap√≥s servi√ßos principais)
echo -e "${GREEN}üìä Iniciando Observer Service (M√©tricas de Longo Prazo)...${NC}"
cd "$PROJECT_ROOT"

# Verificar se j√° est√° rodando
if [ -f "$PROJECT_ROOT/logs/observer_service.pid" ]; then
    OLD_PID=$(cat "$PROJECT_ROOT/logs/observer_service.pid" 2>/dev/null || echo "")
    if [ -n "$OLD_PID" ] && ps -p "$OLD_PID" > /dev/null 2>&1; then
        echo -e "${YELLOW}‚ö†Ô∏è  Observer Service j√° est√° rodando (PID $OLD_PID)${NC}"
        OBSERVER_PID=$OLD_PID
    else
        # Criar diret√≥rio de logs se n√£o existir
        mkdir -p "$PROJECT_ROOT/data/long_term_logs" "$PROJECT_ROOT/logs"

        # Garantir permiss√£o de execu√ß√£o no script
        chmod +x "$PROJECT_ROOT/scripts/canonical/system/run_observer_service.py" 2>/dev/null || true

        # Iniciar Observer Service em background usando script wrapper
        nohup python "$PROJECT_ROOT/scripts/canonical/system/run_observer_service.py" > "$PROJECT_ROOT/logs/observer_service.log" 2>&1 &
        OBSERVER_PID=$!
        echo $OBSERVER_PID > "$PROJECT_ROOT/logs/observer_service.pid"
        echo "‚úì Observer Service iniciado (PID $OBSERVER_PID)"
        echo "   Log: tail -f logs/observer_service.log"
        echo "   M√©tricas: data/long_term_logs/omnimind_metrics.jsonl"
        sleep 3  # Aguardar inicializa√ß√£o
    fi
else
    # Criar diret√≥rio de logs se n√£o existir
    mkdir -p "$PROJECT_ROOT/data/long_term_logs" "$PROJECT_ROOT/logs"

    # Garantir permiss√£o de execu√ß√£o no script
    chmod +x "$PROJECT_ROOT/scripts/canonical/system/run_observer_service.py" 2>/dev/null || true

    # Iniciar Observer Service em background usando script wrapper
    nohup python "$PROJECT_ROOT/scripts/canonical/system/run_observer_service.py" > "$PROJECT_ROOT/logs/observer_service.log" 2>&1 &
    OBSERVER_PID=$!
    echo $OBSERVER_PID > "$PROJECT_ROOT/logs/observer_service.pid"
    echo "‚úì Observer Service iniciado (PID $OBSERVER_PID)"
    echo "   Log: tail -f logs/observer_service.log"
    echo "   M√©tricas: data/long_term_logs/omnimind_metrics.jsonl"
    sleep 3  # Aguardar inicializa√ß√£o
fi

# 8. Iniciar eBPF Monitor Cont√≠nuo (FASE 3: MONITORAMENTO AVAN√áADO)
echo -e "${GREEN}üìä Iniciando eBPF Monitor Cont√≠nuo...${NC}"

# Voltar para a raiz do projeto para encontrar scripts/canonical/system/secure_run.py
cd "$PROJECT_ROOT"

if command -v bpftrace &> /dev/null; then
    EBPF_LOG="$PROJECT_ROOT/logs/ebpf_monitor.log"
    mkdir -p "$PROJECT_ROOT/logs"

    # Garantir permiss√µes no arquivo de log se ele existir
    if [ -f "$EBPF_LOG" ]; then
        # Tentar mudar dono para usu√°rio atual se poss√≠vel, ou remover se falhar
        if ! touch "$EBPF_LOG" 2>/dev/null; then
            echo "‚ö†Ô∏è  Sem permiss√£o de escrita em $EBPF_LOG. Tentando remover com sudo..."
            sudo rm -f "$EBPF_LOG"
        fi
    fi

    # Parar eBPF anterior
    python3 "$PROJECT_ROOT/scripts/canonical/system/secure_run.py" pkill -f "bpftrace.*monitor_mcp_bpf" || true
    sleep 1
    # Iniciar em background
    # Nota: secure_run.py j√° lida com sudo -n
    python3 "$PROJECT_ROOT/scripts/canonical/system/secure_run.py" bpftrace "$PROJECT_ROOT/scripts/canonical/system/monitor_mcp_bpf.bt" > "${EBPF_LOG}" 2>&1 &
    sleep 2
    echo -e "${GREEN}‚úÖ eBPF Monitor ativo${NC}"
    echo "   Log: tail -f ${EBPF_LOG}"
else
    echo -e "${RED}‚ö†Ô∏è  bpftrace n√£o encontrado. Instale com: sudo apt install bpftrace${NC}"
fi

echo -e "${GREEN}‚ú® Sistema OmniMind Reiniciado!${NC}"
echo ""
echo -e "${GREEN}üìã SERVI√áOS ATIVOS:${NC}"
echo "   Backend Cluster: Ports 8000, 8080, 3001"
if [ -n "${MCP_ORCHESTRATOR_PID:-}" ]; then
    echo "   MCP Orchestrator: PID ${MCP_ORCHESTRATOR_PID}"
fi
echo "   Ciclo Principal (Autopoiese Phase 23): PID $MAIN_CYCLE_PID"
if [ -n "${OBSERVER_PID:-}" ]; then
    echo "   Observer Service: PID ${OBSERVER_PID}"
fi
echo "   Frontend: http://localhost:3000"
echo ""
echo -e "${GREEN}üîê CREDENCIAIS DA SESS√ÉO ATUAL (CLUSTER UNIFICADO):${NC}"
echo -e "   User: ${GREEN}${OMNIMIND_DASHBOARD_USER}${NC}"
echo -e "   Pass: ${GREEN}${OMNIMIND_DASHBOARD_PASS}${NC}"
echo "   (Use estas credenciais para logar no Dashboard)"
echo ""
echo -e "${GREEN}üìä MONITORAMENTO:${NC}"
echo "   eBPF Monitor: logs/ebpf_monitor.log"
if [ -n "${OBSERVER_PID:-}" ]; then
    echo "   Observer Service: logs/observer_service.log"
    echo "   M√©tricas Longo Prazo: data/long_term_logs/omnimind_metrics.jsonl"
    echo "   Heartbeat: data/long_term_logs/heartbeat.status"
fi
echo "   Logs Directory: logs/"
echo ""
echo "üìä Autopoiese Phase 23 (Active):"
echo "   - Componentes sintetizados: data/autopoietic/synthesized_code/"
echo "   - Hist√≥rico de ciclos: data/autopoietic/cycle_history.jsonl"
echo "   - Log do ciclo: logs/main_cycle.log"
