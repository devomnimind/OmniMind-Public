#!/usr/bin/env python3
"""
Script para Coletar M√©tricas Baseline - Otimiza√ß√£o de Mem√≥ria

Coleta m√©tricas atuais do sistema ANTES das otimiza√ß√µes:
- Uso de mem√≥ria por agente
- Lat√™ncia de execu√ß√£o
- Model load times
- Cache statistics (se existir)
- Qdrant usage

Autor: Fabr√≠cio da Silva + assist√™ncia de IA
"""

import asyncio
import json
import logging
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict

import psutil

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Imports ap√≥s path setup (E402 aceit√°vel para scripts)
from src.agents.code_agent import CodeAgent  # noqa: E402
from src.integrations.llm_router import get_llm_router  # noqa: E402
from src.integrations.qdrant_adapter import QdrantAdapter, QdrantConfig  # noqa: E402

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BaselineMetricsCollector:
    """Coleta m√©tricas baseline do sistema atual."""

    def __init__(self, output_dir: str = "data/metrics/baseline"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.metrics: Dict[str, Any] = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "baseline": True,
            "memory": {},
            "latency": {},
            "models": {},
            "qdrant": {},
            "cache": {},
        }

    def collect_all(self) -> Dict[str, Any]:
        """Coleta todas as m√©tricas baseline."""
        logger.info("=" * 80)
        logger.info("COLETANDO M√âTRICAS BASELINE - OTIMIZA√á√ÉO DE MEM√ìRIA")
        logger.info("=" * 80)

        # 1. Mem√≥ria do sistema
        logger.info("1. Coletando m√©tricas de mem√≥ria...")
        self.metrics["memory"] = self._collect_memory_metrics()

        # 2. Lat√™ncia de agentes
        logger.info("2. Coletando m√©tricas de lat√™ncia...")
        self.metrics["latency"] = asyncio.run(self._collect_latency_metrics())

        # 3. Model loading
        logger.info("3. Coletando m√©tricas de modelos...")
        self.metrics["models"] = self._collect_model_metrics()

        # 4. Qdrant usage
        logger.info("4. Coletando m√©tricas do Qdrant...")
        self.metrics["qdrant"] = self._collect_qdrant_metrics()

        # 5. Cache statistics (se existir)
        logger.info("5. Coletando estat√≠sticas de cache...")
        self.metrics["cache"] = self._collect_cache_metrics()

        # Salvar m√©tricas (serializar Enums e objetos n√£o-JSON)
        output_file = self.output_dir / f"baseline_{int(time.time())}.json"
        serializable_metrics = self._make_json_serializable(self.metrics)
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(serializable_metrics, f, indent=2, ensure_ascii=False)

        logger.info(f"‚úÖ M√©tricas baseline salvas em: {output_file}")
        logger.info("=" * 80)

        return self.metrics

    def _collect_memory_metrics(self) -> Dict[str, Any]:
        """Coleta m√©tricas de mem√≥ria do sistema."""
        process = psutil.Process()
        memory_info = process.memory_info()

        # Mem√≥ria do sistema
        system_memory = psutil.virtual_memory()

        return {
            "process_memory_mb": memory_info.rss / (1024 * 1024),
            "process_memory_gb": memory_info.rss / (1024 * 1024 * 1024),
            "system_total_gb": system_memory.total / (1024 * 1024 * 1024),
            "system_available_gb": system_memory.available / (1024 * 1024 * 1024),
            "system_used_percent": system_memory.percent,
            "num_threads": process.num_threads(),
            "num_fds": process.num_fds() if hasattr(process, "num_fds") else None,
        }

    async def _collect_latency_metrics(self) -> Dict[str, Any]:
        """Coleta m√©tricas de lat√™ncia de agentes (vers√£o r√°pida - sem execu√ß√£o completa)."""
        latencies = {
            "note": "Lat√™ncia completa ser√° coletada em testes separados",
            "agent_initialization": {},
        }

        # Apenas medir tempo de inicializa√ß√£o (n√£o execu√ß√£o completa)
        config_path = "config/agent_config.yaml"

        # Teste inicializa√ß√£o CodeAgent
        try:
            logger.info("  Testando inicializa√ß√£o CodeAgent...")
            start_time = time.time()
            CodeAgent(config_path)  # Apenas inicializar, n√£o usar
            init_time = time.time() - start_time

            latencies["agent_initialization"]["code_agent"] = {
                "init_time_seconds": init_time,
                "init_time_ms": init_time * 1000,
            }
            logger.info("    ‚úÖ CodeAgent init: %.2fs", init_time)

        except Exception as e:
            logger.error("    ‚ùå CodeAgent init falhou: %s", e)
            latencies["agent_initialization"]["code_agent"] = {"error": str(e)}

        return latencies

    def _collect_model_metrics(self) -> Dict[str, Any]:
        """Coleta m√©tricas de modelos."""
        metrics = {
            "llm_router": {},
            "ollama": {},
        }

        # LLM Router
        try:
            router = get_llm_router()
            metrics["llm_router"] = {
                "available": True,
                "providers": list(router.providers.keys()),
                "metrics": router.metrics.copy() if hasattr(router, "metrics") else {},
            }
        except Exception as e:
            logger.warning(f"LLM Router n√£o dispon√≠vel: {e}")
            metrics["llm_router"] = {"available": False, "error": str(e)}

        # Ollama (verificar se est√° rodando)
        try:
            import requests

            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                models = response.json().get("models", [])
                metrics["ollama"] = {
                    "available": True,
                    "models": [m.get("name", "unknown") for m in models],
                    "model_count": len(models),
                }
            else:
                metrics["ollama"] = {"available": False, "status_code": response.status_code}
        except Exception as e:
            logger.warning(f"Ollama n√£o dispon√≠vel: {e}")
            metrics["ollama"] = {"available": False, "error": str(e)}

        return metrics

    def _collect_qdrant_metrics(self) -> Dict[str, Any]:
        """Coleta m√©tricas do Qdrant."""
        metrics = {
            "available": False,
            "collections": [],
            "collection_sizes": {},
        }

        try:
            config = QdrantConfig.from_env()
            if not config:
                logger.warning("Qdrant config n√£o encontrada")
                return metrics

            adapter = QdrantAdapter(config)
            collections = adapter.list_collections()

            metrics["available"] = True
            metrics["collections"] = collections

            # Tentar obter tamanhos das cole√ß√µes
            try:
                from qdrant_client import QdrantClient

                client = QdrantClient(url=config.url, api_key=config.api_key)
                for collection in collections:
                    try:
                        info = client.get_collection(collection)
                        metrics["collection_sizes"][collection] = {
                            "points_count": (
                                info.points_count if hasattr(info, "points_count") else None
                            ),
                            "vectors_count": (
                                info.vectors_count if hasattr(info, "vectors_count") else None
                            ),
                        }
                    except Exception as e:
                        logger.warning(f"Erro ao obter info da cole√ß√£o {collection}: {e}")

            except Exception as e:
                logger.warning(f"Erro ao conectar ao Qdrant para m√©tricas: {e}")

        except Exception as e:
            logger.warning(f"Qdrant n√£o dispon√≠vel: {e}")
            metrics["error"] = str(e)

        return metrics

    def _collect_cache_metrics(self) -> Dict[str, Any]:
        """Coleta estat√≠sticas de cache existente."""
        metrics = {
            "neural_response_cache": {},
            "mcp_cache": {},
        }

        # Neural Response Cache
        try:
            from src.neurosymbolic.response_cache import get_response_cache

            cache = get_response_cache()
            stats = cache.get_stats()
            metrics["neural_response_cache"] = {
                "available": True,
                "stats": stats,
            }
        except Exception as e:
            logger.debug(f"Neural Response Cache n√£o dispon√≠vel: {e}")
            metrics["neural_response_cache"] = {"available": False}

        # MCP Cache (se existir)
        # TODO: Adicionar quando MCP cache estiver dispon√≠vel

        return metrics

    def _make_json_serializable(self, obj: Any) -> Any:
        """Converte objetos n√£o-JSON serializ√°veis para tipos b√°sicos."""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif hasattr(obj, "value"):  # Enum
            return obj.value
        elif hasattr(obj, "__dict__"):
            return self._make_json_serializable(obj.__dict__)
        else:
            return obj

    def print_summary(self) -> None:
        """Imprime resumo das m√©tricas coletadas."""
        print("\n" + "=" * 80)
        print("RESUMO DAS M√âTRICAS BASELINE")
        print("=" * 80)

        # Mem√≥ria
        memory = self.metrics.get("memory", {})
        print("\nüíæ MEM√ìRIA:")
        mem_gb = memory.get("process_memory_gb", 0)
        mem_percent = memory.get("system_used_percent", 0)
        print(f"  Processo: {mem_gb:.2f} GB")
        print(f"  Sistema: {mem_percent:.1f}% usado")

        # Lat√™ncia
        latency = self.metrics.get("latency", {})
        print("\n‚è±Ô∏è  LAT√äNCIA:")
        if "code_agent" in latency and "latency_seconds" in latency["code_agent"]:
            code_lat = latency["code_agent"]["latency_seconds"]
            print(f"  CodeAgent: {code_lat:.2f}s")
        if "orchestrator_agent" in latency and "latency_seconds" in latency["orchestrator_agent"]:
            orch_lat = latency["orchestrator_agent"]["latency_seconds"]
            print(f"  OrchestratorAgent: {orch_lat:.2f}s")

        # Qdrant
        qdrant = self.metrics.get("qdrant", {})
        print("\nüóÑÔ∏è  QDRANT:")
        qdrant_avail = qdrant.get("available", False)
        qdrant_cols = len(qdrant.get("collections", []))
        print(f"  Dispon√≠vel: {qdrant_avail}")
        print(f"  Cole√ß√µes: {qdrant_cols}")

        # Cache
        cache = self.metrics.get("cache", {})
        print("\nüíø CACHE:")
        if cache.get("neural_response_cache", {}).get("available"):
            stats = cache["neural_response_cache"].get("stats", {})
            hit_rate = stats.get("hit_rate", "N/A")
            print(f"  Neural Response Cache: Hit rate {hit_rate}")

        print("\n" + "=" * 80)


def main():
    """Fun√ß√£o principal."""
    collector = BaselineMetricsCollector()
    metrics = collector.collect_all()
    collector.print_summary()

    return metrics


if __name__ == "__main__":
    main()
