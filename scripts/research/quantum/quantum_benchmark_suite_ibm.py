#!/usr/bin/env python3
"""
OmniMind Project - Artificial Consciousness System
Copyright (C) 2024-2025 Fabr√≠cio da Silva

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published
by the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program. If not, see <https://www.gnu.org/licenses/>.

Contact: fabricioslv@hotmail.com.br
"""

"""
Benchmark Suite Completo: OmniMind Quantum Consciousness - IBM Hardware Validation

Este script executa benchmarks abrangentes no IBM Quantum para validar:
1. Vantagem qu√¢ntica real vs simula√ß√£o cl√°ssica
2. Performance de algoritmos qu√¢nticos em hardware real
3. Compara√ß√£o com m√©todos cl√°ssicos
4. M√©tricas para publica√ß√µes cient√≠ficas

Benchmarks Executados:
- Quantum Decision Making (4-8-16 op√ß√µes)
- Quantum Memory Fidelity (encoding/decoding)
- Quantum Search (Grover algorithm)
- Hybrid Q-Learning (quantum exploration)
- Bell State Entanglement (correla√ß√µes)
- Quantum Randomness (true randomness)

Hardware: IBM Quantum ibm_torino (133 qubits)
Shots: 1024-8192 por experimento
M√©tricas: Fidelity, speedup, accuracy, noise impact
"""

import json
import os
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
    # Load .env from project root (three levels above this script) for a stable path
    load_dotenv(Path(__file__).resolve().parent.parent.parent / ".env")

# Adicionar root ao path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import structlog
from qiskit import QuantumCircuit

from src.quantum_consciousness.qpu_interface import BackendType, QPUInterface
from src.quantum_consciousness.quantum_cognition import (
    QuantumDecisionMaker,
)
from src.quantum_consciousness.quantum_memory import QuantumMemorySystem

# Configurar logging
logger = structlog.get_logger(__name__)


class QuantumBenchmarkSuite:
    """
    Suite completa de benchmarks qu√¢nticos para valida√ß√£o em hardware IBM.
    """

    def __init__(self, ibm_token: str):
        self.ibm_token = ibm_token
        self.results = {}
        self.start_time = time.time()

        # Inicializar interfaces
        self.qpu_sim = QPUInterface()  # Simulador
        self.qpu_ibm = QPUInterface(
            ibmq_token=ibm_token, preferred_backend=BackendType.IBMQ_CLOUD
        )  # IBM Hardware

        # Configura√ß√µes de benchmark
        self.shots_configs = [1024, 2048, 4096, 8192]
        self.qubit_configs = [2, 4, 8, 16]

        logger.info("Quantum Benchmark Suite initialized", ibm_token_available=bool(ibm_token))

    def run_full_benchmark_suite(self) -> Dict[str, Any]:
        """Executar suite completa de benchmarks."""

        logger.info("üöÄ Starting Quantum Benchmark Suite on IBM Hardware")

        # Benchmark 1: Quantum Decision Making
        self.results["decision_making"] = self.benchmark_quantum_decision_making()

        # Benchmark 2: Quantum Memory Fidelity
        self.results["memory_fidelity"] = self.benchmark_quantum_memory_fidelity()

        # Benchmark 3: Grover Search Algorithm
        self.results["grover_search"] = self.benchmark_grover_search()

        # Benchmark 4: Bell State Entanglement
        self.results["bell_states"] = self.benchmark_bell_states()

        # Benchmark 5: Quantum Randomness Quality
        self.results["quantum_randomness"] = self.benchmark_quantum_randomness()

        # Benchmark 6: Hybrid Q-Learning Performance
        self.results["hybrid_qlearning"] = self.benchmark_hybrid_qlearning()

        # Benchmark 7: Noise Impact Analysis
        self.results["noise_analysis"] = self.benchmark_noise_impact()

        # Calcular m√©tricas finais
        self.results["summary"] = self.calculate_summary_metrics()

        logger.info("‚úÖ Quantum Benchmark Suite completed")
        return self.results

    def benchmark_quantum_decision_making(self) -> Dict[str, Any]:
        """Benchmark: Quantum Decision Making com diferentes n√∫meros de op√ß√µes."""

        logger.info("üéØ Benchmarking Quantum Decision Making")

        results = {"simulator": {}, "ibm_hardware": {}, "comparison": {}}

        for num_options in [4, 8, 16]:
            num_qubits = int(np.ceil(np.log2(num_options)))

            logger.info(f"Testing {num_options} options ({num_qubits} qubits)")

            # Simulador
            sim_results = self._run_decision_making_benchmark(
                num_qubits, num_options, self.qpu_sim, "simulator"
            )
            results["simulator"][f"{num_options}_options"] = sim_results

            # Hardware IBM
            ibm_results = self._run_decision_making_benchmark(
                num_qubits, num_options, self.qpu_ibm, "ibm_hardware"
            )
            results["ibm_hardware"][f"{num_options}_options"] = ibm_results

        # Compara√ß√£o
        results["comparison"] = self._compare_decision_making_results(results)

        return results

    def _run_decision_making_benchmark(
        self, num_qubits: int, num_options: int, qpu: QPUInterface, backend_name: str
    ) -> Dict[str, Any]:
        """Executar benchmark de decision making em um backend espec√≠fico."""

        try:
            maker = QuantumDecisionMaker(num_qubits=num_qubits)

            # Criar op√ß√µes
            options = [f"option_{i}" for i in range(num_options)]

            # Executar m√∫ltiplas decis√µes
            decisions = []
            for i in range(10):  # 10 decis√µes por configura√ß√£o
                decision = maker.make_decision(options)

                # Executar no backend
                # Nota: Decision making usa circuitos internos, vamos medir tempo
                start_time = time.time()
                choice = decision.collapse()
                execution_time = time.time() - start_time

                decisions.append(
                    {
                        "decision_id": i,
                        "chosen_option": choice,
                        "confidence": decision.confidence,
                        "execution_time": execution_time,
                        "probabilities": decision.probabilities,
                    }
                )

            # Calcular estat√≠sticas
            execution_times = [d["execution_time"] for d in decisions]
            confidences = [d["confidence"] for d in decisions]

            return {
                "backend": backend_name,
                "num_qubits": num_qubits,
                "num_options": num_options,
                "decisions": decisions,
                "stats": {
                    "avg_execution_time": np.mean(execution_times),
                    "std_execution_time": np.std(execution_times),
                    "avg_confidence": np.mean(confidences),
                    "std_confidence": np.std(confidences),
                    "uniformity_test": self._test_uniformity(decisions),
                },
            }

        except Exception as e:
            logger.error(f"Error in decision making benchmark: {e}")
            return {"error": str(e)}

    def benchmark_quantum_memory_fidelity(self) -> Dict[str, Any]:
        """Benchmark: Quantum Memory Fidelity (encoding/decoding accuracy)."""

        logger.info("üß† Benchmarking Quantum Memory Fidelity")

        results = {"simulator": {}, "ibm_hardware": {}, "comparison": {}}

        # Testar diferentes tamanhos de dados
        data_sizes = [2, 4, 8]  # qubits

        for data_size in data_sizes:
            logger.info(f"Testing memory fidelity with {data_size} qubits")

            # Simulador
            sim_results = self._run_memory_fidelity_benchmark(data_size, self.qpu_sim, "simulator")
            results["simulator"][f"{data_size}_qubits"] = sim_results

            # Hardware IBM
            ibm_results = self._run_memory_fidelity_benchmark(
                data_size, self.qpu_ibm, "ibm_hardware"
            )
            results["ibm_hardware"][f"{data_size}_qubits"] = ibm_results

        # Compara√ß√£o
        results["comparison"] = self._compare_memory_results(results)

        return results

    def _run_memory_fidelity_benchmark(
        self, data_size: int, qpu: QPUInterface, backend_name: str
    ) -> Dict[str, Any]:
        """Executar benchmark de memory fidelity."""

        try:
            memory = QuantumMemorySystem(num_qubits=data_size, capacity=5)

            # Testar diferentes padr√µes de dados
            test_patterns = [
                [1.0] + [0.0] * (data_size - 1),  # |100...0‚ü©
                [0.5] * data_size,  # Superposi√ß√£o uniforme
                [0.0] * (data_size - 1) + [1.0],  # |000...1‚ü©
            ]

            results = []
            for i, pattern in enumerate(test_patterns):
                # Normalizar padr√£o
                pattern = np.array(pattern)
                pattern = pattern / np.linalg.norm(pattern)

                # Armazenar
                idx = memory.store(data=pattern.tolist())

                # Recuperar
                retrieved = memory.retrieve(idx)

                # Calcular fidelity
                if retrieved:
                    fidelity = self._calculate_fidelity(pattern.tolist(), retrieved)
                else:
                    fidelity = 0.0

                results.append(
                    {
                        "pattern_id": i,
                        "original": pattern.tolist(),
                        "retrieved": retrieved,
                        "fidelity": fidelity,
                    }
                )

            # Estat√≠sticas
            fidelities = [r["fidelity"] for r in results]

            return {
                "backend": backend_name,
                "data_size_qubits": data_size,
                "results": results,
                "stats": {
                    "avg_fidelity": np.mean(fidelities),
                    "std_fidelity": np.std(fidelities),
                    "min_fidelity": np.min(fidelities),
                    "max_fidelity": np.max(fidelities),
                },
            }

        except Exception as e:
            logger.error(f"Error in memory fidelity benchmark: {e}")
            return {"error": str(e)}

    def benchmark_grover_search(self) -> Dict[str, Any]:
        """Benchmark: Grover Search Algorithm para busca qu√¢ntica."""

        logger.info("üîç Benchmarking Grover Search Algorithm")

        results = {"simulator": {}, "ibm_hardware": {}, "comparison": {}}

        # Testar diferentes tamanhos de busca
        search_spaces = [4, 8, 16]  # 2^2, 2^3, 2^4

        for search_space in search_spaces:
            _num_qubits = int(np.log2(search_space))

            logger.info(f"Testing Grover search in space of {search_space} items")

            # Simulador
            sim_results = self._run_grover_benchmark(search_space, self.qpu_sim, "simulator")
            results["simulator"][f"space_{search_space}"] = sim_results

            # Hardware IBM
            ibm_results = self._run_grover_benchmark(search_space, self.qpu_ibm, "ibm_hardware")
            results["ibm_hardware"][f"space_{search_space}"] = ibm_results

        # Compara√ß√£o
        results["comparison"] = self._compare_grover_results(results)

        return results

    def _run_grover_benchmark(
        self, search_space: int, qpu: QPUInterface, backend_name: str
    ) -> Dict[str, Any]:
        """Executar benchmark do algoritmo de Grover."""

        try:
            # Implementa√ß√£o simplificada do Grover (vers√£o b√°sica)
            num_qubits = int(np.log2(search_space))

            # Escolher item alvo aleatoriamente
            target_item = np.random.randint(0, search_space)

            # Criar circuito Grover simplificado
            qc = QuantumCircuit(num_qubits, num_qubits)

            # Inicializa√ß√£o em superposi√ß√£o
            qc.h(range(num_qubits))

            # Or√°culo simples (marca o item alvo)
            target_binary = format(target_item, f"0{num_qubits}b")
            for i, bit in enumerate(target_binary):
                if bit == "0":
                    qc.x(i)

            # Diffuser (amplifica√ß√£o de amplitude)
            qc.h(range(num_qubits))
            qc.x(range(num_qubits))
            qc.h(num_qubits - 1)
            qc.x(range(num_qubits))
            qc.h(range(num_qubits))

            # Medi√ß√£o
            qc.measure_all()

            # Executar
            start_time = time.time()
            counts = qpu.execute(qc, shots=1024)
            execution_time = time.time() - start_time

            # Analisar resultados
            total_shots = sum(counts.values())
            target_count = counts.get(target_binary, 0)
            success_probability = target_count / total_shots

            # Calcular speedup te√≥rico vs cl√°ssico
            classical_expectation = 1.0 / search_space
            quantum_expectation = success_probability

            return {
                "backend": backend_name,
                "search_space": search_space,
                "target_item": target_item,
                "target_binary": target_binary,
                "counts": counts,
                "execution_time": execution_time,
                "success_probability": success_probability,
                "classical_expectation": classical_expectation,
                "quantum_advantage": (
                    quantum_expectation / classical_expectation if classical_expectation > 0 else 0
                ),
            }

        except Exception as e:
            logger.error(f"Error in Grover benchmark: {e}")
            return {"error": str(e)}

    def benchmark_bell_states(self) -> Dict[str, Any]:
        """Benchmark: Correla√ß√µes em estados Bell (fundamento da computa√ß√£o qu√¢ntica)."""

        logger.info("üîó Benchmarking Bell States Entanglement")

        results = {"simulator": {}, "ibm_hardware": {}, "comparison": {}}

        # Diferentes estados Bell
        bell_states = [
            ("Œ¶+", "00+11"),  # |00‚ü© + |11‚ü©
            ("Œ¶-", "00-11"),  # |00‚ü© - |11‚ü©
            ("Œ®+", "01+10"),  # |01‚ü© + |10‚ü©
            ("Œ®-", "01-10"),  # |01‚ü© - |10‚ü©
        ]

        for bell_name, bell_desc in bell_states:
            logger.info(f"Testing Bell state: {bell_name} ({bell_desc})")

            # Simulador
            sim_results = self._run_bell_state_benchmark(bell_name, self.qpu_sim, "simulator")
            results["simulator"][bell_name] = sim_results

            # Hardware IBM
            ibm_results = self._run_bell_state_benchmark(bell_name, self.qpu_ibm, "ibm_hardware")
            results["ibm_hardware"][bell_name] = ibm_results

        # Compara√ß√£o
        results["comparison"] = self._compare_bell_results(results)

        return results

    def _run_bell_state_benchmark(
        self, bell_state: str, qpu: QPUInterface, backend_name: str
    ) -> Dict[str, Any]:
        """Executar benchmark de estado Bell."""

        try:
            # Criar circuito Bell
            qc = QuantumCircuit(2, 2)

            if bell_state == "Œ¶+":
                qc.h(0)
                qc.cx(0, 1)
            elif bell_state == "Œ¶-":
                qc.h(0)
                qc.cx(0, 1)
                qc.z(1)
            elif bell_state == "Œ®+":
                qc.h(0)
                qc.x(1)
                qc.cx(0, 1)
            elif bell_state == "Œ®-":
                qc.h(0)
                qc.x(1)
                qc.cx(0, 1)
                qc.z(1)

            qc.measure_all()

            # Executar
            start_time = time.time()
            counts = qpu.execute(qc, shots=1024)
            execution_time = time.time() - start_time

            # Analisar correla√ß√µes
            total_shots = sum(counts.values())
            prob_00 = counts.get("00", 0) / total_shots
            prob_01 = counts.get("01", 0) / total_shots
            prob_10 = counts.get("10", 0) / total_shots
            prob_11 = counts.get("11", 0) / total_shots

            # Calcular entanglement witness
            # Para estado Bell perfeito: P(00) = P(11) = 0.5, P(01) = P(10) = 0
            entanglement_fidelity = 1.0 - abs(prob_00 - prob_11) - prob_01 - prob_10

            return {
                "backend": backend_name,
                "bell_state": bell_state,
                "counts": counts,
                "probabilities": {
                    "00": prob_00,
                    "01": prob_01,
                    "10": prob_10,
                    "11": prob_11,
                },
                "execution_time": execution_time,
                "entanglement_fidelity": entanglement_fidelity,
                "perfect_correlation": abs(prob_00 - prob_11) < 0.1
                and prob_01 < 0.1
                and prob_10 < 0.1,
            }

        except Exception as e:
            logger.error(f"Error in Bell state benchmark: {e}")
            return {"error": str(e)}

    def benchmark_quantum_randomness(self) -> Dict[str, Any]:
        """Benchmark: Qualidade da aleatoriedade qu√¢ntica vs pseudo-aleat√≥ria."""

        logger.info("üé≤ Benchmarking Quantum Randomness Quality")

        results = {
            "simulator": {},
            "ibm_hardware": {},
            "classical_comparison": {},
            "comparison": {},
        }

        # Testar diferentes n√∫meros de bits
        bit_lengths = [8, 16, 32]

        for bit_length in bit_lengths:
            logger.info(f"Testing randomness quality with {bit_length} bits")

            # Simulador qu√¢ntico
            sim_results = self._run_randomness_benchmark(bit_length, self.qpu_sim, "simulator")
            results["simulator"][f"{bit_length}_bits"] = sim_results

            # Hardware IBM
            ibm_results = self._run_randomness_benchmark(bit_length, self.qpu_ibm, "ibm_hardware")
            results["ibm_hardware"][f"{bit_length}_bits"] = ibm_results

            # Compara√ß√£o com PRNG cl√°ssico
            classical_results = self._run_classical_randomness_benchmark(bit_length)
            results["classical_comparison"][f"{bit_length}_bits"] = classical_results

        # Compara√ß√£o geral
        results["comparison"] = self._compare_randomness_results(results)

        return results

    def _run_randomness_benchmark(
        self, bit_length: int, qpu: QPUInterface, backend_name: str
    ) -> Dict[str, Any]:
        """Executar benchmark de qualidade da aleatoriedade."""

        try:
            num_qubits = int(np.ceil(bit_length / 1))  # 1 bit por qubit
            sequences = []

            # Gerar m√∫ltiplas sequ√™ncias
            for i in range(10):
                qc = QuantumCircuit(num_qubits, num_qubits)
                qc.h(range(num_qubits))  # Superposi√ß√£o em todos os qubits
                qc.measure_all()

                counts = qpu.execute(qc, shots=1024)

                # Converter para string bin√°ria
                # Usar o resultado mais frequente como representa√ß√£o
                most_frequent = max(counts, key=lambda k: counts[k])
                binary_string = most_frequent.replace(" ", "")  # Remove espa√ßos

                # Truncar para o tamanho desejado
                binary_string = binary_string[:bit_length]
                sequences.append(binary_string)

            # Analisar qualidade da aleatoriedade
            quality_metrics = self._analyze_randomness_quality(sequences)

            return {
                "backend": backend_name,
                "bit_length": bit_length,
                "sequences": sequences,
                "quality_metrics": quality_metrics,
            }

        except Exception as e:
            logger.error(f"Error in randomness benchmark: {e}")
            return {"error": str(e)}

    def benchmark_hybrid_qlearning(self) -> Dict[str, Any]:
        """Benchmark: Performance do Q-Learning h√≠brido qu√¢ntico-cl√°ssico."""

        logger.info("üéì Benchmarking Hybrid Q-Learning Performance")

        # Este benchmark compara Q-learning cl√°ssico vs h√≠brido
        # Implementa√ß√£o simplificada para demonstra√ß√£o

        results = {"classical_qlearning": {}, "hybrid_qlearning": {}, "comparison": {}}

        # Simula√ß√£o de ambiente simples (grid world)
        env_config = {"states": 10, "actions": 4, "episodes": 100}

        # Q-Learning Cl√°ssico
        classical_results = self._run_classical_qlearning(env_config)
        results["classical_qlearning"] = classical_results

        # Q-Learning H√≠brido (usando simulador para explora√ß√£o qu√¢ntica)
        hybrid_results = self._run_hybrid_qlearning(env_config, self.qpu_sim)
        results["hybrid_qlearning"] = hybrid_results

        # Compara√ß√£o
        results["comparison"] = self._compare_qlearning_results(classical_results, hybrid_results)

        return results

    def benchmark_noise_impact(self) -> Dict[str, Any]:
        """Benchmark: Impacto do ru√≠do qu√¢ntico na performance."""

        logger.info("üìä Benchmarking Noise Impact Analysis")

        results = {
            "simulator_clean": {},
            "ibm_hardware_noisy": {},
            "noise_analysis": {},
        }

        # Testar circuitos de diferentes profundidades
        circuit_depths = [5, 10, 15, 20]

        for depth in circuit_depths:
            logger.info(f"Testing noise impact at circuit depth {depth}")

            # Simulador (sem ru√≠do)
            clean_results = self._run_noise_test(depth, self.qpu_sim, "clean")
            results["simulator_clean"][f"depth_{depth}"] = clean_results

            # Hardware IBM (com ru√≠do)
            noisy_results = self._run_noise_test(depth, self.qpu_ibm, "noisy")
            results["ibm_hardware_noisy"][f"depth_{depth}"] = noisy_results

        # An√°lise de degrada√ß√£o por ru√≠do
        results["noise_analysis"] = self._analyze_noise_impact(results)

        return results

    def calculate_summary_metrics(self) -> Dict[str, Any]:
        """Calcular m√©tricas de resumo da suite completa."""

        summary = {
            "total_execution_time": time.time() - self.start_time,
            "benchmarks_completed": len(self.results),
            "quantum_advantage_detected": False,
            "noise_impact_significant": False,
            "recommendations": [],
        }

        # Verificar vantagem qu√¢ntica
        if "grover_search" in self.results:
            grover_comp = self.results["grover_search"].get("comparison", {})
            if grover_comp.get("avg_quantum_advantage", 0) > 1.5:
                summary["quantum_advantage_detected"] = True

        # Verificar impacto do ru√≠do
        if "noise_analysis" in self.results:
            noise_analysis = self.results["noise_analysis"]
            if noise_analysis.get("significant_degradation", False):
                summary["noise_impact_significant"] = True

        # Recomenda√ß√µes
        if summary["quantum_advantage_detected"]:
            summary["recommendations"].append(
                "Quantum advantage detected - pursue quantum algorithms"
            )
        else:
            summary["recommendations"].append(
                "No quantum advantage detected - focus on hybrid approaches"
            )

        if summary["noise_impact_significant"]:
            summary["recommendations"].append("High noise impact - implement error mitigation")
        else:
            summary["recommendations"].append("Low noise impact - quantum algorithms feasible")

        return summary

    # M√©todos auxiliares
    def _calculate_fidelity(self, state1: List[float], state2: List[float]) -> float:
        """Calcular fidelity entre dois estados qu√¢nticos."""
        if not state2:
            return 0.0

        s1 = np.array(state1)
        s2 = np.array(state2)

        # Fidelity = |‚ü®œà|œÜ‚ü©|¬≤
        fidelity = abs(np.dot(np.conj(s1), s2)) ** 2

        # Ensure it's a scalar value (fix for "only length-1 arrays can be converted to Python scalars")
        if isinstance(fidelity, np.ndarray):
            fidelity = fidelity.item()

        return float(fidelity)

    def _test_uniformity(self, decisions: List[Dict]) -> Dict[str, Any]:
        """Testar se as decis√µes s√£o uniformemente distribu√≠das."""
        if not decisions:
            return {"uniform": False, "chi_squared": float("inf")}

        # Contar frequ√™ncia de cada op√ß√£o escolhida
        choice_counts = {}
        for decision in decisions:
            choice = decision["chosen_option"]
            choice_counts[choice] = choice_counts.get(choice, 0) + 1

        total_decisions = len(decisions)
        num_options = len(choice_counts)

        if num_options == 0:
            return {"uniform": False, "chi_squared": float("inf")}

        expected_count = total_decisions / num_options

        # Chi-squared test
        chi_squared = sum(
            (count - expected_count) ** 2 / expected_count for count in choice_counts.values()
        )

        # Para distribui√ß√£o uniforme, chi-squared deve ser pequeno
        # Usando alpha=0.05, graus de liberdade = num_options - 1
        critical_value = 3.84  # Aproximado para df=3

        return {
            "uniform": chi_squared < critical_value,
            "chi_squared": chi_squared,
            "critical_value": critical_value,
            "choice_distribution": choice_counts,
        }

    def _analyze_randomness_quality(self, sequences: List[str]) -> Dict[str, Any]:
        """Analisar qualidade da aleatoriedade das sequ√™ncias."""
        if not sequences:
            return {"quality_score": 0.0}

        # Testes b√°sicos de aleatoriedade
        quality_metrics = {
            "sequence_length": len(sequences[0]) if sequences else 0,
            "num_sequences": len(sequences),
            "monobit_test": 0.0,  # Propor√ß√£o de 0s vs 1s
            "runs_test": 0.0,  # N√∫mero de sequ√™ncias de bits iguais
            "quality_score": 0.0,
        }

        # Monobit test (propor√ß√£o de 1s deve ser ~0.5)
        total_bits = 0
        total_ones = 0

        for seq in sequences:
            total_bits += len(seq)
            total_ones += seq.count("1")

        if total_bits > 0:
            proportion_ones = total_ones / total_bits
            quality_metrics["monobit_test"] = 1.0 - abs(proportion_ones - 0.5) * 2  # 1.0 = perfeito

        # Runs test (contar mudan√ßas entre 0s e 1s)
        total_runs = 0
        for seq in sequences:
            runs = 1
            for i in range(1, len(seq)):
                if seq[i] != seq[i - 1]:
                    runs += 1
            total_runs += runs

        expected_runs = (total_bits / len(sequences)) * 0.5 + 1  # Aproximado
        if expected_runs > 0:
            quality_metrics["runs_test"] = min(1.0, expected_runs / total_runs)

        # Pontua√ß√£o geral
        quality_metrics["quality_score"] = (
            quality_metrics["monobit_test"] + quality_metrics["runs_test"]
        ) / 2

        return quality_metrics

    # M√©todos de compara√ß√£o (placeholders - implementar conforme necess√°rio)
    def _compare_decision_making_results(self, results: Dict) -> Dict[str, Any]:
        return {"comparison": "placeholder"}

    def _compare_memory_results(self, results: Dict) -> Dict[str, Any]:
        return {"comparison": "placeholder"}

    def _compare_grover_results(self, results: Dict) -> Dict[str, Any]:
        return {"comparison": "placeholder"}

    def _compare_bell_results(self, results: Dict) -> Dict[str, Any]:
        return {"comparison": "placeholder"}

    def _compare_randomness_results(self, results: Dict) -> Dict[str, Any]:
        return {"comparison": "placeholder"}

    def _run_classical_randomness_benchmark(self, bit_length: int) -> Dict[str, Any]:
        return {"classical_randomness": "placeholder"}

    def _run_classical_qlearning(self, env_config: Dict) -> Dict[str, Any]:
        return {"classical_qlearning": "placeholder"}

    def _run_hybrid_qlearning(self, env_config: Dict, qpu: QPUInterface) -> Dict[str, Any]:
        return {"hybrid_qlearning": "placeholder"}

    def _compare_qlearning_results(self, classical: Dict, hybrid: Dict) -> Dict[str, Any]:
        return {"qlearning_comparison": "placeholder"}

    def _run_noise_test(self, depth: int, qpu: QPUInterface, noise_type: str) -> Dict[str, Any]:
        return {"noise_test": "placeholder"}

    def _analyze_noise_impact(self, results: Dict) -> Dict[str, Any]:
        return {"noise_analysis": "placeholder"}


def main():
    """Executar suite completa de benchmarks qu√¢nticos."""

    print("üöÄ OmniMind Quantum Consciousness - IBM Hardware Benchmark Suite")
    print("=" * 70)
    print("Este script executa benchmarks abrangentes no IBM Quantum para validar:")
    print("‚Ä¢ Vantagem qu√¢ntica real vs simula√ß√£o")
    print("‚Ä¢ Performance de algoritmos qu√¢nticos em hardware")
    print("‚Ä¢ Impacto do ru√≠do qu√¢ntico")
    print("‚Ä¢ M√©tricas para publica√ß√µes cient√≠ficas")
    print()
    print("Hardware: IBM Quantum ibm_torino (133 qubits)")
    print("Dura√ß√£o estimada: 30-60 minutos")
    print("Custo estimado: ~50-100 cr√©ditos IBM")
    print("=" * 70)

    # Carregar token IBM
    # Explicitly load .env from current directory to avoid find_dotenv stack issues
    load_dotenv(os.path.join(os.getcwd(), ".env"))
    ibm_token = os.getenv("IBM_API_KEY")

    if not ibm_token:
        print("‚ùå ERRO: IBM_API_KEY n√£o encontrado no .env")
        print("Configure seu token IBM antes de executar os benchmarks")
        return 1

    print(f"‚úÖ Token IBM configurado: {ibm_token[:10]}...")

    try:
        # Inicializar suite de benchmarks
        suite = QuantumBenchmarkSuite(ibm_token)

        # Executar benchmarks completos
        results = suite.run_full_benchmark_suite()

        # Salvar resultados
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        output_file = Path(f"data/benchmarks/quantum_benchmark_suite_{timestamp}.json")

        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print("‚úÖ Benchmarks conclu√≠dos com sucesso!")
        print(f"üìä Resultados salvos em: {output_file}")
        print(f"‚è±Ô∏è  Tempo total: {results['summary']['total_execution_time']:.1f} segundos")

        # Resumo executivo
        print("\nüìà RESUMO EXECUTIVO:")
        print(f"‚Ä¢ Benchmarks executados: {results['summary']['benchmarks_completed']}")
        print(f"‚Ä¢ Vantagem qu√¢ntica detectada: {results['summary']['quantum_advantage_detected']}")
        print(f"‚Ä¢ Impacto significativo do ru√≠do: {results['summary']['noise_impact_significant']}")

        if results["summary"]["recommendations"]:
            print("\nüí° RECOMENDA√á√ïES:")
            for rec in results["summary"]["recommendations"]:
                print(f"‚Ä¢ {rec}")

        return 0

    except Exception as e:
        print(f"‚ùå ERRO FATAL: {str(e)}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
