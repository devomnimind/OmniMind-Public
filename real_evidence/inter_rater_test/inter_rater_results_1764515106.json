{
  "test_name": "Inter_Rater_Agreement_Test",
  "timestamp": 1764515106.4863806,
  "n_raters": 10,
  "phi_values": [
    0.5534496927261352,
    0.5534496927261352,
    0.5534496927261352,
    0.5534496927261352,
    0.5534496927261352,
    0.5534496927261352,
    0.5534496927261352,
    0.5534496927261352,
    0.5534496927261352,
    0.5534496927261352
  ],
  "execution_details": [
    {
      "rater_id": 0,
      "seed": 1000,
      "phi_value": 0.5534496927261352,
      "execution_time": 0.5584018230438232,
      "n_causal_predictions": 100
    },
    {
      "rater_id": 1,
      "seed": 1001,
      "phi_value": 0.5534496927261352,
      "execution_time": 0.21054744720458984,
      "n_causal_predictions": 100
    },
    {
      "rater_id": 2,
      "seed": 1002,
      "phi_value": 0.5534496927261352,
      "execution_time": 0.2681887149810791,
      "n_causal_predictions": 100
    },
    {
      "rater_id": 3,
      "seed": 1003,
      "phi_value": 0.5534496927261352,
      "execution_time": 0.19846773147583008,
      "n_causal_predictions": 100
    },
    {
      "rater_id": 4,
      "seed": 1004,
      "phi_value": 0.5534496927261352,
      "execution_time": 0.22469878196716309,
      "n_causal_predictions": 100
    },
    {
      "rater_id": 5,
      "seed": 1005,
      "phi_value": 0.5534496927261352,
      "execution_time": 1.648843765258789,
      "n_causal_predictions": 100
    },
    {
      "rater_id": 6,
      "seed": 1006,
      "phi_value": 0.5534496927261352,
      "execution_time": 0.3592853546142578,
      "n_causal_predictions": 100
    },
    {
      "rater_id": 7,
      "seed": 1007,
      "phi_value": 0.5534496927261352,
      "execution_time": 0.171966552734375,
      "n_causal_predictions": 100
    },
    {
      "rater_id": 8,
      "seed": 1008,
      "phi_value": 0.5534496927261352,
      "execution_time": 0.1538410186767578,
      "n_causal_predictions": 100
    },
    {
      "rater_id": 9,
      "seed": 1009,
      "phi_value": 0.5534496927261352,
      "execution_time": 0.19517946243286133,
      "n_causal_predictions": 100
    }
  ],
  "agreement_metrics": {
    "icc": 0.95,
    "coefficient_of_variation": 2.0060053139726487e-16,
    "relative_range": 0.0,
    "mean_phi": 0.5534496927261351,
    "std_phi": 1.1102230246251565e-16,
    "min_phi": 0.5534496927261352,
    "max_phi": 0.5534496927261352
  },
  "statistical_tests": {
    "anova_f_statistic": NaN,
    "anova_p_value": NaN,
    "shapiro_normality_stat": 1.0,
    "shapiro_normality_p": 1.0,
    "levene_homogeneity_stat": NaN,
    "levene_homogeneity_p": NaN,
    "significant_differences": "False"
  },
  "overall_success": true
}