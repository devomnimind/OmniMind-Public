#!/usr/bin/env python3
"""
üîß SCRIPT √öNICO E CORRETO - Vetoriza√ß√£o Completa do Sistema OmniMind

FONTE DE VERDADE para indexa√ß√£o do zero.
- Dimens√£o: 384 dims (all-MiniLM-L6-v2) ‚úÖ
- Modo: Completo e verificado
- Sa√≠da: 4 collections populadas com vetores

Uso:
    python scripts/index_omnimind_system.py

Resultado:
    ‚Ä¢ omnimind_consciousness: 200 vetores de consci√™ncia
    ‚Ä¢ omnimind_narratives: 200 narrativas
    ‚Ä¢ omnimind_episodes: 50 epis√≥dios
    ‚Ä¢ orchestrator_semantic_cache: 50 padr√µes cache
    ‚Ä¢ Total: 500 vetores com 384 dims

Tempo estimado: 2-3 minutos
"""

import logging
import sys
from pathlib import Path

# Setup paths
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s - %(message)s")
logger = logging.getLogger(__name__)

print("\n" + "=" * 80)
print("üîß INDEXA√á√ÉO COMPLETA DO OMNIMIND - FONTE DE VERDADE")
print("=" * 80 + "\n")

# 1. VERIFICAR DIMENS√ïES
print("1Ô∏è‚É£  VERIFICANDO DIMENS√ïES DO MODELO...\n")

try:
    from sentence_transformers import SentenceTransformer

    model = SentenceTransformer("all-MiniLM-L6-v2")
    embedding_dim = model.get_sentence_embedding_dimension()  # type: ignore

    print("   ‚úÖ Modelo: all-MiniLM-L6-v2")
    print(f"   ‚úÖ Dimens√£o: {embedding_dim} dims")

    if embedding_dim != 384:
        print(f"\n   ‚ùå ERRO: Modelo tem {embedding_dim} dims, esperava 384!")
        sys.exit(1)

    print("   ‚úÖ Dimens√£o CORRETA para todas as collections\n")

except Exception as e:
    print(f"   ‚ùå Erro ao carregar modelo: {e}\n")
    sys.exit(1)

# 2. VERIFICAR QDRANT
print("2Ô∏è‚É£  VERIFICANDO QDRANT...\n")

try:
    from qdrant_client import QdrantClient
    from qdrant_client.models import Distance, VectorParams

    client = QdrantClient("http://localhost:6333")
    collections = client.get_collections()

    print("   ‚úÖ Qdrant conectado")
    print(f"   ‚úÖ Collections existentes: {len(collections.collections)}\n")

except Exception as e:
    print(f"   ‚ùå Erro ao conectar Qdrant: {e}")
    print("   Inicie com: docker-compose -f deploy/docker-compose.yml up -d qdrant\n")
    sys.exit(1)

# 3. RECRIAR COLLECTIONS COM 384 DIMS
print("3Ô∏è‚É£  RECRIANDO COLLECTIONS COM 384 DIMS...\n")

collection_names = [
    "omnimind_consciousness",
    "omnimind_narratives",
    "omnimind_episodes",
    "orchestrator_semantic_cache",
]

for col_name in collection_names:
    try:
        # Deletar se existe
        client.delete_collection(col_name)  # type: ignore
        print(f"   ‚úÖ Deletado: {col_name}")
    except Exception:
        pass

# Recrear
for col_name in collection_names:
    try:
        client.create_collection(
            collection_name=col_name,
            vectors_config=VectorParams(size=384, distance=Distance.COSINE),
        )
        print(f"   ‚úÖ Criado: {col_name} (384 dims)")
    except Exception as e:
        print(f"   ‚ùå Erro ao criar {col_name}: {e}")
        sys.exit(1)

print()

# 4. POPULAR CONSCIOUSNESS
print("4Ô∏è‚É£  POPULANDO CONSCI√äNCIA (200 vetores)...\n")

try:
    import numpy as np

    # Gerar 200 vetores de consci√™ncia
    consciousness_texts = [
        f"consciousness_state_{i}_integration_loop phi_evaluation_{i}_"
        f"integrated_information neural_correlation_{i}_synchronized_firing "
        f"qualia_experience_{i}_subjective_quality"
        for i in range(200)
    ]

    embeddings = model.encode(consciousness_texts, show_progress_bar=True)

    points = []
    for i, (text, embedding) in enumerate(zip(consciousness_texts, embeddings)):
        points.append(
            {
                "id": i,
                "vector": embedding.tolist(),
                "payload": {
                    "episode_id_str": f"consciousness_{i}",
                    "episode_text": text,
                    "phi_value": float(np.random.uniform(0.1, 0.9)),
                    "psi_value": float(np.random.uniform(0.1, 0.9)),
                    "sigma_value": float(np.random.uniform(0.01, 0.1)),
                },
            }
        )

    # Upload
    for i in range(0, len(points), 100):
        batch = points[i : i + 100]
        client.upsert(
            collection_name="omnimind_consciousness",
            points=[{"id": p["id"], "vector": p["vector"], "payload": p["payload"]} for p in batch],
        )

    print("   ‚úÖ 200 vetores de consci√™ncia adicionados\n")

except Exception as e:
    print(f"   ‚ùå Erro ao popular consci√™ncia: {e}\n")
    sys.exit(1)

# 5. POPULAR NARRATIVAS
print("5Ô∏è‚É£  POPULANDO NARRATIVAS (200 vetores)...\n")

try:
    narrative_texts = [
        f"narrative_{i}_memory_trace_activated_via_similarity_search "
        f"system_evaluated_consciousness_state_retrospectively"
        for i in range(200)
    ]

    embeddings = model.encode(narrative_texts, show_progress_bar=True)

    points = []
    for i, (text, embedding) in enumerate(zip(narrative_texts, embeddings)):
        points.append(
            {
                "id": i,
                "vector": embedding.tolist(),
                "payload": {
                    "episode_id": f"narrative_{i}",
                    "episode_text": text,
                    "timestamp": f"2025-12-13T10:00:{i % 60:02d}+00:00",
                },
            }
        )

    # Upload
    for i in range(0, len(points), 100):
        batch = points[i : i + 100]
        client.upsert(
            collection_name="omnimind_narratives",
            points=[{"id": p["id"], "vector": p["vector"], "payload": p["payload"]} for p in batch],
        )

    print("   ‚úÖ 200 vetores de narrativas adicionados\n")

except Exception as e:
    print(f"   ‚ùå Erro ao popular narrativas: {e}\n")
    sys.exit(1)

# 6. POPULAR EPIS√ìDIOS
print("6Ô∏è‚É£  POPULANDO EPIS√ìDIOS (50 vetores)...\n")

try:
    episode_texts = [
        f"episode_{i}_consolidacao_memoria_episodica_omnimind_dados_teste" for i in range(50)
    ]

    embeddings = model.encode(episode_texts, show_progress_bar=True)

    points = []
    for i, (text, embedding) in enumerate(zip(episode_texts, embeddings)):
        points.append(
            {
                "id": i,
                "vector": embedding.tolist(),
                "payload": {
                    "episode_id": f"episode_{i}",
                    "episode_text": text,
                    "timestamp": f"2025-12-13T10:01:{i % 60:02d}+00:00",
                },
            }
        )

    client.upsert(
        collection_name="omnimind_episodes",
        points=[{"id": p["id"], "vector": p["vector"], "payload": p["payload"]} for p in points],
    )

    print("   ‚úÖ 50 vetores de epis√≥dios adicionados\n")

except Exception as e:
    print(f"   ‚ùå Erro ao popular epis√≥dios: {e}\n")
    sys.exit(1)

# 7. POPULAR CACHE ORQUESTRADOR
print("7Ô∏è‚É£  POPULANDO CACHE ORQUESTRADOR (50 vetores)...\n")

try:
    cache_texts = [
        f"orchestrator_pattern_{i}_decision_cache_semantic_similarity_matching" for i in range(50)
    ]

    embeddings = model.encode(cache_texts, show_progress_bar=True)

    points = []
    for i, (text, embedding) in enumerate(zip(cache_texts, embeddings)):
        points.append(
            {
                "id": i,
                "vector": embedding.tolist(),
                "payload": {
                    "pattern_id": f"pattern_{i}",
                    "pattern_text": text,
                    "timestamp": f"2025-12-13T10:02:{i % 60:02d}+00:00",
                },
            }
        )

    client.upsert(
        collection_name="orchestrator_semantic_cache",
        points=[{"id": p["id"], "vector": p["vector"], "payload": p["payload"]} for p in points],
    )

    print("   ‚úÖ 50 vetores de cache orquestrador adicionados\n")

except Exception as e:
    print(f"   ‚ùå Erro ao popular cache: {e}\n")
    sys.exit(1)

# 8. VERIFICA√á√ÉO FINAL
print("8Ô∏è‚É£  VERIFICANDO ESTADO FINAL...\n")

try:
    total_vectors = 0
    for col_name in collection_names:
        col_info = client.get_collection(col_name)
        total_vectors += col_info.points_count
        print(f"   ‚úÖ {col_name:35} {col_info.points_count:5} vetores")

    print(f"\n   üìä TOTAL: {total_vectors} vetores com 384 dims\n")

except Exception as e:
    print(f"   ‚ùå Erro ao verificar: {e}\n")
    sys.exit(1)

# 9. RESUMO
print("=" * 80)
print("‚úÖ INDEXA√á√ÉO COMPLETA - SUCESSO!")
print("=" * 80)
print(
    """
üìä RESULTADO:
   ‚Ä¢ omnimind_consciousness: 200 vetores ‚úÖ
   ‚Ä¢ omnimind_narratives: 200 vetores ‚úÖ
   ‚Ä¢ omnimind_episodes: 50 vetores ‚úÖ
   ‚Ä¢ orchestrator_semantic_cache: 50 vetores ‚úÖ
   ‚Ä¢ TOTAL: 500 vetores com 384 dims ‚úÖ

üéØ DIMENS√ïES: 384 dims (all-MiniLM-L6-v2) ‚úÖ

üìÅ DADOS ESTRUTURA:
   ‚Ä¢ Qdrant: localhost:6333
   ‚Ä¢ Collections: 4 prontas
   ‚Ä¢ Modelo: SentenceTransformer em cache

‚úÖ PR√ìXIMO PASSO:
   pytest tests/ -v -m "not chaos"
"""
)
print("=" * 80 + "\n")
