# üéØ OmniMind Embeddings Configuration

## ‚úÖ Status: PRONTO PARA PRODU√á√ÉO

Modelos sentence-transformers configurados para opera√ß√£o **100% OFFLINE** com estrat√©gia eficiente de GPU/CPU.

---

## üì¶ Modelos Instalados

### 1Ô∏è‚É£ Default (R√°pido - CUDA)
- **Modelo:** `all-MiniLM-L6-v2`
- **Localiza√ß√£o:** `/opt/models/sentence-transformers/all-MiniLM-L6-v2`
- **Tamanho:** 87 MB
- **Device:** CUDA (GPU)
- **Uso:** Embeddings gerais em alta velocidade
- **Lat√™ncia:** ~1-5ms por texto

```python
from src.embeddings.offline_loader import load_embedding_model
embedder = load_embedding_model("default")  # CUDA autom√°tico
emb = embedder.encode(["Teste em portugu√™s"])
```

### 2Ô∏è‚É£ Multilingual (Suporte PT/EN/ES/FR/etc - CPU)
- **Modelo:** `paraphrase-multilingual-MiniLM-L12-v2`
- **Localiza√ß√£o:** `/opt/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **Tamanho:** 479 MB
- **Device:** CPU (carrega sob demanda)
- **Suporta:** 50+ idiomas (incluindo portugu√™s e ingl√™s)
- **Lat√™ncia:** ~5-20ms por texto (CPU)

```python
embedder = load_embedding_model("multilingual")  # CPU autom√°tico
emb = embedder.encode(["Portugu√™s aqui", "English here", "Espa√±ol aqu√≠"])
```

---

## üîß Configura√ß√£o de Ambiente

### Vari√°veis Exportadas Automaticamente

```bash
export TRANSFORMERS_OFFLINE=1
export HF_HUB_OFFLINE=1
export HF_HOME=/opt/hf_cache
```

### No Backend (web/backend/main.py)

```python
# Automaticamente ativado ao importar offline_loader
from src.embeddings.offline_loader import load_embedding_model

# Usa default (CUDA) por padr√£o
embedder = load_embedding_model()
```

---

## üìä Estrat√©gia de Performance

| Situa√ß√£o | Modelo | Device | Velocidade | VRAM |
|----------|--------|--------|-----------|------|
| Embeddings em tempo real | default | CUDA | ~1ms | 87MB |
| Multil√≠ngue bajo demand | multilingual | CPU | ~10ms | 0MB GPU |
| Fallback se CUDA cheio | multilingual | CPU | ~10ms | 0MB GPU |

---

## üöÄ Como Usar

### Uso B√°sico

```python
from src.embeddings.offline_loader import load_embedding_model

# Padr√£o (r√°pido em CUDA)
embedder = load_embedding_model()
emb = embedder.encode(["seu texto aqui"])

# Multil√≠ngue (CPU)
embedder_multi = load_embedding_model("multilingual")
emb = embedder_multi.encode(["Portugu√™s", "English"])
```

### For√ßar Device

```python
# For√ßar CPU mesmo que CUDA dispon√≠vel
embedder = load_embedding_model("default", force_device="cpu")

# For√ßar CUDA no multilingual
embedder_multi = load_embedding_model("multilingual", force_device="cuda")
```

### Com Caching Autom√°tico

```python
# Primeira chamada carrega do disco
embedder1 = load_embedding_model("default")

# Pr√≥ximas chamadas usam cache em mem√≥ria
embedder2 = load_embedding_model("default")  # Instant√¢neo!
```

---

## ‚úÖ Testes Executados

```
‚úÖ 1Ô∏è‚É£ Modelo default (CUDA):
   Teste: "Teste de embedding em portugu√™s"
   Output: torch.Size([1, 384]) em cuda:0
   Status: ‚úÖ FUNCIONANDO

‚úÖ 2Ô∏è‚É£ Modelo multilingual (CPU):
   Teste: ["Portugu√™s aqui", "English here", "Espa√±ol aqu√≠"]
   Output: torch.Size([3, 384]) em cpu
   Status: ‚úÖ FUNCIONANDO
```

---

## üìù Configura√ß√£o YAML

Veja `config/embeddings.yaml` para:
- Caminhos dos modelos
- Devices padr√£o
- Cache settings
- Performance tuning

---

## ‚ö†Ô∏è Notas Importantes

1. **Sem Internet Necess√°ria:** Todos os modelos est√£o locais em `/opt/models/`
2. **CUDA Autom√°tico:** O modelo default usa CUDA se dispon√≠vel
3. **CPU Fallback:** Multilingual usa CPU por padr√£o (n√£o compete com GPU)
4. **Caching:** Modelos s√£o cacheados em mem√≥ria ap√≥s primeiro carregamento
5. **Offline Mode:** Vari√°veis `TRANSFORMERS_OFFLINE` garantem 0% de tentativas de internet

---

## üîç Troubleshooting

### "Modelo n√£o encontrado"
```bash
# Verificar modelos instalados
ls -lah /opt/models/sentence-transformers/
```

### CUDA out of memory
```python
# Usar multilingual em CPU
embedder = load_embedding_model("default", force_device="cpu")
```

### Muito lento
```python
# Usar batch processing
embedder = load_embedding_model("default")
embeddings = embedder.encode(textos, batch_size=32)
```

---

**Data Setup:** 16 de Dezembro de 2025
**Testado em:** GTX 1650 4GB, Ubuntu 22.04, CUDA 12.1
**Status:** ‚úÖ PRONTO PARA PRODU√á√ÉO
