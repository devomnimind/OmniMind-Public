# üîß Guia de Configura√ß√£o de Ambiente - OmniMind

**√öltima Atualiza√ß√£o**: 5 de Dezembro de 2025
**Vers√£o**: Phase 24+ (Lacanian Memory + Autopoietic Evolution)

---

## üìã Pr√©-requisitos do Sistema

### Hardware M√≠nimo Recomendado

- **CPU**: 4 cores (Intel i5/Ryzen 5 ou superior)
- **RAM**: 8GB (16GB recomendado)
- **GPU**: NVIDIA GTX 1650 ou superior (4GB VRAM) - **Opcional mas recomendado**
- **Armazenamento**: 50GB SSD dispon√≠vel
- **SO**: Linux Ubuntu 20.04+ ou similar (Kali Linux 6.16.8+ validado)

### Software Obrigat√≥rio

- **Python**: 3.12.8 (obrigat√≥rio, outras vers√µes podem causar problemas)
- **Ollama**: Instalado e rodando com modelo `phi:latest`
- **CUDA**: 12.4+ (se GPU dispon√≠vel)
- **Git**: Para controle de vers√£o
- **Docker & Docker Compose**: Opcional, para containeriza√ß√£o

---

## üöÄ Instala√ß√£o Passo a Passo

### 1. Clone do Reposit√≥rio

```bash
cd /home/fahbrain/projects
git clone <repository-url> omnimind
cd omnimind
```

### 2. Cria√ß√£o do Ambiente Virtual

```bash
# Criar ambiente virtual com Python 3.12.8
python3.12 -m venv .venv

# Ativar ambiente virtual
source .venv/bin/activate

# Verificar Python
python --version  # Deve ser 3.12.8
```

### 3. Instala√ß√£o de Depend√™ncias

```bash
# Instalar depend√™ncias principais
pip install -r requirements.txt

# Ou instalar por categoria (recomendado)
pip install -r requirements/requirements-core.txt
pip install -r requirements/requirements-dev.txt

# Se GPU dispon√≠vel
pip install -r requirements/requirements-gpu.txt
```

### 4. Configura√ß√£o do Ollama

```bash
# Verificar se Ollama est√° instalado
ollama --version

# Se n√£o estiver, instalar (Linux)
curl -fsSL https://ollama.com/install.sh | sh

# Baixar modelo phi:latest (modelo padr√£o)
ollama pull phi:latest

# Verificar modelos dispon√≠veis
ollama list
# Deve mostrar: phi:latest
```

### 5. Configura√ß√£o de Vari√°veis de Ambiente

Criar arquivo `.env` na raiz do projeto:

```bash
# Modelo LLM
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=phi:latest

# GPU
CUDA_VISIBLE_DEVICES=0
OMNIMIND_GPU=true
OMNIMIND_FORCE_GPU=true

# Modo
OMNIMIND_MODE=development
OMNIMIND_DEV=true
OMNIMIND_DEBUG=true

# Qdrant (opcional)
OMNIMIND_QDRANT_URL=http://localhost:6333
OMNIMIND_QDRANT_API_KEY=

# Supabase (opcional)
OMNIMIND_SUPABASE_URL=
OMNIMIND_SUPABASE_ANON_KEY=

# Dashboard Auth (gerado automaticamente)
# Ver: config/dashboard_auth.json ap√≥s primeira execu√ß√£o
```

### 6. Verifica√ß√£o de GPU (Opcional)

```bash
# Verificar CUDA
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'CUDA Device Count: {torch.cuda.device_count()}')"

# Verificar driver NVIDIA
nvidia-smi

# Output esperado:
# NVIDIA Driver Version: 550.163.01  CUDA Version: 12.4
```

### 7. Configura√ß√£o do Qdrant (Opcional)

Para testes completos que requerem Qdrant:

```bash
# Via Docker
docker run -p 6333:6333 qdrant/qdrant

# Ou via docker-compose
cd deploy
docker-compose up -d qdrant
```

Verificar se est√° rodando:
```bash
curl http://localhost:6333/health
# Deve retornar: {"status":"ok"}
```

---

## üîß Configura√ß√£o Avan√ßada

### Configura√ß√£o do Modelo LLM

**Arquivo**: `config/agent_config.yaml`

```yaml
model:
  name: "phi:latest"           # Modelo prim√°rio (Microsoft Phi)
  provider: "ollama"
  base_url: "http://localhost:11434"
  fallback_model: "qwen2:7b-instruct"  # Fallback se phi n√£o dispon√≠vel
```

**Verificar configura√ß√£o**:
```bash
python -c "
import yaml
with open('config/agent_config.yaml') as f:
    config = yaml.safe_load(f)
    print(f\"Modelo: {config['model']['name']}\")
    print(f\"Provider: {config['model']['provider']}\")
"
```

### Configura√ß√£o de Sudo (Para Scripts que Requerem)

**Script**: `scripts/configure_sudo_omnimind.sh`

```bash
# Executar UMA VEZ para configurar sudo sem senha
bash scripts/configure_sudo_omnimind.sh
```

**O que faz**:
- Cria arquivo `/etc/sudoers.d/omnimind-automation`
- Adiciona permiss√µes NOPASSWD para comandos espec√≠ficos
- Permite execu√ß√£o de scripts sem prompt de senha

---

## ‚úÖ Verifica√ß√£o Final

### Checklist de Valida√ß√£o

```bash
# 1. Python correto
python --version  # Deve ser 3.12.8

# 2. Ambiente virtual ativado
which python  # Deve apontar para .venv/bin/python

# 3. Depend√™ncias instaladas
python -c "import torch; import numpy; print('‚úÖ Depend√™ncias OK')"

# 4. Ollama rodando
curl http://localhost:11434/api/tags  # Deve retornar JSON

# 5. Modelo phi dispon√≠vel
ollama list | grep phi  # Deve mostrar: phi:latest

# 6. GPU (se dispon√≠vel)
python -c "import torch; print(f'GPU: {torch.cuda.is_available()}')"

# 7. Qdrant (se necess√°rio)
curl http://localhost:6333/health  # Deve retornar: {"status":"ok"}
```

### Teste R√°pido

```bash
# Executar teste simples
python -c "
from src.boot import check_hardware
profile = check_hardware()
print(f'‚úÖ Hardware Profile: {profile}')
"
```

---

## üö® Troubleshooting

### Python 3.12.8 n√£o encontrado

```bash
# Instalar Python 3.12.8 via pyenv (recomendado)
pyenv install 3.12.8
pyenv local 3.12.8

# Ou via sistema
sudo apt update
sudo apt install python3.12 python3.12-venv python3.12-dev
```

### Ollama n√£o responde

```bash
# Verificar se Ollama est√° rodando
ps aux | grep ollama

# Se n√£o estiver, iniciar
ollama serve

# Verificar logs
journalctl -u ollama -f  # Se instalado como servi√ßo
```

### Modelo phi:latest n√£o encontrado

```bash
# Baixar modelo
ollama pull phi:latest

# Verificar
ollama list
```

### Erros de GPU/CUDA

```bash
# Verificar vari√°veis de ambiente
echo $CUDA_VISIBLE_DEVICES
echo $CUDA_HOME

# Verificar PyTorch
python -c "import torch; print(torch.__version__)"
python -c "import torch; print(torch.cuda.is_available())"

# Se CUDA n√£o detectado mas GPU presente
export CUDA_VISIBLE_DEVICES=0
export OMNIMIND_FORCE_GPU=true
```

### Qdrant n√£o acess√≠vel

```bash
# Verificar se est√° rodando
docker ps | grep qdrant

# Se n√£o estiver, iniciar
docker run -d -p 6333:6333 qdrant/qdrant

# Verificar health
curl http://localhost:6333/health
```

---

## üìö Pr√≥ximos Passos

Ap√≥s configura√ß√£o bem-sucedida:

1. **Leia o Quick Start**: `docs/canonical/QUICK_START.md`
2. **Execute testes**: `./scripts/run_tests_fast.sh`
3. **Inicie o sistema**: `./scripts/canonical/system/start_omnimind_system.sh`
4. **Consulte a documenta√ß√£o**: `docs/DOCUMENTATION_INDEX.md`

---

## üîó Refer√™ncias

- **Quick Start**: `docs/canonical/QUICK_START.md`
- **Technical Checklist**: `docs/canonical/TECHNICAL_CHECKLIST.md`
- **Safe Commands**: `docs/canonical/SAFE_COMMANDS.md`
- **System Initialization**: `docs/canonical/omnimind_system_initialization.md`

---

**Autor**: Fabr√≠cio da Silva + assist√™ncia de IA (Copilot GitHub/Cursor/Gemini/Perplexity)
