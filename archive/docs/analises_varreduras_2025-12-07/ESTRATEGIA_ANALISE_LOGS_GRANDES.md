# Estrat√©gia de An√°lise de Logs Grandes - OmniMind
**Data:** 2025-12-07
**Problema:** Logs de 600K+ linhas (225MB+) s√£o dif√≠ceis de analisar

---

## üéØ OBJETIVO

Analisar logs extensos de forma eficiente, extraindo:
- ‚úÖ M√©tricas de execu√ß√£o
- ‚úÖ Padr√µes de erro
- ‚úÖ Timeouts e problemas cr√≠ticos
- ‚úÖ Refer√™ncias a modelos
- ‚úÖ Resumo de testes

**Sem carregar tudo na mem√≥ria!**

---

## üìä ESTRAT√âGIA PROPOSTA

### 1. **Processamento em Streaming (Chunks)**

**Problema:** Carregar 627K linhas na mem√≥ria = ~225MB RAM
**Solu√ß√£o:** Processar em chunks de 10.000 linhas

```python
# Processa linha por linha, n√£o carrega tudo
with open(log_path, 'r') as f:
    chunk = []
    for line in f:
        chunk.append(line)
        if len(chunk) >= 10000:
            process_chunk(chunk)  # Processa e descarta
            chunk = []
```

**Benef√≠cios:**
- ‚úÖ Uso de mem√≥ria constante (~50MB)
- ‚úÖ Processa arquivos de qualquer tamanho
- ‚úÖ Progresso vis√≠vel (linhas processadas)

---

### 2. **Compress√£o Inteligente**

**Problema:** Logs grandes ocupam muito espa√ßo
**Solu√ß√£o:** Comprimir ap√≥s an√°lise (gzip)

```bash
# Comprimir log original
gzip consolidated_fast_20251207_120233.log
# Resultado: ~50MB (compress√£o ~78%)
```

**Benef√≠cios:**
- ‚úÖ Reduz espa√ßo em disco
- ‚úÖ Mant√©m hist√≥rico completo
- ‚úÖ Pode descomprimir quando necess√°rio

---

### 3. **Extra√ß√£o de Se√ß√µes-Chave**

**Problema:** Procurar erros em 600K linhas √© lento
**Solu√ß√£o:** Extrair se√ß√µes cr√≠ticas em arquivos separados

```python
# Extrair apenas:
- errors.log (todas as linhas com ERROR/CRITICAL)
- failures.log (todos os testes FAILED)
- timeouts.log (todos os timeouts)
- summary.log (resumo final)
- critical.log (Œ¶ collapse, structural failures)
```

**Benef√≠cios:**
- ‚úÖ An√°lise r√°pida de problemas espec√≠ficos
- ‚úÖ Arquivos pequenos e focados
- ‚úÖ F√°cil de compartilhar/debugar

---

### 4. **Agrega√ß√£o de Padr√µes**

**Problema:** Muitas ocorr√™ncias do mesmo erro
**Solu√ß√£o:** Agregar e contar padr√µes

```python
# Em vez de armazenar todas as linhas:
errors['CUDA_OOM'] = {
    'count': 188,
    'first_occurrence': '...',
    'last_occurrence': '...',
    'sample_lines': [5 exemplos]
}
```

**Benef√≠cios:**
- ‚úÖ Relat√≥rio compacto
- ‚úÖ Foco nos problemas principais
- ‚úÖ F√°cil identificar tend√™ncias

---

## üõ†Ô∏è IMPLEMENTA√á√ÉO

### Script Principal: `scripts/analyze_large_log.py`

#### Uso B√°sico:
```bash
python scripts/analyze_large_log.py data/test_reports/consolidated_fast_20251207_120233.log
```

#### Com Compress√£o:
```bash
python scripts/analyze_large_log.py data/test_reports/consolidated_fast_20251207_120233.log --compress
```

#### Com Extra√ß√£o de Se√ß√µes:
```bash
python scripts/analyze_large_log.py data/test_reports/consolidated_fast_20251207_120233.log --extract-sections
```

#### Tudo Junto:
```bash
python scripts/analyze_large_log.py \
    data/test_reports/consolidated_fast_20251207_120233.log \
    --compress \
    --extract-sections \
    --chunk-size 5000 \
    --output-dir data/test_reports/analysis
```

---

## üìã FLUXO DE AN√ÅLISE RECOMENDADO

### Passo 1: Extrair Se√ß√µes-Chave (R√°pido - 2-3 min)
```bash
python scripts/analyze_large_log.py \
    data/test_reports/consolidated_fast_20251207_120233.log \
    --extract-sections
```

**Resultado:**
- `data/test_reports/analysis/sections/errors.log` - Todos os erros
- `data/test_reports/analysis/sections/failures.log` - Testes que falharam
- `data/test_reports/analysis/sections/timeouts.log` - Timeouts
- `data/test_reports/analysis/sections/critical.log` - Problemas cr√≠ticos
- `data/test_reports/analysis/sections/summary.log` - Resumo final

**Uso:** An√°lise r√°pida de problemas espec√≠ficos

---

### Passo 2: An√°lise Streaming Completa (10-15 min)
```bash
python scripts/analyze_large_log.py \
    data/test_reports/consolidated_fast_20251207_120233.log \
    --chunk-size 10000
```

**Resultado:**
- `data/test_reports/analysis/analysis_TIMESTAMP.json` - Relat√≥rio completo

**Conte√∫do:**
- Estat√≠sticas de testes
- Padr√µes de erro agregados
- Timeouts detectados
- Refer√™ncias a modelos
- Quest√µes cr√≠ticas

---

### Passo 3: Comprimir Log Original (Opcional)
```bash
python scripts/analyze_large_log.py \
    data/test_reports/consolidated_fast_20251207_120233.log \
    --compress
```

**Resultado:**
- `data/test_reports/consolidated_fast_20251207_120233.log.gz` - Log comprimido (~50MB)

**Benef√≠cio:** Economiza espa√ßo, mant√©m hist√≥rico

---

## üìä ESTRUTURA DE SA√çDA

### Diret√≥rio de An√°lise
```
data/test_reports/analysis/
‚îú‚îÄ‚îÄ sections/
‚îÇ   ‚îú‚îÄ‚îÄ errors.log          # Linhas com ERROR/CRITICAL
‚îÇ   ‚îú‚îÄ‚îÄ failures.log        # Testes FAILED
‚îÇ   ‚îú‚îÄ‚îÄ timeouts.log        # Timeouts detectados
‚îÇ   ‚îú‚îÄ‚îÄ critical.log        # Problemas cr√≠ticos (Œ¶, structural)
‚îÇ   ‚îî‚îÄ‚îÄ summary.log         # Resumo final de testes
‚îú‚îÄ‚îÄ analysis_20251207_140912.json  # Relat√≥rio completo JSON
‚îî‚îÄ‚îÄ consolidated_fast_20251207_120233.log.gz  # Log comprimido (se --compress)
```

### Relat√≥rio JSON
```json
{
  "timestamp": "2025-12-07T14:09:12",
  "summary": {
    "total_tests": 4479,
    "passed": 4281,
    "failed": 85,
    "errors": 26,
    "skipped": 87,
    "success_rate": 95.6
  },
  "errors": {
    "CUDA_OOM": {
      "count": 188,
      "first_occurrence": "...",
      "sample_lines": [...]
    }
  },
  "timeouts": {
    "30": 45,
    "60": 120,
    "120": 89,
    "240": 1
  },
  "critical_issues_count": 15,
  "model_references": {
    "gpt-4": 4,
    "phi": 1200,
    "qwen": 450
  }
}
```

---

## üöÄ OTIMIZA√á√ïES

### 1. Chunk Size Ajust√°vel
- **Padr√£o:** 10.000 linhas
- **Mem√≥ria baixa:** `--chunk-size 5000`
- **Mem√≥ria alta:** `--chunk-size 50000`

### 2. Regex Compilado
- Padr√µes compilados uma vez
- Reutilizados em todas as linhas
- ~10x mais r√°pido que recompilar

### 3. Agrega√ß√£o Incremental
- Contadores incrementais (n√£o armazena todas as linhas)
- Apenas amostras (5 primeiras ocorr√™ncias)
- Mem√≥ria constante

### 4. Processamento Paralelo (Futuro)
- Dividir log em chunks
- Processar chunks em paralelo
- Agregar resultados

---

## üìà COMPARA√á√ÉO: ANTES vs DEPOIS

### ANTES (Script Original)
- ‚ùå Carrega tudo na mem√≥ria (225MB+)
- ‚ùå Falha com MemoryError
- ‚ùå Lento (processa tudo de uma vez)
- ‚ùå N√£o escala para logs maiores

### DEPOIS (Script Novo)
- ‚úÖ Processa em streaming (mem√≥ria constante)
- ‚úÖ Funciona com logs de qualquer tamanho
- ‚úÖ R√°pido (progresso vis√≠vel)
- ‚úÖ Escal√°vel (chunks configur√°veis)

---

## üîç AN√ÅLISE DE SE√á√ïES EXTRA√çDAS

### errors.log
```bash
# Ver todos os erros
cat data/test_reports/analysis/sections/errors.log | head -50

# Contar tipos de erro
grep -o "ERROR.*" data/test_reports/analysis/sections/errors.log | sort | uniq -c | sort -rn
```

### failures.log
```bash
# Ver testes que falharam
cat data/test_reports/analysis/sections/failures.log

# Extrair nomes de testes
grep -o "FAILED.*::.*::.*" data/test_reports/analysis/sections/failures.log
```

### timeouts.log
```bash
# Ver timeouts
cat data/test_reports/analysis/sections/timeouts.log

# Contar timeouts por valor
grep -oE "timeout.*\d+\s*(?:s|sec)" data/test_reports/analysis/sections/timeouts.log | sort | uniq -c
```

---

## ‚úÖ CHECKLIST DE USO

### An√°lise R√°pida (5 minutos)
- [ ] Extrair se√ß√µes-chave: `--extract-sections`
- [ ] Verificar `errors.log` para problemas cr√≠ticos
- [ ] Verificar `failures.log` para testes que falharam
- [ ] Verificar `summary.log` para estat√≠sticas gerais

### An√°lise Completa (15 minutos)
- [ ] Rodar an√°lise streaming completa
- [ ] Revisar `analysis_TIMESTAMP.json`
- [ ] Identificar padr√µes de erro principais
- [ ] Verificar timeouts e quest√µes cr√≠ticas

### Manuten√ß√£o (Opcional)
- [ ] Comprimir log original: `--compress`
- [ ] Mover log comprimido para arquivo
- [ ] Manter apenas se√ß√µes extra√≠das para refer√™ncia r√°pida

---

## üéØ CASOS DE USO

### 1. Debug R√°pido de Erro Espec√≠fico
```bash
# Extrair apenas erros
python scripts/analyze_large_log.py log.log --extract-sections

# Procurar erro espec√≠fico
grep "CUDA out of memory" data/test_reports/analysis/sections/errors.log
```

### 2. An√°lise Completa para Relat√≥rio
```bash
# An√°lise completa + compress√£o
python scripts/analyze_large_log.py log.log --compress

# Usar JSON para gerar relat√≥rio
cat data/test_reports/analysis/analysis_*.json | jq '.summary'
```

### 3. Monitoramento Cont√≠nuo
```bash
# Script automatizado
#!/bin/bash
LOG_FILE="data/test_reports/consolidated_fast_$(date +%Y%m%d_%H%M%S).log"
python scripts/analyze_large_log.py "$LOG_FILE" --compress --extract-sections
```

---

## üìù NOTAS T√âCNICAS

### Limita√ß√µes Conhecidas
- **Regex pode ser lento:** Para logs muito grandes, considerar otimiza√ß√µes
- **Encoding:** Usa `errors='ignore'` para lidar com caracteres inv√°lidos
- **Mem√≥ria:** Chunk size pode precisar ajuste em m√°quinas com pouca RAM

### Melhorias Futuras
- [ ] Processamento paralelo de chunks
- [ ] Cache de padr√µes compilados
- [ ] Suporte a m√∫ltiplos formatos de log
- [ ] Interface web para visualiza√ß√£o
- [ ] Integra√ß√£o com dashboard

---

**Documento criado:** 2025-12-07
**Script:** `scripts/analyze_large_log.py`
**Status:** ‚úÖ Pronto para uso

