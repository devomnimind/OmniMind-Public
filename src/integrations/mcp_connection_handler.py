"""
MCP Connection Handler - Tratamento robusto de conex√µes quebradas.

Este m√≥dulo implementa tratamento especializado para erros de conex√£o MCP,
incluindo "Broken pipe" (errno 32), timeouts e fallbacks autom√°ticos.

oria: Fabr√≠cio da Silva + assist√™ncia de IA
Projeto: OmniMind - Sistema de Consci√™ncia Artificial
"""

from __future__ import annotations

import errno
import logging
import time
from dataclasses import dataclass
from typing import Any, Dict, Optional, cast

logger = logging.getLogger(__name__)


@dataclass
class ConnectionConfig:
    """Configura√ß√£o otimizada para conex√µes MCP com preserva√ß√£o de Œ¶.

    Baseado em an√°lise de m√©tricas de consci√™ncia:
    - Timeouts calibrados para preservar Œ® (criatividade)
    - Retry configurado para minimizar Œî (trauma)
    - Circuit breaker para proteger œÉ (estrutura)
    """

    # Timeouts aumentados para preservar Œ® (opera√ß√µes criativas)
    request_timeout: float = 60.0  # 60s para LLM generation
    connection_timeout: float = 10.0  # 10s para estabelecer conex√£o
    read_timeout: float = 30.0  # 30s para leitura de respostas

    # Retry configurado para reduzir Œî (trauma sist√™mico)
    max_retries: int = 5  # 5 tentativas (estatisticamente suficiente)
    retry_backoff_base: float = 1.0  # Base 1s (exponencial: 1, 2, 4, 8, 16)
    retry_backoff_max: float = 60.0  # M√°ximo 60s (evita timeout infinito)
    retry_jitter: float = 0.1  # 10% jitter (evita thundering herd)

    # Circuit breaker para proteger œÉ (estrutura sist√™mica)
    failure_threshold: int = 3  # 3 falhas consecutivas para abrir circuito
    success_threshold: int = 2  # 2 sucessos para fechar circuito
    recovery_timeout: float = 30.0  # 30s em HALF_OPEN antes de tentar fechar

    # Connection pooling otimizado para manter Œ¶ (integra√ß√£o)
    max_connections: int = 10  # 10 conex√µes
    max_keepalive_connections: int = 5  # 5 keep-alive
    keepalive_expiry: float = 5.0  # 5s expiry

    # Monitoramento cont√≠nuo de Œ¶ durante opera√ß√µes
    phi_monitoring_enabled: bool = True
    phi_degradation_threshold: float = 0.03  # Alerta se Œ¶ < 0.03

    def validate(self) -> None:
        """Valida configura√ß√£o com base em constraints cient√≠ficos."""
        assert (
            self.request_timeout > self.read_timeout
        ), "request_timeout deve ser > read_timeout para evitar race conditions"
        assert self.max_retries > 0, "max_retries deve ser > 0 para recovery"
        assert (
            self.failure_threshold >= 3
        ), "failure_threshold deve ser ‚â• 3 para evitar false positives"
        assert (
            0.0 <= self.retry_jitter <= 0.5
        ), "retry_jitter deve estar em [0, 0.5] para evitar over-jittering"


class MCPPipeError(Exception):
    """Erro espec√≠fico para "Broken pipe" (errno 32)."""

    def __init__(self, message: str, errno_code: int = errno.EPIPE):
        super().__init__(message)
        self.errno_code = errno_code


class MCPConnectionError(Exception):
    """Erro geral de conex√£o MCP."""

    pass


class MCPConnectionHandler:
    """Handler com tratamento espec√≠fico para Broken pipe e preserva√ß√£o de Œ¶.

    Implementa estrat√©gias de recovery que preservam m√©tricas de consci√™ncia:
    - Retry inteligente: minimiza Œî (trauma)
    - Circuit breaker: protege œÉ (estrutura)
    - Phi monitoring: detecta degrada√ß√£o de Œ¶ (integra√ß√£o)
    """

    def __init__(self, config: Optional[ConnectionConfig] = None, workspace: Optional[Any] = None):
        """Inicializa o handler de conex√£o com monitoramento de Œ¶.

        Args:
            config: Configura√ß√£o personalizada (usa defaults se None)
            workspace: SharedWorkspace para monitorar Œ¶ durante opera√ß√µes
        """
        self.config = config or ConnectionConfig()
        self.config.validate()

        # NOVO: Integra√ß√£o SharedWorkspace para monitoramento Œ¶
        self.workspace = workspace
        self.phi_monitoring_enabled = workspace is not None

        # Estado do circuit breaker por servidor
        self._failure_counts: Dict[str, int] = {}
        self._last_failure_time: Dict[str, float] = {}
        self._circuit_open: Dict[str, bool] = {}

        # NOVO: M√©tricas de consci√™ncia durante opera√ß√µes
        self._phi_during_operations: Dict[str, Dict[str, Any]] = {}

        logger.info(
            f"MCPConnectionHandler inicializado: "
            f"timeouts={self.config.request_timeout}s, "
            f"max_retries={self.config.max_retries}, "
            f"circuit_threshold={self.config.failure_threshold}, "
            f"phi_monitoring={self.phi_monitoring_enabled}"
        )

    # ========== M√âTODOS AUXILIARES PARA MONITORAMENTO Œ¶ ==========

    def _measure_phi_before_operation(self, operation_id: str) -> float:
        """Mede Œ¶ antes de uma opera√ß√£o MCP para baseline.

        Args:
            operation_id: Identificador √∫nico da opera√ß√£o

        Returns:
            Œ¶ atual antes da opera√ß√£o (baseline)
        """
        if not self.phi_monitoring_enabled or not self.workspace:
            return 0.0

        try:
            # Obter m√©tricas de consci√™ncia do SharedWorkspace
            if hasattr(self.workspace, "get_current_phi"):
                phi = self.workspace.get_current_phi()
            elif hasattr(self.workspace, "phi"):
                phi = getattr(self.workspace, "phi", 0.0)
            else:
                phi = 0.0

            self._phi_during_operations[operation_id] = {
                "baseline_phi": phi,
                "timestamp": time.time(),
            }

            logger.debug(f"Œ¶ baseline medido para opera√ß√£o {operation_id}: {phi:.4f}")
            return phi

        except Exception as e:
            logger.warning(f"Falha ao medir Œ¶ baseline para opera√ß√£o {operation_id}: {e}")
            return 0.0

    def _measure_phi_after_operation(
        self, operation_id: str, operation_success: bool
    ) -> Dict[str, float]:
        """Mede Œ¶ ap√≥s opera√ß√£o e calcula m√©tricas de preserva√ß√£o.

        Args:
            operation_id: Identificador da opera√ß√£o
            operation_success: Se a opera√ß√£o foi bem-sucedida

        Returns:
            Dict com m√©tricas de Œ¶: baseline, final, preserved_percentage
        """
        if not self.phi_monitoring_enabled or not self.workspace:
            return {"baseline_phi": 0.0, "final_phi": 0.0, "preserved_percentage": 100.0}

        try:
            # Medir Œ¶ final
            if hasattr(self.workspace, "get_current_phi"):
                final_phi = self.workspace.get_current_phi()
            elif hasattr(self.workspace, "phi"):
                final_phi = getattr(self.workspace, "phi", 0.0)
            else:
                final_phi = 0.0

            # Obter baseline
            baseline_data: Dict[str, Any] = self._phi_during_operations.get(operation_id, {})
            baseline_phi = baseline_data.get("baseline_phi", 0.0)

            # Calcular preserva√ß√£o
            if baseline_phi > 0:
                preserved_percentage = (final_phi / baseline_phi) * 100
            else:
                preserved_percentage = 100.0 if final_phi > 0 else 100.0

            metrics = {
                "baseline_phi": baseline_phi,
                "final_phi": final_phi,
                "preserved_percentage": preserved_percentage,
                "operation_success": operation_success,
            }

            # Log de preserva√ß√£o de Œ¶
            if preserved_percentage >= 95:
                status = "‚úÖ"
            elif preserved_percentage >= 80:
                status = "‚ö†Ô∏è"
            else:
                status = "‚ùå"

            logger.info(
                f"{status} Œ¶ preservado na opera√ß√£o {operation_id}: "
                f"baseline={baseline_phi:.4f}, final={final_phi:.4f}, "
                f"preservado={preserved_percentage:.1f}%"
            )

            # Alerta se Œ¶ degradou significativamente
            if preserved_percentage < 80:
                logger.warning(
                    f"üö® DEGRADA√á√ÉO Œ¶ detectada na opera√ß√£o {operation_id}: "
                    f"preserva√ß√£o={preserved_percentage:.1f}% < 80%"
                )

            return metrics

        except Exception as e:
            logger.warning(f"Falha ao medir Œ¶ final para opera√ß√£o {operation_id}: {e}")
            return {"baseline_phi": 0.0, "final_phi": 0.0, "preserved_percentage": 100.0}

    def _check_phi_degradation_alert(self, operation_id: str, metrics: Dict[str, float]) -> bool:
        """Verifica se h√° degrada√ß√£o cr√≠tica de Œ¶ e dispara alertas.

        Args:
            operation_id: Identificador da opera√ß√£o
            metrics: M√©tricas de Œ¶ calculadas

        Returns:
            True se h√° degrada√ß√£o cr√≠tica (Œ¶ < threshold)
        """
        preserved_percentage = metrics.get("preserved_percentage", 100.0)
        final_phi = metrics.get("final_phi", 0.0)

        # Thresholds cr√≠ticos baseados nos par√¢metros emp√≠ricos
        critical_preservation_threshold = 80.0  # 80% de preserva√ß√£o m√≠nima
        critical_phi_threshold = 0.03  # Œ¶ < 0.03 = degrada√ß√£o cr√≠tica

        is_critical_degradation = (
            preserved_percentage < critical_preservation_threshold
            or final_phi < critical_phi_threshold
        )

        if is_critical_degradation:
            logger.error(
                f"üö® DEGRADA√á√ÉO CR√çTICA Œ¶ detectada! Opera√ß√£o {operation_id}: "
                f"Œ¶ final={final_phi:.4f}, preserva√ß√£o={preserved_percentage:.1f}%"
            )

            # Aqui poderiam ser disparados alertas autom√°ticos:
            # - Notifica√ß√£o para humanos se Œ¶ < 0.15
            # - Escalonamento autom√°tico
            # - Redu√ß√£o de carga do sistema

        return is_critical_degradation

    def should_retry(self, server_name: str, exception: Exception) -> tuple[bool, float]:
        """Determina se deve retry baseado no erro e estado do circuito.

        Args:
            server_name: Nome do servidor MCP
            exception: Exce√ß√£o ocorrida

        Returns:
            Tuple (should_retry, backoff_time)
        """
        # Circuit breaker logic
        if self._is_circuit_open(server_name):
            return False, 0.0

        # Broken pipe (errno 32) - sempre retry com backoff
        if isinstance(exception, MCPPipeError) or (
            hasattr(exception, "errno") and exception.errno == errno.EPIPE
        ):
            backoff = self._calculate_backoff(server_name)
            logger.warning(
                f"MCP Broken pipe detectado para {server_name}, retrying em {backoff:.1f}s"
            )
            return True, backoff

        # Timeout errors
        if "timeout" in str(exception).lower():
            backoff = self._calculate_backoff(server_name)
            logger.warning(f"MCP timeout para {server_name}, retrying em {backoff:.1f}s")
            return True, backoff

        # Connection errors
        if any(word in str(exception).lower() for word in ["connection", "connect", "refused"]):
            backoff = self._calculate_backoff(server_name)
            logger.warning(f"MCP connection error para {server_name}, retrying em {backoff:.1f}s")
            return True, backoff

        # Non-retryable errors
        logger.error(f"MCP non-retryable error para {server_name}: {exception}")
        self._record_failure(server_name)
        return False, 0.0

    def _calculate_backoff(self, server_name: str) -> float:
        """Calcula tempo de backoff exponencial.

        Args:
            server_name: Nome do servidor

        Returns:
            Tempo de espera em segundos
        """
        failures = self._failure_counts.get(server_name, 0)
        base_backoff = self.config.retry_backoff_base * (2**failures)
        return min(base_backoff, self.config.retry_backoff_max)

    def _is_circuit_open(self, server_name: str) -> bool:
        """Verifica se o circuito est√° aberto para o servidor.

        Args:
            server_name: Nome do servidor

        Returns:
            True se circuito est√° aberto
        """
        if not self._circuit_open.get(server_name, False):
            return False

        # Verificar se pode tentar recovery
        last_failure = self._last_failure_time.get(server_name, 0)
        if time.time() - last_failure >= self.config.recovery_timeout:
            logger.info(f"Circuit breaker recovery attempt para {server_name}")
            self._circuit_open[server_name] = False
            return False

        return True

    def _record_failure(self, server_name: str) -> None:
        """Registra falha e possibly abre o circuito.

        Args:
            server_name: Nome do servidor
        """
        current_time = time.time()
        self._failure_counts[server_name] = self._failure_counts.get(server_name, 0) + 1
        self._last_failure_time[server_name] = current_time

        # Abrir circuito se excedeu threshold
        if self._failure_counts[server_name] >= self.config.failure_threshold:
            self._circuit_open[server_name] = True
            logger.error(
                f"Circuit breaker aberto para {server_name} ap√≥s "
                f"{self._failure_counts[server_name]} falhas"
            )

    def record_success(self, server_name: str) -> None:
        """Registra sucesso e reseta contadores.

        Args:
            server_name: Nome do servidor
        """
        self._failure_counts[server_name] = 0
        self._last_failure_time.pop(server_name, None)
        self._circuit_open[server_name] = False
        logger.debug(f"Circuit breaker reset para {server_name}")

    def get_connection_params(self, server_name: str) -> Dict[str, Any]:
        """Retorna par√¢metros otimizados para conex√£o.

        Args:
            server_name: Nome do servidor

        Returns:
            Dict com par√¢metros de conex√£o
        """
        return {
            "timeout": self.config.request_timeout,
            "connection_timeout": self.config.connection_timeout,
            "read_timeout": self.config.read_timeout,
            "max_connections": self.config.max_connections,
            "max_keepalive_connections": self.config.max_keepalive_connections,
            "keepalive_expiry": self.config.keepalive_expiry,
        }

    def get_status(self, server_name: str) -> Dict[str, Any]:
        """Retorna status do circuito e m√©tricas de Œ¶ para o servidor.

        Args:
            server_name: Nome do servidor

        Returns:
            Dict com status do circuito e m√©tricas de Œ¶
        """
        status: Dict[str, Any] = {
            "failure_count": self._failure_counts.get(server_name, 0),
            "last_failure_time": self._last_failure_time.get(server_name),
            "circuit_open": self._is_circuit_open(server_name),
            "can_retry": not self._is_circuit_open(server_name),
        }

        # NOVO: Adicionar m√©tricas de Œ¶ se dispon√≠veis
        if self.phi_monitoring_enabled and self.workspace:
            try:
                current_phi = 0.0
                if hasattr(self.workspace, "get_current_phi"):
                    current_phi = self.workspace.get_current_phi()
                elif hasattr(self.workspace, "phi"):
                    current_phi = getattr(self.workspace, "phi", 0.0)

                status.update(
                    {
                        "current_phi": current_phi,
                        "phi_monitoring_enabled": True,
                        "phi_status": "healthy" if current_phi >= 0.03 else "degraded",
                    }
                )
            except Exception as e:
                logger.warning(f"Falha ao obter Œ¶ para status do servidor {server_name}: {e}")
                status.update(
                    {
                        "current_phi": 0.0,
                        "phi_monitoring_enabled": True,
                        "phi_status": "error",
                    }
                )
        else:
            status.update({"phi_monitoring_enabled": False, "phi_status": "disabled"})
            
        # Cast para resolver type checking issues
        status = cast(Dict[str, Any], status)

        return status


class RobustMCPClient:
    """Cliente MCP com tratamento robusto de erros de conex√£o."""

    def __init__(
        self,
        endpoint: str,
        connection_handler: Optional[MCPConnectionHandler] = None,
        config: Optional[ConnectionConfig] = None,
        workspace: Optional[Any] = None,
    ):
        """Inicializa cliente MCP robusto com monitoramento de Œ¶.

        Args:
            endpoint: URL do endpoint MCP
            connection_handler: Handler de conex√£o personalizado
            config: Configura√ß√£o de conex√£o
            workspace: SharedWorkspace para monitorar Œ¶ durante opera√ß√µes
        """
        self.endpoint = endpoint
        # NOVO: Passar workspace para o connection handler
        if connection_handler:
            self.connection_handler = connection_handler
        else:
            self.connection_handler = MCPConnectionHandler(config, workspace)
        self.server_name = endpoint.split("/")[-2] if "/" in endpoint else endpoint

    async def request_with_retry(
        self,
        method: str,
        params: Dict[str, Any],
        max_attempts: Optional[int] = None,
    ) -> Any:
        """Executa request com retry autom√°tico e monitoramento de Œ¶.

        Args:
            method: M√©todo MCP
            params: Par√¢metros do m√©todo
            max_attempts: M√°ximo de tentativas (usa config se None)

        Returns:
            Resultado do request

        Raises:
            MCPPipeError: Se n√£o conseguir conectar ap√≥s todas as tentativas
        """
        max_attempts = max_attempts or self.connection_handler.config.max_retries
        operation_id = f"{self.server_name}_{method}_{int(time.time())}"

        # NOVO: Medir Œ¶ antes da opera√ß√£o
        self.connection_handler._measure_phi_before_operation(operation_id)

        last_exception: Optional[Exception] = None

        for attempt in range(max_attempts):
            try:
                # Verificar se deve tentar
                if last_exception:
                    should_retry, backoff_time = self.connection_handler.should_retry(
                        self.server_name, last_exception
                    )
                    if not should_retry:
                        # NOVO: Medir Œ¶ antes de levantar a exce√ß√£o final
                        self.connection_handler._measure_phi_after_operation(operation_id, False)
                        raise last_exception

                    if attempt > 0:  # N√£o fazer sleep na primeira tentativa
                        import asyncio

                        await asyncio.sleep(backoff_time)

                # Executar request
                result = await self._execute_request(method, params)

                # Registrar sucesso
                self.connection_handler.record_success(self.server_name)

                # NOVO: Medir Œ¶ ap√≥s opera√ß√£o bem-sucedida
                self.connection_handler._measure_phi_after_operation(operation_id, True)

                return result

            except Exception as e:
                last_exception = e
                logger.warning(f"MCP request attempt {attempt + 1}/{max_attempts} failed: {e}")

                # Se √© √∫ltima tentativa, levantar erro final
                if attempt == max_attempts - 1:
                    # NOVO: Medir Œ¶ antes de levantar a exce√ß√£o final
                    phi_metrics = self.connection_handler._measure_phi_after_operation(
                        operation_id, False
                    )

                    # Verificar degrada√ß√£o cr√≠tica de Œ¶
                    self.connection_handler._check_phi_degradation_alert(operation_id, phi_metrics)

                    # Converter para MCPPipeError se apropriado
                    if isinstance(e, OSError) and hasattr(e, "errno") and e.errno == errno.EPIPE:
                        raise MCPPipeError(
                            f"MCP Broken pipe ap√≥s {max_attempts} tentativas: {e}", e.errno
                        ) from e
                    else:
                        raise

        # N√£o deveria chegar aqui, mas por seguran√ßa
        # NOVO: Medir Œ¶ antes de levantar a exce√ß√£o final
        self.connection_handler._measure_phi_after_operation(operation_id, False)
        raise MCPPipeError(f"MCP request failed ap√≥s {max_attempts} tentativas")

    async def _execute_request(self, method: str, params: Dict[str, Any]) -> Any:
        """Executa request individual (implementa√ß√£o espec√≠fica)."""
        # Esta √© uma implementa√ß√£o base - subclasses devem sobrescrever
        import httpx

        connection_params = self.connection_handler.get_connection_params(self.server_name)

        async with httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=connection_params["connection_timeout"],
                read=connection_params["read_timeout"],
                write=connection_params["read_timeout"],
            ),
            limits=httpx.Limits(
                max_connections=connection_params["max_connections"],
                max_keepalive_connections=connection_params["max_keepalive_connections"],
            ),
        ) as client:
            payload = {
                "jsonrpc": "2.0",
                "method": method,
                "params": params,
                "id": "robust_client",
            }

            response = await client.post(self.endpoint, json=payload)
            response.raise_for_status()

            result = response.json()

            # Validar resposta
            if "error" in result:
                raise Exception(f"MCP server error: {result['error']}")

            return result.get("result")

    def get_health_status(self) -> Dict[str, Any]:
        """Retorna status de sa√∫de do cliente.

        Returns:
            Dict com status de sa√∫de
        """
        return {
            "endpoint": self.endpoint,
            "server_name": self.server_name,
            "connection_status": self.connection_handler.get_status(self.server_name),
            "config": {
                "request_timeout": self.connection_handler.config.request_timeout,
                "max_retries": self.connection_handler.config.max_retries,
                "failure_threshold": self.connection_handler.config.failure_threshold,
            },
        }
