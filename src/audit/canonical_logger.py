#!/usr/bin/env python3
"""
OmniMind Canonical Action Logger
Sistema para registro automÃ¡tico de aÃ§Ãµes das AIs com integridade garantida.
"""

import hashlib
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class CanonicalLogger:
    """Logger canÃ´nico para aÃ§Ãµes das AIs com hash chain para integridade."""

    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.canonical_dir = base_dir / ".omnimind" / "canonical"
        self.canonical_dir.mkdir(parents=True, exist_ok=True)

        self.md_file = self.canonical_dir / "action_log.md"
        self.json_file = self.canonical_dir / "action_log.json"

        # Initialize if files don't exist
        if not self.json_file.exists():
            self._initialize_files()

    def _initialize_files(self):
        """Initialize canonical log files."""
        initial_data = {
            "metadata": {
                "version": "1.0.0",
                "created_at": datetime.now().isoformat(),
                "responsible": "OmniMind System",
                "purpose": "Canonical action logging system",
                "integrity_rules": [
                    "Records are immutable",
                    "SHA-256 hash chain",
                    "Automatic validation",
                    "Audit trail"
                ]
            },
            "current_metrics": {
                "total_files": 0,
                "tests_passing": "0/0",
                "qdrant_collections": {"local": 0, "cloud": 0},
                "knowledge_points": 0,
                "canonical_documents": 1
            },
            "action_log": [],
            "pending_automatic_actions": [],
            "system_integrity": {
                "last_validation": datetime.now().isoformat(),
                "hash_chain_valid": True,
                "total_records": 0,
                "corruption_detected": False
            }
        }

        with open(self.json_file, 'w', encoding='utf-8') as f:
            json.dump(initial_data, f, indent=2, ensure_ascii=False)

        # Create MD file
        md_content = f"""# ðŸ“‹ OMNIMIND CANONICAL ACTION LOG
# Sistema CanÃ´nico de Registro de AÃ§Ãµes - VersÃ£o 1.0

## ðŸ“Š METADADOS GERAIS
- **VersÃ£o**: 1.0.0
- **Data CriaÃ§Ã£o**: {datetime.now().strftime('%Y-%m-%d')}
- **ResponsÃ¡vel**: OmniMind System

## ðŸ” REGRAS DE INTEGRIDADE
1. **Imutabilidade**: Registros nunca sÃ£o alterados
2. **Hash Chain**: SHA-256 para integridade
3. **ValidaÃ§Ã£o**: Commits verificam integridade
4. **Auditoria**: VerificaÃ§Ã£o automÃ¡tica

## ðŸ“ REGISTROS DE AÃ‡ÃƒO

---
*Arquivo gerado automaticamente - NÃ£o editar manualmente*
"""
        with open(self.md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)

    def log_action(self, ai_agent: str, action_type: str, target: str,
                   result: str, description: str, details: str = "",
                   impact: str = "", automatic_actions: List[str] = None) -> str:
        """Log an action with integrity hash."""

        # Load current data
        with open(self.json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Create new record
        timestamp = datetime.now().isoformat()
        record_content = f"{timestamp}{ai_agent}{action_type}{target}{result}{description}"
        
        # Calculate hash
        if data["action_log"]:
            prev_hash = data["action_log"][-1]["hash"]
        else:
            prev_hash = "0000000000000000000000000000000000000000000000000000000000000000"
        
        content_hash = hashlib.sha256((prev_hash + record_content).encode()).hexdigest()

        new_record = {
            "timestamp": timestamp,
            "ai_agent": ai_agent,
            "action_type": action_type,
            "target": target,
            "result": result,
            "hash": content_hash,
            "description": description,
            "details": details,
            "impact": impact,
            "automatic_actions": automatic_actions or []
        }

        # Add to JSON
        data["action_log"].append(new_record)
        data["system_integrity"]["total_records"] = len(data["action_log"])
        data["system_integrity"]["last_validation"] = timestamp

        with open(self.json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        # Update MD file
        self._update_md_file(new_record)

        logger.info(f"Action logged: {ai_agent} {action_type} {target} -> {result}")
        return content_hash

    def _update_md_file(self, record: Dict[str, Any]):
        """Update the MD file with new record."""
        md_entry = f"""
### [{record['timestamp'][:19]}] {record['ai_agent']} {record['action_type']} {record['target']} {record['result']} {record['hash'][:16]}...
**DescriÃ§Ã£o**: {record['description']}
**Detalhes**: {record['details']}
**Impacto**: {record['impact']}
**AÃ§Ãµes AutomÃ¡ticas**: {', '.join(record['automatic_actions'])}
"""

        # Read current content
        with open(self.md_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # Insert before the footer
        footer_pos = content.find("---\n*Arquivo gerado automaticamente")
        if footer_pos > 0:
            new_content = content[:footer_pos] + md_entry + "\n" + content[footer_pos:]
        else:
            new_content = content + md_entry

        with open(self.md_file, 'w', encoding='utf-8') as f:
            f.write(new_content)

    def validate_integrity(self) -> bool:
        """Validate the hash chain integrity."""
        with open(self.json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        expected_hash = "0000000000000000000000000000000000000000000000000000000000000000"

        for record in data["action_log"]:
            record_content = f"{record['timestamp']}{record['ai_agent']}{record['action_type']}{record['target']}{record['result']}{record['description']}"
            calculated_hash = hashlib.sha256((expected_hash + record_content).encode()).hexdigest()

            if calculated_hash != record["hash"]:
                logger.error(f"Hash chain broken at record: {record['timestamp']}")
                return False

            expected_hash = record["hash"]

        return True

    def get_metrics(self) -> Dict[str, Any]:
        """Get current system metrics."""
        with open(self.json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data["current_metrics"]

    def update_metrics(self, metrics: Dict[str, Any]):
        """Update system metrics."""
        with open(self.json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        data["current_metrics"].update(metrics)

        with open(self.json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)


# Global instance
canonical_logger = CanonicalLogger(Path.cwd())