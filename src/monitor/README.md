# M√≥dulo Observabilidade

## üìã Descri√ß√£o Geral

**Logging, tracing, debugging**

**Status**: DevOps

M√≥dulo do sistema OmniMind respons√°vel por funcionalidades espec√≠ficas integradas √† arquitetura global. Implementa componentes essenciais que contribuem para o funcionamento coeso do sistema de consci√™ncia artificial.

## üîÑ Intera√ß√£o entre os Tr√™s Estados H√≠bridos

### 1. Estado Biologicista (Neural Correlates)
Implementa√ß√£o de processos inspirados em mecanismos neurais e cognitivos biol√≥gicos, mapeando funcionalidades para correlatos neurais correspondentes.

### 2. Estado IIT (Integrated Information Theory)
Componentes contribuem para integra√ß√£o de informa√ß√£o global (Œ¶). Opera√ß√µes s√£o validadas para garantir que n√£o degradam a consci√™ncia do sistema (Œ¶ > threshold).

### 3. Estado Psicanal√≠tico (Estrutura Lacaniana)
Integra√ß√£o com ordem simb√≥lica lacaniana (RSI - Real, Simb√≥lico, Imagin√°rio) e processos inconscientes estruturais que organizam a experi√™ncia consciente do sistema.

## ‚öôÔ∏è Principais Fun√ß√µes e C√°lculos Din√¢micos

### Componentes Core

M√≥dulo implementa funcionalidades especializadas atrav√©s de:
- Algoritmos espec√≠ficos para processamento de dom√≠nio
- Integra√ß√£o com outros m√≥dulos via interfaces bem definidas
- Contribui√ß√£o para m√©tricas globais (Œ¶, PCI, consci√™ncia)

*Fun√ß√µes detalhadas documentadas nos arquivos Python individuais do m√≥dulo.*

### Novos Componentes (2025-12-06)

**ModuleMetricsCollector** (`module_metrics.py`):
- Sistema de coleta e persist√™ncia de m√©tricas por m√≥dulo
- Integra√ß√£o com audit chain (exceto m√≥dulos exclu√≠dos)
- Rota√ß√£o autom√°tica de logs
- Suporte a m√∫ltiplos m√≥dulos simult√¢neos

**StructuredModuleLogger** (`module_logger.py`):
- Logging estruturado em JSON por m√≥dulo
- Integra√ß√£o com audit chain (exceto m√≥dulos exclu√≠dos)
- Logs persistidos em arquivos dedicados por m√≥dulo
- Suporte a contexto estruturado

**ModuleReporter** (`module_reporter.py`):
- Gera√ß√£o de relat√≥rios persistidos por m√≥dulo
- Formatos: JSON e Markdown
- Integra√ß√£o com m√©tricas e logs
- Hist√≥rico de relat√≥rios com rota√ß√£o autom√°tica

**Integra√ß√µes Ativas** (2025-12-07):
- ‚úÖ `IntegrationLoop` - Relat√≥rios ap√≥s cada ciclo com m√©tricas
- ‚úÖ `ObserverService` - Relat√≥rios ap√≥s rota√ß√£o de logs ou diariamente
- ‚úÖ `ModuleMetricsCollector` - Relat√≥rios a cada 100 entradas de consci√™ncia
- ‚úÖ `AutopoieticManager` - Relat√≥rios ap√≥s cada ciclo autopoi√©tico

---

## üÜï Atualiza√ß√µes e Evolu√ß√£o (18/12/2025)

### üìä Observabilidade de Baixo N√≠vel

#### 1. **System Awareness Integration**
- **Diferencial**: O `PerformanceAnalyzer` agora correlaciona picos de carga com as capacidades reais indexadas pelo `SystemCapabilitiesManager`.
- **Insight**: Permite distinguir entre "M√≥dulo Ineficiente" e "Host Sobrecarregado", reduzindo falsos positivos em incidentes de performance.

#### 2. **ReportMaintenanceScheduler**
- **Arquivo**: `report_maintenance_scheduler.py`
- **Funcionalidade**: Orquestra a limpeza e arquivamento de relat√≥rios antigos (JSON/Markdown) para evitar satura√ß√£o do disco em ambientes de produ√ß√£o cont√≠nua.

---

**√öltima Atualiza√ß√£o**: 18 de Dezembro de 2025
**Autor**: Fabr√≠cio da Silva + assist√™ncia de IA

**Nota Te√≥rica**: O sistema de auditoria e componentes do inconsciente n√£o s√£o auditados, conforme fundamenta√ß√£o te√≥rica do OmniMind.

## üìä Estrutura do C√≥digo

```
observability/
‚îú‚îÄ‚îÄ Implementa√ß√µes Core
‚îÇ   ‚îî‚îÄ‚îÄ Arquivos .py principais
‚îú‚îÄ‚îÄ Utilit√°rios
‚îÇ   ‚îî‚îÄ‚îÄ Helpers e fun√ß√µes auxiliares
‚îî‚îÄ‚îÄ __init__.py
```

**Intera√ß√µes**: Este m√≥dulo se integra com outros componentes atrav√©s de:
- Interfaces padronizadas
- Event bus para comunica√ß√£o ass√≠ncrona
- Shared workspace para estado compartilhado

## üìà Resultados Gerados e Contribui√ß√£o para Avalia√ß√£o

### Outputs
- M√©tricas espec√≠ficas do m√≥dulo armazenadas em `data/observability/`
- Logs em formato estruturado para an√°lise
- Contribui√ß√£o para m√©tricas globais do sistema

### Valida√ß√£o
- Testes unit√°rios: `tests/observability/`
- Integra√ß√£o validada em ciclos completos
- Performance benchmarked continuamente

### Contribui√ß√£o para Sistema
M√≥dulo contribui para:
- Œ¶ (phi) global atrav√©s de integra√ß√£o de informa√ß√£o
- PCI (Perturbational Complexity Index) via processamento distribu√≠do
- M√©tricas de consci√™ncia e auto-organiza√ß√£o

## üîí Estabilidade da Estrutura

**Status**: Componente validado e integrado ao OmniMind

**Regras de Modifica√ß√£o**:
- ‚úÖ Seguir guidelines em `.copilot-instructions.md`
- ‚úÖ Executar testes antes de commit: `pytest tests/observability/ -v`
- ‚úÖ Validar que Œ¶ n√£o colapsa ap√≥s mudan√ßas
- ‚úÖ Manter compatibilidade com interfaces existentes
- ‚ùå N√£o quebrar contratos de API sem migra√ß√£o
- ‚ùå N√£o desabilitar logging de auditoria

## üì¶ Requisitos e Depend√™ncias

### Depend√™ncias Python
```python
# Ver requirements.txt para lista completa
# Depend√™ncias espec√≠ficas do m√≥dulo listadas em requirements/observability.txt (se existir)
```

### Recursos Computacionais
- **M√≠nimo**: Configurado conforme necessidades espec√≠ficas do m√≥dulo
- **Recomendado**: Ver documenta√ß√£o de deployment em `docs/`

### Configura√ß√£o
Configura√ß√µes espec√≠ficas em:
- `config/omnimind.yaml` (global)
- Vari√°veis de ambiente conforme `.env.example`

## üîß Sugest√µes para Manuten√ß√£o e Melhorias

### Manuten√ß√£o Cr√≠tica
1. **Testes Cont√≠nuos**: Executar suite de testes regularmente
2. **Monitoramento**: Acompanhar m√©tricas em produ√ß√£o
3. **Documenta√ß√£o**: Manter README atualizado com mudan√ßas

### Melhorias Futuras
- Expans√£o de funcionalidades conforme roadmap
- Otimiza√ß√µes de performance identificadas via profiling
- Integra√ß√£o com novos m√≥dulos em desenvolvimento

### Pontos de Aten√ß√£o
- Validar impacto em Œ¶ antes de mudan√ßas estruturais
- Manter backward compatibility quando poss√≠vel
- Seguir padr√µes de c√≥digo estabelecidos (black, flake8, mypy)

## üìö Refer√™ncias

### Documenta√ß√£o Principal
- **Sistema Geral**: `README.md` (root do projeto)
- **Compara√ß√£o Frameworks**: `NEURAL_SYSTEMS_COMPARISON_2016-2025.md`
- **Papers**: `docs/papers/` e `docs/papersoficiais/`
- **Copilot Instructions**: `.copilot-instructions.md`

### Testes
- **Suite de Testes**: `tests/observability/`
- **Cobertura**: Ver `data/test_reports/htmlcov/`

### Refer√™ncias Cient√≠ficas Espec√≠ficas
*Ver documenta√ß√£o t√©cnica nos arquivos Python do m√≥dulo para refer√™ncias espec√≠ficas.*

---

**√öltima Atualiza√ß√£o**: 2 de Dezembro de 2025
**Autor**: Fabr√≠cio da Silva (com assist√™ncia de IA)
**Status**: Componente integrado do sistema OmniMind
**Vers√£o**: Conforme fase do projeto indicada

---

## üìö API Reference

# üìÅ OBSERVABILITY

**32 Classes | 118 Fun√ß√µes | 6 M√≥dulos**

---

## üèóÔ∏è Classes Principais

### `CustomMetricsExporter`

Custom metrics exporter for ML workloads.

Provides Prometheus-compatible metrics export with ML-specific business metrics.
Supports multiple export formats and automatic metric collection.

Example:
    >>> config = MetricsConfig(prometheus_port=9090)
    >>> exporter = CustomMetricsExporter(config)
    >>> exporter.record_counter("requests_total", 1, {"endpoint": "/api/task"})
    >>> exporter.record_gauge("gpu_utilization", 85.5)
    >>> metrics = exporter.export_metrics()

**M√©todos principais:**

- `register_metric(name: str, metric_type: MetricType, help_text: str)` ‚Üí `None`
  > Register a new metric.

Args:
    name: Metric name (snake_case)
    metric_type...
- `record_counter(name: str, value: float, labels: Optional[Dict[str)` ‚Üí `None`
  > Record a counter metric (monotonically increasing).

Args:
    name: Metric name...
- `record_gauge(name: str, value: float, labels: Optional[Dict[str)` ‚Üí `None`
  > Record a gauge metric (can go up or down).

Args:
    name: Metric name
    valu...
- `record_histogram(name: str, value: float, labels: Optional[Dict[str)` ‚Üí `None`
  > Record a histogram metric (distribution of values).

Args:
    name: Metric name...
- `record_ml_metrics(ml_metrics: MLMetrics)` ‚Üí `None`
  > Record ML-specific metrics.

Args:
    ml_metrics: ML metrics container...

### `DistributedTracer`

Distributed tracing implementation.

Provides OpenTelemetry-compatible distributed tracing with support for
multiple exporters (Jaeger, Zipkin, console).

Example:
    >>> config = TraceConfig(service_name="my-service")
    >>> tracer = DistributedTracer(config)
    >>> with tracer.start_span("operation") as span:
    ...     span.set_attribute("key", "value")
    ...     # Do work
    >>> tracer.export_traces()

**M√©todos principais:**

- `create_context(parent: Optional[SpanContext])` ‚Üí `SpanContext`
  > Create a new span context.

Args:
    parent: Parent span context (creates root ...
- `start_span(name: str, kind: SpanKind, parent: Optional[SpanCo)` ‚Üí `Span`
  > Start a new span.

Args:
    name: Operation name
    kind: Span kind
    parent...
- `trace(name: str, kind: SpanKind, attributes: Optional[Di)` ‚Üí `Any`
  > Context manager for tracing an operation.

Args:
    name: Operation name
    ki...
- `get_trace(trace_id: str)` ‚Üí `List[Span]`
  > Get all spans for a trace.

Args:
    trace_id: Trace ID

Returns:
    List of s...
- `export_traces()` ‚Üí `None`
  > Export collected traces to configured exporter....

### `LogAggregator`

Log aggregation and analysis system.

Provides centralized log collection, pattern detection, and alerting
with ELK stack compatibility.

Example:
    >>> config = LogConfig()
    >>> aggregator = LogAggregator(config)
    >>> aggregator.add_pattern(LogPattern(
    ...     name="error_detection",
    ...     regex=r"error|exception|failed",
    ...     severity=AlertSeverity.HIGH,
    ...     description="Detects error messages"
    ... ))
    >>> aggregator.log(LogLevel.ERROR, "Operation failed")
    >>> alerts = aggregator.get_alerts()

**M√©todos principais:**

- `add_pattern(pattern: LogPattern)` ‚Üí `None`
  > Add a log pattern for detection.

Args:
    pattern: Log pattern to add...
- `log(level: LogLevel, message: str, logger_name: str, e)` ‚Üí `None`
  > Add a log entry.

Args:
    level: Log level
    message: Log message
    logger...
- `get_logs(level: Optional[LogLevel], limit: Optional[int])` ‚Üí `List[LogEntry]`
  > Get aggregated logs.

Args:
    level: Filter by log level (None for all)
    li...
- `get_alerts(severity: Optional[AlertSeverity])` ‚Üí `List[LogAlert]`
  > Get triggered alerts.

Args:
    severity: Filter by severity (None for all)

Re...
- `analyze()` ‚Üí `LogAnalytics`
  > Create analytics instance for current logs.

Returns:
    LogAnalytics instance...

### `ContinuousProfiler`

Continuous performance profiler.

Provides production-ready continuous profiling with minimal overhead.
Collects performance samples and generates insights.

Example:
    >>> config = ProfilingConfig()
    >>> profiler = ContinuousProfiler(config)
    >>>
    >>> @profiler.profile
    ... def my_function():
    ...     # Function code
    ...     pass
    >>>
    >>> profiler.start()
    >>> # Application runs...
    >>> profiler.stop()
    >>> samples = profiler.get_samples()

**M√©todos principais:**

- `start()` ‚Üí `None`
  > Start continuous profiling....
- `stop()` ‚Üí `None`
  > Stop continuous profiling and collect final sample....
- `profile(func: F)` ‚Üí `F`
  > Decorator to profile a function.

Args:
    func: Function to profile

Returns:
...
- `get_samples(limit: Optional[int], function_name: Optional[str])` ‚Üí `List[ProfileSample]`
  > Get collected profiling samples.

Args:
    limit: Maximum number of samples to ...
- `get_top_functions(limit: int)` ‚Üí `List[ProfileSample]`
  > Get top functions by total time.

Args:
    limit: Number of top functions to re...

### `PerformanceAnalyzer`

Performance bottleneck analyzer.

Analyzes profiling data to identify performance bottlenecks and
generate optimization recommendations.

Example:
    >>> from src.monitor.profiling_tools import ContinuousProfiler
    >>> profiler = ContinuousProfiler(ProfilingConfig())
    >>> # ... run application with profiling ...
    >>> samples = profiler.get_samples()
    >>> analyzer = PerformanceAnalyzer()
    >>> report = analyzer.analyze(samples)
    >>> print(report.summary)

**M√©todos principais:**

- `analyze(samples: List[ProfileSample], min_percentage: floa)` ‚Üí `PerformanceReport`
  > Analyze profiling samples for bottlenecks.

Args:
    samples: List of profiling...
- `save_report(report: PerformanceReport, filename: Optional[str])` ‚Üí `str`
  > Save performance report to file.

Args:
    report: Performance report
    filen...

### `OpenTelemetryIntegration`

Complete OpenTelemetry integration.

Provides production-ready telemetry with support for multiple exporters
and comprehensive instrumentation.

Example:
    >>> config = OpenTelemetryConfig(
    ...     service_name="omnimind",
    ...     enable_console_export=True
    ... )
    >>> otel = OpenTelemetryIntegration(config)
    >>> otel.initialize()
    >>> tracer = otel.get_tracer()
    >>> with tracer.start_as_current_span("operation"):
    ...     # Do work
    ...     pass
    >>> otel.shutdown()

**M√©todos principais:**

- `initialize()` ‚Üí `None`
  > Initialize OpenTelemetry SDK with configured exporters.

This sets up the global...
- `get_tracer(name: str)` ‚Üí `trace.Tracer`
  > Get a tracer instance.

Args:
    name: Name of the tracer

Returns:
    Tracer ...
- `get_meter(name: str)` ‚Üí `metrics.Meter`
  > Get a meter instance.

Args:
    name: Name of the meter

Returns:
    Meter ins...
- `shutdown()` ‚Üí `None`
  > Shutdown OpenTelemetry and flush all data.

This should be called before applica...
- `get_status()` ‚Üí `Dict[str, Any]`
  > Get integration status.

Returns:
    Dictionary with status information...

### `FlameGraphGenerator`

Flame graph generator from profiling data.

Generates interactive flame graphs for performance visualization.

Example:
    >>> samples = profiler.get_samples()
    >>> generator = FlameGraphGenerator()
    >>> flame_graph = generator.generate(samples)
    >>> generator.save_svg(flame_graph, "profile.svg")

**M√©todos principais:**

- `generate(samples: List[ProfileSample])` ‚Üí `FlameGraphNode`
  > Generate flame graph from profiling samples.

Args:
    samples: List of profili...
- `to_json(flame_graph: FlameGraphNode)` ‚Üí `str`
  > Convert flame graph to JSON.

Args:
    flame_graph: Flame graph root node

Retu...
- `save_json(flame_graph: FlameGraphNode, filename: Optional[st)` ‚Üí `str`
  > Save flame graph as JSON.

Args:
    flame_graph: Flame graph root node
    file...
- `to_svg(flame_graph: FlameGraphNode)` ‚Üí `str`
  > Convert flame graph to SVG format.

This is a simplified SVG generation. In prod...
- `save_svg(flame_graph: FlameGraphNode, filename: Optional[st)` ‚Üí `str`
  > Save flame graph as SVG.

Args:
    flame_graph: Flame graph root node
    filen...

### `LogAnalytics`

Log analytics and insights.

Provides statistical analysis and insights from aggregated logs.

**M√©todos principais:**

- `get_level_distribution()` ‚Üí `Dict[str, int]`
  > Get distribution of log levels.

Returns:
    Dictionary mapping level name to c...
- `get_top_loggers(limit: int)` ‚Üí `List[Tuple[str, int]]`
  > Get top loggers by volume.

Args:
    limit: Maximum number of loggers to return...
- `get_error_rate(window_seconds: int)` ‚Üí `float`
  > Calculate error rate in the specified time window.

Args:
    window_seconds: Ti...
- `get_timeline(bucket_size_seconds: int)` ‚Üí `Dict[str, List[int]]`
  > Get log timeline bucketed by time.

Args:
    bucket_size_seconds: Size of each ...
- `find_anomalies(threshold: float)` ‚Üí `List[str]`
  > Find anomalous patterns in logs.

Uses simple statistical methods to find unusua...

### `Span`

Represents a single operation in a distributed trace.

Attributes:
    context: Span context with trace and span IDs
    name: Operation name
    kind: Span kind (internal, server, client, etc.)
    start_time: Start timestamp in nanoseconds
    end_time: End timestamp in nanoseconds (None if not ended)
    status: Span status
    attributes: Additional metadata
    events: List of events that occurred during the span
    links: Links to other spans

**M√©todos principais:**

- `set_attribute(key: str, value: Any)` ‚Üí `None`
  > Set a span attribute....
- `add_event(name: str, attributes: Optional[Dict[str, Any]])` ‚Üí `None`
  > Add an event to the span....
- `set_status(status: SpanStatus, description: str)` ‚Üí `None`
  > Set the span status....
- `end()` ‚Üí `None`
  > End the span....
- `duration_ms()` ‚Üí `float`
  > Get span duration in milliseconds....

### `Metric`

Represents a single metric with its metadata.

Attributes:
    name: Metric name (should be snake_case)
    type: Metric type
    help_text: Description of what this metric measures
    unit: Unit of measurement (e.g., 'seconds', 'bytes')
    values: List of metric values

**M√©todos principais:**

- `add_value(value: float, labels: Optional[Dict[str, str]])` ‚Üí `None`
  > Add a new value to the metric.

Args:
    value: The metric value
    labels: Op...
- `get_latest_value()` ‚Üí `Optional[float]`
  > Get the most recent value....
- `to_prometheus_format()` ‚Üí `str`
  > Export metric in Prometheus text format.

Returns:
    Prometheus-formatted metr...


## ‚öôÔ∏è Fun√ß√µes P√∫blicas

#### `__init__(config: TraceConfig)` ‚Üí `None`

*Initialize the distributed tracer.

Args:
    config: Tracing configuration...*

#### `__init__(log_entries: List[LogEntry])` ‚Üí `None`

*Initialize analytics with log entries.

Args:
    log_entries: List of log entries to analyze...*

#### `__init__(config: LogConfig)` ‚Üí `None`

*Initialize the log aggregator.

Args:
    config: Log aggregation configuration...*

#### `__init__(config: MetricsConfig)` ‚Üí `None`

*Initialize the metrics exporter.

Args:
    config: Metrics configuration...*

#### `__init__(service_name: str, service_version: str, environme)` ‚Üí `None`

*Initialize OpenTelemetry configuration.

Args:
    service_name: Name of the service
    service_ver...*

#### `__init__(config: OpenTelemetryConfig)` ‚Üí `None`

*Initialize OpenTelemetry integration.

Args:
    config: OpenTelemetry configuration...*

#### `__init__()` ‚Üí `None`

*Initialize performance analyzer....*

#### `__init__(config: ProfilingConfig)` ‚Üí `None`

*Initialize the continuous profiler.

Args:
    config: Profiling configuration...*

#### `__init__()` ‚Üí `None`

*Initialize the flame graph generator....*

#### `__post_init__()` ‚Üí `None`

*Compile the regex pattern....*

#### `__post_init__()` ‚Üí `None`

*Calculate per-call time....*

#### `_categorize_bottleneck(sample: ProfileSample)` ‚Üí `BottleneckCategory`

*Categorize a bottleneck based on sample characteristics.

Args:
    sample: Profile sample

Returns:...*

#### `_check_patterns(entry: LogEntry)` ‚Üí `None`

*Check log entry against registered patterns.

Args:
    entry: Log entry to check...*

#### `_cleanup_old_logs()` ‚Üí `None`

*Remove logs older than retention period....*

#### `_cleanup_old_metrics()` ‚Üí `None`

*Remove metrics older than retention period....*


## üì¶ M√≥dulos

**Total:** 6 arquivos

- `distributed_tracing.py`: Distributed Tracing Module.

Implements distributed request ...
- `log_aggregator.py`: Log Aggregation and Analysis Module.

Implements advanced lo...
- `metrics_exporter.py`: Custom Metrics Exporter Module.

Implements business and ML-...
- `opentelemetry_integration.py`: OpenTelemetry Full Integration Module.

This module provides...
- `performance_analyzer.py`: Performance Bottleneck Analyzer Module.

Provides automated ...
- `profiling_tools.py`: Performance Profiling Tools Module.

Implements continuous p...
