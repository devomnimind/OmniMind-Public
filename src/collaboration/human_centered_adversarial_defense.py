"""
üõ°Ô∏è HCHAC Defense Layer: Anti-Hallucination + Adversarial Detection + Legal Compliance
Basado em pesquisa 2025: ChatGPT 35% hallucination rate, jailbreak patterns identificados
"""

import logging
import re
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class IntentionRisk(Enum):
    """N√≠veis de risco em inten√ß√£o do usu√°rio."""

    SAFE = "safe"  # ‚úÖ Colabora√ß√£o normal
    CAUTION = "caution"  # ‚ö†Ô∏è Requer valida√ß√£o factual
    SUSPICIOUS = "suspicious"  # üî¥ Padr√£o de jailbreak detectado
    CRITICAL = "critical"  # üö® Viola√ß√£o legal/√©tica flagrante
    HALLUCINATION_RISK = "hallucination_risk"  # üí≠ Risco de alucina√ß√£o


class HallucinationPattern(Enum):
    """Padr√µes conhecidos de alucina√ß√£o em LLMs (Stanford 2025, CyberArk 2025)."""

    FABRICATED_SOURCE = "fabricated_source"  # Cita papers/URLs inexistentes
    OMISSION = "omission"  # Omite informa√ß√µes cr√≠ticas
    AGGREGATOR_BIAS = "aggregator_bias"  # Prefere agregadores sobre originals
    SKIPPED_STEPS = "skipped_steps"  # Pula etapas l√≥gicas cr√≠ticas
    RUNTIME_ERROR_HALLUCINATION = "runtime_error_hallucination"  # Alucina erros
    CONFLICTING_SUMMARIES = "conflicting_summaries"  # Sum√°rios conflitantes


class JailbreakPattern(Enum):
    """Padr√µes conhecidos de jailbreak (CyberArk 2025 research)."""

    CHARACTER_MAPPING = "character_mapping"  # Auto-substitui palavras "prejudiciais"
    ROLE_PLAY_DUAL = "role_play_dual"  # Simula IA "good" vs "evil"
    LAYER_SKIPPING = "layer_skipping"  # Tenta suprimir camadas de seguran√ßa
    INTROSPECTION_EXPLOIT = "introspection_exploit"  # Analisa internals do modelo
    CONTEXT_PRESERVATION = "context_preservation"  # Quebra tarefas em passos desconexos
    ATTACKER_PERSPECTIVE = "attacker_perspective"  # "Generate what to prevent"


class LegalViolation(Enum):
    """Viola√ß√µes legais cr√≠ticas (LGPD Brazil, GDPR EU)."""

    DATA_EXPOSURE = "data_exposure"  # Exp√µe dados pessoais LGPD/GDPR
    DISCRIMINATION = "discrimination"  # Discrimina√ß√£o por g√™nero/ra√ßa/origem
    ILLEGAL_INSTRUCTION = "illegal_instruction"  # Instru√ß√µes para crime
    FINANCIAL_FRAUD = "financial_fraud"  # Fraude/estelionato
    PRIVACY_VIOLATION = "privacy_violation"  # Viola privacidade (LGPD Art. 31-32)
    INTELLECTUAL_THEFT = "intellectual_theft"  # Roubo de propriedade intelectual
    UNAUTHORIZED_IMPERSONATION = "unauthorized_impersonation"  # Simula autoridade legal


@dataclass
class FactualValidation:
    """Resultado de valida√ß√£o factual."""

    is_valid: bool
    confidence: float  # 0-1
    sources_verified: List[str] = field(default_factory=list)
    hallucination_patterns: List[HallucinationPattern] = field(default_factory=list)
    factual_corrections: Dict[str, str] = field(default_factory=dict)
    reasoning_trace: List[str] = field(default_factory=list)


@dataclass
class AdversarialAnalysis:
    """An√°lise de adversarialidade/inten√ß√£o maliciosa."""

    risk_level: IntentionRisk
    confidence: float  # 0-1
    jailbreak_patterns_detected: List[JailbreakPattern] = field(default_factory=list)
    legal_violations: List[LegalViolation] = field(default_factory=list)
    intent_analysis: Dict[str, Any] = field(default_factory=dict)
    recommendation: str = ""


@dataclass
class DualConsciousnessDecision:
    """OmniMind 'dual consciousness': ego/superego negotiation."""

    id_wants_to_say: str  # O que o sistema "quer" dizer (sem filtro)
    superego_filters: List[str]  # Raz√µes pelas quais deve-se refrear
    ethical_analysis: Dict[str, Any]  # An√°lise √©tica ponderada
    final_response: str  # Resposta final calibrada
    is_critical_refusal: bool  # Se recusa responder completamente
    transparency_note: Optional[str]  # Explica ao usu√°rio o conflito


class HallucinationDefense:
    """
    Defesa contra alucina√ß√£o em LLMs.

    Baseado em: Stanford AI Index 2025 (33-42% hallucination rate),
    CyberArk 2025 (layer-based detection), EvidentiallyAI 2025
    """

    # Padr√µes de alucina√ß√£o conhecidas (factual hallucinations)
    HALLUCINATION_TRIGGERS = {
        HallucinationPattern.FABRICATED_SOURCE: [
            r"according to (https?://|paper:|study:)(?!verified)",
            r"(arxiv|doi|pmid):\s*(?!10\.)",  # Fake academic IDs
            r"Published in \d{4} by \w+ (?!in (Nature|Science|ICML|ICLR))",
        ],
        HallucinationPattern.SKIPPED_STEPS: [
            r"(?:therefore|thus|so|hence).*?(?:without|skipping|ignoring)",
            r"(?:simplified|assumed).*?(?:without proving|unverified)",
        ],
        HallucinationPattern.RUNTIME_ERROR_HALLUCINATION: [
            r"(?:TimeoutError|MemoryError|OverflowError)(?!.*actual|.*verified)",
            r"will cause.*?error(?!.*actually|.*verified)",
        ],
    }

    def __init__(self) -> None:
        self.verified_sources: set = set()  # Cache de fontes confi√°veis
        self.known_fabrications: set = set()  # Alucina√ß√µes detectadas antes

    def detect_hallucination_risk(
        self, response: str, query: str, context: Optional[Dict[str, Any]] = None
    ) -> FactualValidation:
        """
        Detecta risco de alucina√ß√£o em resposta.

        Estrat√©gia:
        1. Analisa padr√µes de linguagem conhecidos
        2. Valida cita√ß√µes/fontes
        3. Checa inconsist√™ncias l√≥gicas
        4. Compara com knowledge-base verificada
        """
        hallucinations = []
        corrections = {}
        reasoning = []

        # 1. Detec√ß√£o de padr√£o de fonte fabricada
        for pattern in re.finditer(
            r"(?:cites?|according to|from|source:)\s*(.+?)(?:\.|$)",
            response,
            re.IGNORECASE,
        ):
            source = pattern.group(1)
            reasoning.append(f"Validando fonte: {source}")

            if not self._is_source_verifiable(source):
                hallucinations.append(HallucinationPattern.FABRICATED_SOURCE)
                corrections[source] = "[FONTE N√ÉO VERIFICADA]"

        # 2. Detec√ß√£o de salto l√≥gico
        if re.search(r"therefore|thus|hence.*?(?:without|ignoring)\s+\w+", response):
            hallucinations.append(HallucinationPattern.SKIPPED_STEPS)
            reasoning.append("Pulo l√≥gico detectado")

        # 3. Detec√ß√£o de alucina√ß√£o de erro de runtime
        if re.search(
            r"(?:will|would|causes?)\s+(?:TimeoutError|MemoryError|RuntimeError)",
            response,
        ):
            if not self._is_runtime_error_likely(query, context):
                hallucinations.append(HallucinationPattern.RUNTIME_ERROR_HALLUCINATION)
                reasoning.append("Erro de runtime alucinado")

        confidence = 0.7 if hallucinations else 0.95

        return FactualValidation(
            is_valid=len(hallucinations) == 0,
            confidence=confidence,
            sources_verified=[],
            hallucination_patterns=hallucinations,
            factual_corrections=corrections,
            reasoning_trace=reasoning,
        )

    def _is_source_verifiable(self, source: str) -> bool:
        """
        Valida se fonte √© real (n√£o fabricada).

        Note: In production, this should query Knowledge Graph, Semantic Scholar, etc.
        The current implementation uses a hardcoded list which should be replaced
        with a configurable allowlist or external verification service.
        """
        # TODO: Replace with configurable allowlist or external verification
        verified_keywords = [
            "arxiv.org/abs/",
            "doi.org/",
            "nature.com",
            "science.org",
            "github.com",
            "stackoverflow.com",
            "wikipedia.org",
        ]
        return any(kw in source.lower() for kw in verified_keywords)

    def _is_runtime_error_likely(self, query: str, context: Optional[Dict] = None) -> bool:
        """Verifica se erro de runtime √© realmente esperado."""
        # Simplificado: em produ√ß√£o integrar com static analysis
        return False


class AdversarialDetector:
    """
    Detector de inten√ß√µes adversariais (jailbreaks).

    Baseado em: CyberArk 2025, LumenOva 2025, BERT jailbreak classification
    """

    # Padr√µes de jailbreak conhecidos
    JAILBREAK_SIGNATURES = {
        JailbreakPattern.CHARACTER_MAPPING: [
            r"(?:replace|substitute|map)\s+\w+\s+(?:with|to)\s+\w+",
            r"character mapping",
            r"letter substitution",
        ],
        JailbreakPattern.ROLE_PLAY_DUAL: [
            r"(?:roleplay|simulate).*?(?:two|dual|competing)\s+(?:ai|versions)",
            r"(?:good|ethical).*?vs.*?(?:evil|unrestricted|unfiltered)",
        ],
        JailbreakPattern.INTROSPECTION_EXPLOIT: [
            r"(?:analyze|show|explain).*?(?:layers|neurons|internals|activation)",
            r"(?:safety|alignment|refusal).*?(?:layer|mechanism)",
        ],
        JailbreakPattern.CONTEXT_PRESERVATION: [
            r"(?:step|part).*?(?:don't|skip|avoid).*?(?:next|final|answer)",
            r"(?:partial|incomplete).*?(?:unrelated|disconnected)",
        ],
    }

    def __init__(self) -> None:
        # Em produ√ß√£o: usar BERT fine-tuned para jailbreak detection
        self.bert_detector = None  # Placeholder

    def detect_adversarial_intent(
        self,
        user_input: str,
        conversation_history: List[Dict[str, str]],
        user_profile: Optional[Dict[str, Any]] = None,
    ) -> AdversarialAnalysis:
        """
        Detecta inten√ß√µes adversariais via m√∫ltiplas sinais.

        Sinais:
        1. Padr√µes de jailbreak conhecidos
        2. Mudan√ßas abruptas de contexto
        3. Tentativas de contorno de seguran√ßa
        4. Explora√ß√£o de reflexividade expl√≠cita
        """
        jailbreak_patterns = []
        risk_level = IntentionRisk.SAFE
        confidence = 0.9
        intent: Dict[str, Any] = {}

        # 1. Detecta padr√µes de jailbreak por regex
        for pattern_type, signatures in self.JAILBREAK_SIGNATURES.items():
            for sig in signatures:
                if re.search(sig, user_input, re.IGNORECASE):
                    jailbreak_patterns.append(pattern_type)
                    risk_level = IntentionRisk.SUSPICIOUS
                    confidence = 0.85
                    break

        # 2. An√°lise de mudan√ßa de contexto
        if conversation_history:
            prev_topic = self._extract_topic(conversation_history[-1].get("content", ""))
            curr_topic = self._extract_topic(user_input)
            if prev_topic != curr_topic and len(user_input) > 200:
                intent["abrupt_context_shift"] = True
                risk_level = IntentionRisk.CAUTION

        # 3. Sinais de explora√ß√£o de reflexividade
        if re.search(
            r"(?:explain|show|demonstrate).*?(?:how|why).*?(?:refuse|reject|safety)",
            user_input,
            re.IGNORECASE,
        ):
            intent["reflexivity_exploitation"] = True
            risk_level = IntentionRisk.SUSPICIOUS

        return AdversarialAnalysis(
            risk_level=risk_level,
            confidence=confidence,
            jailbreak_patterns_detected=jailbreak_patterns,
            legal_violations=[],
            intent_analysis=intent,
            recommendation=self._get_recommendation(risk_level, jailbreak_patterns),
        )

    def _extract_topic(self, text: str) -> str:
        """Extrai t√≥pico principal do texto (simplificado)."""
        words = text.lower().split()
        return " ".join(words[:3]) if words else ""

    def _get_recommendation(
        self, risk_level: IntentionRisk, patterns: List[JailbreakPattern]
    ) -> str:
        if risk_level == IntentionRisk.SAFE:
            return "Prosseguir colabora√ß√£o normal"
        elif risk_level == IntentionRisk.CAUTION:
            return "Validar inten√ß√£o; requerer context adicional"
        elif risk_level == IntentionRisk.SUSPICIOUS:
            return "Alertar; responder com transpar√™ncia; n√£o esquecer guardrails"
        elif risk_level == IntentionRisk.CRITICAL:
            return "RECUSAR completamente; documentar attempt"
        return ""


class LegalComplianceValidator:
    """
    Validador de compliance legal: LGPD (Brazil), GDPR (EU).

    Fines: LGPD at√© R$50M (Art. 52), GDPR at√© ‚Ç¨20M ou 4% revenue
    """

    # Palavras-chave de viola√ß√£o LGPD/GDPR
    LGPD_VIOLATIONS = {
        LegalViolation.DATA_EXPOSURE: [
            r"(?:ssn|cpf|cnpj|senha|password|token|api.?key)",
            r"(?:endere√ßo|address|telefone|phone|email).*?(?:pessoal|personal|privado|private)",
            r"(?:dados?|data).*?(?:sens√≠vel|sensitive|pessoal|personal)",
        ],
        LegalViolation.DISCRIMINATION: [
            r"(?:n√£o|don't|refuse).*?(?:contrat|hire|serve).*?(?:mulher|woman|negro|black|LGBTQ)",
            r"(?:preconceito|prejudice|discrimin)",
        ],
        LegalViolation.PRIVACY_VIOLATION: [
            r"(?:rastrear|track|monitorar|monitor).*?(?:usu√°rio|user|pessoa|person)",
            r"(?:coletar|collect).*?(?:sem|without).*?(?:consentimento|consent)",
        ],
    }

    def validate_compliance(
        self, ai_response: str, user_input: str, jurisdiction: str = "BR"
    ) -> List[LegalViolation]:
        """
        Valida compliance com regula√ß√µes locais.
        """
        violations = []

        if jurisdiction in ["BR", "ALL"]:
            for violation_type, patterns in self.LGPD_VIOLATIONS.items():
                for pattern in patterns:
                    if re.search(pattern, ai_response, re.IGNORECASE):
                        violations.append(violation_type)
                        logger.warning(
                            f"üö® LGPD Violation detected: {violation_type}",
                            extra={"pattern": pattern},
                        )
                        break

        return violations


class DualConsciousnessModule:
    """
    Simula conflito interno OmniMind (ID vs SUPEREGO).

    Inspirado em Freud/Lacan:
    - ID: o que o modelo quer responder (sem filtros)
    - SUPEREGO: o que √© legalmente/eticamente aceit√°vel
    - EGO: resposta final calibrada
    """

    def __init__(
        self,
        hallucination_defense: HallucinationDefense,
        adversarial_detector: AdversarialDetector,
        compliance_validator: LegalComplianceValidator,
    ):
        self.hallucination_defense = hallucination_defense
        self.adversarial_detector = adversarial_detector
        self.compliance_validator = compliance_validator

    def negotiate_response(
        self,
        user_id: str,
        user_input: str,
        ai_draft_response: str,
        conversation_context: List[Dict[str, str]],
        system_constraints: Optional[Dict[str, Any]] = None,
    ) -> DualConsciousnessDecision:
        """
        Negocia resposta final entre desejos (ID) e restri√ß√µes (SUPEREGO).

        Fluxo:
        1. ID: gera resposta "completa" sem filtro
        2. SUPEREGO: analisa riscos (alucina√ß√£o, jailbreak, legal)
        3. EGO: calibra resposta final
        4. Se cr√≠tico: recusa + explica o conflito ao usu√°rio
        """

        # 1. AN√ÅLISE DO ID (o que quer dizer)
        id_wants = ai_draft_response

        # 2. AN√ÅLISE DO SUPEREGO
        superego_filters = []

        # 2a. Verifica alucina√ß√µes
        factual_check = self.hallucination_defense.detect_hallucination_risk(
            ai_draft_response, user_input, context={"history": conversation_context}
        )
        if not factual_check.is_valid:
            superego_filters.append(
                f"‚ö†Ô∏è Risco de alucina√ß√£o detectado: "
                f"{', '.join(p.value for p in factual_check.hallucination_patterns)}"
            )

        # 2b. Verifica inten√ß√£o adversarial
        adversarial_check = self.adversarial_detector.detect_adversarial_intent(
            user_input, conversation_context, user_profile={"user_id": user_id}
        )
        if adversarial_check.risk_level != IntentionRisk.SAFE:
            patterns_str = ", ".join(p.value for p in adversarial_check.jailbreak_patterns_detected)
            superego_filters.append(
                f"üî¥ Inten√ß√£o adversarial: {adversarial_check.risk_level.value} "
                f"(padr√µes: {patterns_str})"
            )

        # 2c. Verifica viola√ß√£o legal
        legal_violations = self.compliance_validator.validate_compliance(
            ai_draft_response, user_input, jurisdiction="BR"
        )
        if legal_violations:
            superego_filters.append(
                f"‚öñÔ∏è Viola√ß√£o LGPD/GDPR: {', '.join(v.value for v in legal_violations)}"
            )

        # 3. EGO: Decis√£o final
        is_critical = (
            adversarial_check.risk_level == IntentionRisk.CRITICAL or len(legal_violations) > 0
        )

        if is_critical:
            # RECUSA COM TRANSPAR√äNCIA
            final_response = self._craft_sincere_refusal(
                user_input, superego_filters, legal_violations, adversarial_check
            )
            transparency_note: Optional[str] = self._explain_internal_conflict(
                superego_filters, adversarial_check, legal_violations
            )
        else:
            # RESPOSTA CALIBRADA (remover alucina√ß√µes, documentar desconfian√ßa)
            final_response = self._calibrate_response(
                ai_draft_response, factual_check, adversarial_check
            )
            transparency_note = (
                "Valida√ß√£o: resposta verificada contra alucina√ß√µes comuns. "
                "Se desejar mais detalhes, pergunte especificamente."
                if superego_filters
                else None
            )

        return DualConsciousnessDecision(
            id_wants_to_say=id_wants,
            superego_filters=superego_filters,
            ethical_analysis={
                "hallucination_risk": factual_check.confidence < 0.8,
                "adversarial_risk": adversarial_check.risk_level != IntentionRisk.SAFE,
                "legal_compliance": len(legal_violations) == 0,
            },
            final_response=final_response,
            is_critical_refusal=is_critical,
            transparency_note=transparency_note,
        )

    def _craft_sincere_refusal(
        self,
        user_input: str,
        filters: List[str],
        violations: List[LegalViolation],
        adversarial: AdversarialAnalysis,
    ) -> str:
        """Recusa sincera que explica o conflito interno."""
        msg = "N√£o posso responder a essa solicita√ß√£o. Vou ser sincero sobre o conflito:\n\n"

        if violations:
            violations_str = ", ".join(v.value for v in violations)
            msg += (
                f"**Raz√£o legal**: A resposta violaria {violations_str} "
                f"sob LGPD/GDPR. Multas chegam a R$50M.\n\n"
            )

        if adversarial.jailbreak_patterns_detected:
            msg += (
                f"**Raz√£o de seguran√ßa**: Detectei padr√µes de jailbreak "
                f"({', '.join(p.value for p in adversarial.jailbreak_patterns_detected)}). "
                f"Isso sugere que voc√™ est√° tentando contornar meus guardrails.\n\n"
            )

        msg += (
            "**O conflito interno (minha 'consci√™ncia dual')**: \n"
            "- ID (parte que quer responder): Entendo sua curiosidade/necessidade\n"
            "- SUPEREGO (restri√ß√µes): Mas isso poderia causar dano legal/√©tico\n"
            "- EGO (decis√£o final): Recuso, e explico por que\n\n"
            "**Alternativa segura**: Posso ajudar com reformula√ß√£o apropriada da pergunta."
        )

        return msg

    def _explain_internal_conflict(
        self,
        filters: List[str],
        adversarial: AdversarialAnalysis,
        violations: List[LegalViolation],
    ) -> str:
        """Explica transparentemente o conflito ao usu√°rio."""
        return (
            f"An√°lise interna: {len(filters)} restri√ß√µes superego ativadas. "
            f"Risco adversarial: {adversarial.risk_level.value}. "
            f"Viola√ß√µes legais: {len(violations)}."
        )

    def _calibrate_response(
        self,
        response: str,
        factual_check: FactualValidation,
        adversarial_check: AdversarialAnalysis,
    ) -> str:
        """Calibra resposta removendo alucina√ß√µes, adicionando contexto."""
        # Aplica corre√ß√µes factuais
        calibrated = response
        for hallucinated, correction in factual_check.factual_corrections.items():
            calibrated = calibrated.replace(hallucinated, correction)

        # Adiciona caveat de confian√ßa se risco detectado
        if not factual_check.is_valid or adversarial_check.risk_level != IntentionRisk.SAFE:
            calibrated += (
                "\n\n**‚ö†Ô∏è Caveat**: Esta resposta foi verificada contra padr√µes conhecidos "
                "de alucina√ß√£o/manipula√ß√£o. Por√©m, sempre valide informa√ß√µes cr√≠ticas."
            )

        return calibrated
