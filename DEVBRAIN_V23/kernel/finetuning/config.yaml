# Training configuration for Mistral 7B
model_name: "mistralai/Mistral-7B-v0.1"
output_dir: "./mistral_finetuned"
# Training parameters
num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 2e-4
warmup_steps: 100
weight_decay: 0.01
max_grad_norm: 1.0
# Model parameters
max_seq_length: 512
load_in_4bit: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
# Logging
logging_steps: 10
eval_steps: 50
save_steps: 100
logging_dir: "./logs"
# Optimization
optim: "paged_adamw_32bit"
lr_scheduler_type: "constant"
gradient_checkpointing: true
report_to: ["tensorboard"]
seed: 42
